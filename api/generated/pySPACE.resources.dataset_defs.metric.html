<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>metric &mdash; pySPACE 1.0 release documentation</title>
    
    <link rel="stylesheet" href="../../_static/pySPACE.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../',
        VERSION:     '1.0 release',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <link rel="shortcut icon" href="../../_static/pyspace-logo.ico"/>
    <link rel="top" title="pySPACE 1.0 release documentation" href="../../index.html" />
    <link rel="up" title="dataset_defs Package" href="pySPACE.resources.dataset_defs.html" />
    <link rel="next" title="performance_result" href="pySPACE.resources.dataset_defs.performance_result.html" />
    <link rel="prev" title="feature_vector" href="pySPACE.resources.dataset_defs.feature_vector.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="pySPACE.resources.dataset_defs.performance_result.html" title="performance_result"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="pySPACE.resources.dataset_defs.feature_vector.html" title="feature_vector"
             accesskey="P">previous</a> |</li>
        <li><a href="../../index.html">pySPACE 1.0 release documentation</a> &raquo;</li>
          <li><a href="../../content.html" >Table of Contents</a> &raquo;</li>
          <li><a href="pySPACE.html" >pySPACE Package</a> &raquo;</li>
          <li><a href="pySPACE.resources.html" >resources Package</a> &raquo;</li>
          <li><a href="pySPACE.resources.dataset_defs.html" accesskey="U">dataset_defs Package</a> &raquo;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../../index.html">
              <img class="logo" src="../../_static/pyspace-logo_small.png" alt="Logo"/>
            </a></p>
  <h3><a href="../../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">metric</a><ul>
<li><a class="reference internal" href="#module-pySPACE.resources.dataset_defs.metric">Module: <tt class="docutils literal"><span class="pre">resources.dataset_defs.metric</span></tt></a></li>
<li><a class="reference internal" href="#class-summary">Class Summary</a></li>
<li><a class="reference internal" href="#classes">Classes</a><ul>
<li><a class="reference internal" href="#metricdict"><tt class="docutils literal"><span class="pre">metricdict</span></tt></a></li>
<li><a class="reference internal" href="#binaryclassificationdataset"><tt class="docutils literal"><span class="pre">BinaryClassificationDataset</span></tt></a></li>
<li><a class="reference internal" href="#multinomialclassificationdataset"><tt class="docutils literal"><span class="pre">MultinomialClassificationDataset</span></tt></a></li>
<li><a class="reference internal" href="#regressiondataset"><tt class="docutils literal"><span class="pre">RegressionDataset</span></tt></a></li>
</ul>
</li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="pySPACE.resources.dataset_defs.feature_vector.html"
                        title="previous chapter">feature_vector</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="pySPACE.resources.dataset_defs.performance_result.html"
                        title="next chapter">performance_result</a></p>
  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="../../_sources/api/generated/pySPACE.resources.dataset_defs.metric.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="metric">
<h1>metric<a class="headerlink" href="#metric" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-pySPACE.resources.dataset_defs.metric">
<span id="module-resources-dataset-defs-metric"></span><h2>Module: <tt class="xref py py-mod docutils literal"><span class="pre">resources.dataset_defs.metric</span></tt><a class="headerlink" href="#module-pySPACE.resources.dataset_defs.metric" title="Permalink to this headline">¶</a></h2>
<p>Methods to calculate and store classification results (metrics)</p>
<p>Several performance measures are supported.</p>
<p>To combine and visualize them, use the
<a class="reference internal" href="pySPACE.resources.dataset_defs.performance_result.html#pySPACE.resources.dataset_defs.performance_result.PerformanceResultSummary" title="pySPACE.resources.dataset_defs.performance_result.PerformanceResultSummary"><tt class="xref py py-class docutils literal"><span class="pre">PerformanceResultSummary</span></tt></a>.</p>
<p>For details concerning parameters in metric calculation, have a look at
<a class="reference internal" href="pySPACE.missions.nodes.sink.classification_performance_sink.html#pySPACE.missions.nodes.sink.classification_performance_sink.PerformanceSinkNode" title="pySPACE.missions.nodes.sink.classification_performance_sink.PerformanceSinkNode"><tt class="xref py py-class docutils literal"><span class="pre">PerformanceSinkNode</span></tt></a>.</p>
<p>Inheritance diagram for <tt class="docutils literal"><span class="pre">pySPACE.resources.dataset_defs.metric</span></tt>:</p>
<p class="graphviz">
<img src="../../_images/inheritance-428fcff4b1316e8fad4366ab7af8efbde854df04.png" alt="Inheritance diagram of pySPACE.resources.dataset_defs.metric" usemap="#inheritance39f914d1d2" class="inheritance"/>
<map id="inheritance39f914d1d2" name="inheritance39f914d1d2">
<area shape="rect" id="node1" href="pySPACE.resources.dataset_defs.base.html#pySPACE.resources.dataset_defs.base.BaseDataset" title="Base class for datasets" alt="" coords="161,1,268,15"/>
<area shape="rect" id="node3" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset" title="Handle and store binary classification performance measures" alt="" coords="115,65,315,79"/>
<area shape="rect" id="node2" title="defaultdict(default_factory) &#45;&#45;&gt; dict with default factory" alt="" coords="612,1,737,15"/>
<area shape="rect" id="node6" href="#pySPACE.resources.dataset_defs.metric.metricdict" title="Interface to dictionaries of metrics" alt="" coords="626,65,723,79"/>
<area shape="rect" id="node4" href="#pySPACE.resources.dataset_defs.metric.MultinomialClassificationDataset" title="Handle and store multiclass classification performance measures" alt="" coords="0,129,229,143"/>
<area shape="rect" id="node5" href="#pySPACE.resources.dataset_defs.metric.RegressionDataset" title="Calculate 1&#45;dimensional and n&#45;dimensional regression metrics" alt="" coords="239,129,389,143"/>
</map>
</p>
</div>
<div class="section" id="class-summary">
<h2>Class Summary<a class="headerlink" href="#class-summary" title="Permalink to this headline">¶</a></h2>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%" />
<col width="90%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="#pySPACE.resources.dataset_defs.metric.metricdict" title="pySPACE.resources.dataset_defs.metric.metricdict"><tt class="xref py py-obj docutils literal"><span class="pre">metricdict</span></tt></a></td>
<td>Interface to dictionaries of metrics</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset" title="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset"><tt class="xref py py-obj docutils literal"><span class="pre">BinaryClassificationDataset</span></tt></a>([dataset_md,&nbsp;...])</td>
<td>Handle and store binary classification performance measures</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#pySPACE.resources.dataset_defs.metric.MultinomialClassificationDataset" title="pySPACE.resources.dataset_defs.metric.MultinomialClassificationDataset"><tt class="xref py py-obj docutils literal"><span class="pre">MultinomialClassificationDataset</span></tt></a>([...])</td>
<td>Handle and store multiclass classification performance measures</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#pySPACE.resources.dataset_defs.metric.RegressionDataset" title="pySPACE.resources.dataset_defs.metric.RegressionDataset"><tt class="xref py py-obj docutils literal"><span class="pre">RegressionDataset</span></tt></a>([dataset_md,&nbsp;dataset_pattern])</td>
<td>Calculate 1-dimensional and n-dimensional regression metrics</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="classes">
<h2>Classes<a class="headerlink" href="#classes" title="Permalink to this headline">¶</a></h2>
<div class="section" id="metricdict">
<h3><a class="reference internal" href="#pySPACE.resources.dataset_defs.metric.metricdict" title="pySPACE.resources.dataset_defs.metric.metricdict"><tt class="xref py py-class docutils literal"><span class="pre">metricdict</span></tt></a><a class="headerlink" href="#metricdict" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.resources.dataset_defs.metric.metricdict">
<em class="property">class </em><tt class="descclassname">pySPACE.resources.dataset_defs.metric.</tt><tt class="descname">metricdict</tt><a class="reference internal" href="../../_modules/pySPACE/resources/dataset_defs/metric.html#metricdict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pySPACE.resources.dataset_defs.metric.metricdict" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">collections.defaultdict</span></tt></p>
<p>Interface to dictionaries of metrics</p>
<p><strong>Class Components Summary</strong></p>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%" />
<col width="90%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="#pySPACE.resources.dataset_defs.metric.metricdict.__missing__" title="pySPACE.resources.dataset_defs.metric.metricdict.__missing__"><tt class="xref py py-obj docutils literal"><span class="pre">__missing__</span></tt></a>(new_key)</td>
<td>Return first occurring fitting entry and give warning, if functional metric is called without parameters</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="pySPACE.resources.dataset_defs.metric.metricdict.__missing__">
<tt class="descname">__missing__</tt><big>(</big><em>new_key</em><big>)</big><a class="reference internal" href="../../_modules/pySPACE/resources/dataset_defs/metric.html#metricdict.__missing__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pySPACE.resources.dataset_defs.metric.metricdict.__missing__" title="Permalink to this definition">¶</a></dt>
<dd><p>Return first occurring fitting entry and give warning, if functional metric is called without parameters</p>
</dd></dl>

<dl class="attribute">
<dt id="pySPACE.resources.dataset_defs.metric.metricdict.__weakref__">
<tt class="descname">__weakref__</tt><a class="headerlink" href="#pySPACE.resources.dataset_defs.metric.metricdict.__weakref__" title="Permalink to this definition">¶</a></dt>
<dd><p>list of weak references to the object (if defined)</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="binaryclassificationdataset">
<h3><a class="reference internal" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset" title="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset"><tt class="xref py py-class docutils literal"><span class="pre">BinaryClassificationDataset</span></tt></a><a class="headerlink" href="#binaryclassificationdataset" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset">
<em class="property">class </em><tt class="descclassname">pySPACE.resources.dataset_defs.metric.</tt><tt class="descname">BinaryClassificationDataset</tt><big>(</big><em>dataset_md=None</em>, <em>dataset_pattern=None</em><big>)</big><a class="reference internal" href="../../_modules/pySPACE/resources/dataset_defs/metric.html#BinaryClassificationDataset"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="pySPACE.resources.dataset_defs.base.html#pySPACE.resources.dataset_defs.base.BaseDataset" title="pySPACE.resources.dataset_defs.base.BaseDataset"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.resources.dataset_defs.base.BaseDataset</span></tt></a></p>
<p>Handle and store binary classification performance measures</p>
<p>This class derived from BaseDataset overwrites the &#8216;store&#8217; and
&#8216;add_split&#8217; method from the BaseDataset class so that it can
handle and store classification performance measures to files.</p>
<p>In the following there is a list of implemented metrics.
After giving the normal name or abbreviation, the name in the final
results file/dictionary is given.
This is for example needed for parameter optimization algorithms.</p>
<p id="metrics"><strong>Metrics</strong></p>
<blockquote>
<div><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">confusion matrix components:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><table class="first docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">TP - True_positives:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">correct classified examples or the <em>ir_class</em> (positive examples)</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">TN - True_negatives:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">correct classified examples or the <em>ir_class</em> (negative examples)</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">FN - False_negatives:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">wrong classified positive examples (classified as negative examples)</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">FP - False_positives:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">wrong classified negative examples (classified as positive examples)</td>
</tr>
</tbody>
</table>
</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">confusion matrix metrics:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">TPR - True_positive_rate:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><p class="first">true positive rate, recall</p>
<div class="math">
<p><img src="../../_images/math/6f2cfc0c4d839033eadd455af9ffaceb938f5798.png" alt="\frac{TP}{TP+FN}"/></p>
</div></td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">PPV - IR_precision:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><p class="first">positive predictive value, precision</p>
<div class="math">
<p><img src="../../_images/math/60b7b54e4c7ca93c1a542a0d3a18191590ed82e0.png" alt="\frac{TP}{TP+FP}"/></p>
</div></td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">TNR - True_negative_rate:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><p class="first">true negative rate, specificity</p>
<div class="math">
<p><img src="../../_images/math/6ad2e85d459119efc840a18b4c0061b410677fc4.png" alt="\frac{TN}{TN+FP}"/></p>
</div></td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">NPV - Non_IR_precision:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><p class="first">negative predictive value</p>
<div class="math">
<p><img src="../../_images/math/eb0b1eb71908d6f46febec7bb1050f9490e73a4b.png" alt="\frac{TN}{TN+FN}"/></p>
</div></td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">FPR - False_positive_rate:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><p class="first">false positive rate</p>
<div class="math">
<p><img src="../../_images/math/080ea2db9adf7594daf193b19aeeefdfe50901fb.png" alt="1-TNR = \frac{FP}{TN+FP}"/></p>
</div></td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">FNR - False_negative_rate:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><p class="first">false negative rate</p>
<div class="math">
<p><img src="../../_images/math/ee04fa10dbb2d322b4de9e51648bd3b1891db5bf.png" alt="1-TPR = \frac{FN}{TP+FN}"/></p>
</div></td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">accuracy - Percent_correct:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><p class="first">rate of correct classified examples (sometimes percent correct)</p>
<div class="math">
<p><img src="../../_images/math/a130a5d36eadd52ab719a020faebbdf66add6de2.png" alt="\frac{TP+TN}{TN+FP+FN+TP}"/></p>
</div></td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">misclassification rate - Percent_incorrect:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><p class="first">error rate, (sometimes percent incorrect)</p>
<div class="math">
<p><img src="../../_images/math/2757f995f95eed11b0e423970defe522677c1f53.png" alt="\frac{FP+FN}{TN+FP+FN+TP}"/></p>
</div></td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">F-Measure - F_measure:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><p class="first">harmonic mean of TNR and NPV</p>
<div class="math">
<p><img src="../../_images/math/f058472cc79bc2ab279d086afc1547c18e9562ae.png" alt="\frac{2 \cdot PPV \cdot TPR}{PPV+TPR}=\frac{2}{\frac{1}{PPV}+\frac{1}{TPR}}"/></p>
</div></td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">F-neg-measure - Non_IR_F_measure:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><p class="first">F-measure for negative class</p>
<div class="math">
<p><img src="../../_images/math/823ed9e11916aadd0c96a25ab639fe42fafa0b64.png" alt="\frac{2 \cdot NPV\cdot TNR}{NPV+TNR}"/></p>
</div></td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">Weighted F-measure - not implemented yet:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><div class="first math">
<p><img src="../../_images/math/6dffcef61176080b430fba79d01cfa076daacc9e.png" alt="\text{lambda } x: \frac{(1+x^2)\cdot PPV\cdot TPR}{x^2 \cdot PPV+TPR}"/></p>
</div></td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">Weighted accuracy (t) - Weighted_accuracy(t):</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><div class="first math">
<p><img src="../../_images/math/ddffd5e0fc6d194588cfc95f2fceabd35378a5eb.png" alt="t\cdot TPR + (1-t)\cdot TNR"/></p>
</div></td>
</tr>
<tr class="field-odd field"><th class="field-name">ROC-measure:</th><td class="field-body"><div class="first math">
<p><img src="../../_images/math/4a53cb285c1977a83176f2477e93686d8f47b6c3.png" alt="\sqrt{\frac{TPR^2+TNR^2}{2}}"/></p>
</div></td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">balanced accuracy - Balanced_accuracy:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><div class="first math">
<p><img src="../../_images/math/9de103df6b900117cfa8e87b685c2529f540a943.png" alt="\frac{TNR + TPR}{2}"/></p>
</div></td>
</tr>
<tr class="field-odd field"><th class="field-name">Gmean:</th><td class="field-body"><div class="first math">
<p><img src="../../_images/math/929d05128f2ea6dfc53af40a96f78778c593b980.png" alt="\sqrt{(TPR \cdot TNR)}"/></p>
</div></td>
</tr>
<tr class="field-even field"><th class="field-name">AUC:</th><td class="field-body"><p class="first">The area under the Receiver Operator Characteristic. Equal to the
Wilcoxon test of ranks or to the probability, that a classifier will
rank a randomly chosen positive instance higher than a randomly
chosen negative one.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">MCC-  Matthews_correlation_coefficient:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><div class="first last math">
<p><img src="../../_images/math/1a543d180101c6f2de873de9b7b8e19ae35867cd.png" alt="\frac{TP*TN-FP*FN}{\sqrt{((TP+FN)*(TP+FP)*(TN+FN)*(TN+FP))}}"/></p>
</div></td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>
</div></blockquote>
<dl class="docutils">
<dt><strong>K-metrics</strong></dt>
<dd><p class="first">These metrics expect classification values between zero and one.
Instead of calculating the number of correct classifications,
the corresponding sums of classification values are built.
The misclassification values we get, by using one minus c-value.
This also defines a confusion matrix, which is used to calculate the
upper metrics.</p>
<p class="last">the notation is <em>k_</em> + normal name of metric.</p>
</dd>
<dt><strong>Loss metrics</strong></dt>
<dd><p class="first">Some classifiers like LDA, SVM and RMM have loss terms in there model
description. These misclassification values can be also calculated
on test data, to evaluate the algorithm.</p>
<p>The longest name used is <em>loss_balanced_rest_L1_SVM</em>
and the shortest is <em>loss_L2</em>.</p>
<p>In the LDA case, you skip the <em>SVM</em> component.
If you want to weight the losses equally and not consider class imbalance,
skip the <em>balanced</em> component and
if you do not want to restrict the maximum loss, delete the <em>rest</em> component.</p>
<p class="last">The parameters <em>calc_loss</em> and <em>loss_restriction</em> can be specified.</p>
</dd>
</dl>
<p><strong>Parameters</strong></p>
<blockquote>
<div><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">dataset_md:</th><td class="field-body"><p class="first">The meta data of the current input</p>
<p class="last">(<em>optional, default: None</em>)</p>
</td>
</tr>
</tbody>
</table>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Author:</th><td class="field-body">Mario Krell (<a class="reference external" href="mailto:mario&#46;krell&#37;&#52;&#48;dfki&#46;de">mario<span>&#46;</span>krell<span>&#64;</span>dfki<span>&#46;</span>de</a>)</td>
</tr>
<tr class="field-even field"><th class="field-name">Created:</th><td class="field-body">2010/04/01</td>
</tr>
</tbody>
</table>
<p><strong>Class Components Summary</strong></p>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%" />
<col width="90%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.add_split" title="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.add_split"><tt class="xref py py-obj docutils literal"><span class="pre">add_split</span></tt></a>(performance,&nbsp;train[,&nbsp;split,&nbsp;run])</td>
<td>Add a split to this dataset</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.calculate_AUC" title="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.calculate_AUC"><tt class="xref py py-obj docutils literal"><span class="pre">calculate_AUC</span></tt></a>(classification_outcome,&nbsp;...[,&nbsp;...])</td>
<td>AUC and ROC points by an algorithm from Fawcett, &#8220;An introduction to ROC analysis&#8221;, 2005</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.calculate_confusion_metrics" title="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.calculate_confusion_metrics"><tt class="xref py py-obj docutils literal"><span class="pre">calculate_confusion_metrics</span></tt></a>(performance[,&nbsp;...])</td>
<td>Calculate each performance metric resulting from the 4 values in the confusion matrix and return it.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.calculate_metrics" title="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.calculate_metrics"><tt class="xref py py-obj docutils literal"><span class="pre">calculate_metrics</span></tt></a>(classification_results[,&nbsp;...])</td>
<td>Calculate performance measures from the given classifications</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.get_average_performance" title="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.get_average_performance"><tt class="xref py py-obj docutils literal"><span class="pre">get_average_performance</span></tt></a>(metric)</td>
<td>Returns the average performance for the given metric</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.get_performance_std" title="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.get_performance_std"><tt class="xref py py-obj docutils literal"><span class="pre">get_performance_std</span></tt></a>(metric)</td>
<td>Returns the average performance for the given metric</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.get_unified_confusion_matrix_performance" title="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.get_unified_confusion_matrix_performance"><tt class="xref py py-obj docutils literal"><span class="pre">get_unified_confusion_matrix_performance</span></tt></a>(metric)</td>
<td>Confusion metrics from the splits altogether</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.k_sig" title="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.k_sig"><tt class="xref py py-obj docutils literal"><span class="pre">k_sig</span></tt></a>(value[,&nbsp;decision_boundary,&nbsp;scaling])</td>
<td>Scaling as in Keerthi 2006 for smooth target function</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.merge_performance" title="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.merge_performance"><tt class="xref py py-obj docutils literal"><span class="pre">merge_performance</span></tt></a>(p_list)</td>
<td>Replace performances of different splits by just one performance value</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.merge_splits" title="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.merge_splits"><tt class="xref py py-obj docutils literal"><span class="pre">merge_splits</span></tt></a>()</td>
<td>Replace performances of different splits by just one performance value</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.mutual_information" title="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.mutual_information"><tt class="xref py py-obj docutils literal"><span class="pre">mutual_information</span></tt></a>(TN,&nbsp;FN,&nbsp;TP,&nbsp;FP)</td>
<td>Computes the mutual information metric I(T;Y) = H(T) - H(T|Y)</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.normalized_mutual_information" title="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.normalized_mutual_information"><tt class="xref py py-obj docutils literal"><span class="pre">normalized_mutual_information</span></tt></a>(TN,&nbsp;FN,&nbsp;TP,&nbsp;FP)</td>
<td>Normalized mutual information IN(T;Y) = (H(T) - H(T|Y))/H(T)</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.pol" title="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.pol"><tt class="xref py py-obj docutils literal"><span class="pre">pol</span></tt></a>(value[,&nbsp;decision_boundary])</td>
<td>Scales the prediction output to [0,1] SMOOTH with a polynomial function to show there reliability contribution in the prediction.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.scale" title="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.scale"><tt class="xref py py-obj docutils literal"><span class="pre">scale</span></tt></a>(value[,&nbsp;decision_boundary])</td>
<td>Scales the prediction output to [0,1] by simple cutting to show there reliability contribution in the prediction.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.sig" title="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.sig"><tt class="xref py py-obj docutils literal"><span class="pre">sig</span></tt></a>(value[,&nbsp;decision_boundary])</td>
<td>Scales the prediction output to [0,1] SMOOTH with a sinusoid function to show there reliability contribution in the prediction.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.store" title="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.store"><tt class="xref py py-obj docutils literal"><span class="pre">store</span></tt></a>(result_dir[,&nbsp;s_format])</td>
<td>Handle meta data and meta information and save result as csv table</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.update_confusion_matrix" title="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.update_confusion_matrix"><tt class="xref py py-obj docutils literal"><span class="pre">update_confusion_matrix</span></tt></a>(...[,&nbsp;...])</td>
<td>Calculate the change in the 4 basic metrics: TP, FP, TN, FN</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.update_loss_values" title="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.update_loss_values"><tt class="xref py py-obj docutils literal"><span class="pre">update_loss_values</span></tt></a>(classification_vector,&nbsp;label)</td>
<td>Calculate classifier loss terms on test data</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.__init__">
<tt class="descname">__init__</tt><big>(</big><em>dataset_md=None</em>, <em>dataset_pattern=None</em><big>)</big><a class="reference internal" href="../../_modules/pySPACE/resources/dataset_defs/metric.html#BinaryClassificationDataset.__init__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.__init__" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.data">
<tt class="descname">data</tt><em class="property"> = None</em><a class="headerlink" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.data" title="Permalink to this definition">¶</a></dt>
<dd><p>The data structure containing the actual data.</p>
<p>The data is stored as a dictionary that maps
(run, split, train/test) tuple to the actual
data obtained in this split in this run for
training/testing.</p>
</dd></dl>

<dl class="attribute">
<dt id="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.meta_data">
<tt class="descname">meta_data</tt><em class="property"> = None</em><a class="headerlink" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.meta_data" title="Permalink to this definition">¶</a></dt>
<dd><p>A dictionary containing some default meta data for the respective dataset</p>
</dd></dl>

<dl class="method">
<dt id="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.store">
<tt class="descname">store</tt><big>(</big><em>result_dir</em>, <em>s_format='csv'</em><big>)</big><a class="reference internal" href="../../_modules/pySPACE/resources/dataset_defs/metric.html#BinaryClassificationDataset.store"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.store" title="Permalink to this definition">¶</a></dt>
<dd><p>Handle meta data and meta information and save result as csv table</p>
<p>This table is later on merged with the other results
to one big result table.</p>
</dd></dl>

<dl class="method">
<dt id="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.add_split">
<tt class="descname">add_split</tt><big>(</big><em>performance</em>, <em>train</em>, <em>split=0</em>, <em>run=0</em><big>)</big><a class="reference internal" href="../../_modules/pySPACE/resources/dataset_defs/metric.html#BinaryClassificationDataset.add_split"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.add_split" title="Permalink to this definition">¶</a></dt>
<dd><p>Add a split to this dataset</p>
<p>The method expects the following parameters:</p>
<p><strong>Parameters</strong></p>
<blockquote>
<div><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">performance:</th><td class="field-body"><p class="first">dictionary of performance measures</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">train:</th><td class="field-body"><p class="first">If train is True, this sample has already been used for training.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">split:</th><td class="field-body"><p class="first">The number of the split this sample belongs to.</p>
<p>(<em>optional, default: 0</em>)</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">run:</th><td class="field-body"><p class="first">The run number this performance belongs to.</p>
<p class="last">(<em>optional, default: 0</em>)</p>
</td>
</tr>
</tbody>
</table>
</div></blockquote>
</dd></dl>

<dl class="method">
<dt id="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.merge_splits">
<tt class="descname">merge_splits</tt><big>(</big><big>)</big><a class="reference internal" href="../../_modules/pySPACE/resources/dataset_defs/metric.html#BinaryClassificationDataset.merge_splits"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.merge_splits" title="Permalink to this definition">¶</a></dt>
<dd><p>Replace performances of different splits by just one performance value</p>
<p>Performances of confusion matrix metrics are calculated by summing
up the confusion matrix entries.
The other metrics are averaged.</p>
<p>This method is the preparation of the merge_performance method.</p>
</dd></dl>

<dl class="method">
<dt id="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.merge_performance">
<tt class="descname">merge_performance</tt><big>(</big><em>p_list</em><big>)</big><a class="reference internal" href="../../_modules/pySPACE/resources/dataset_defs/metric.html#BinaryClassificationDataset.merge_performance"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.merge_performance" title="Permalink to this definition">¶</a></dt>
<dd><p>Replace performances of different splits by just one performance value</p>
<p>Performances of confusion matrix metrics are calculated by summing
up the confusion matrix entries.
The other metrics are averaged.</p>
</dd></dl>

<dl class="method">
<dt id="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.get_average_performance">
<tt class="descname">get_average_performance</tt><big>(</big><em>metric</em><big>)</big><a class="reference internal" href="../../_modules/pySPACE/resources/dataset_defs/metric.html#BinaryClassificationDataset.get_average_performance"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.get_average_performance" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the average performance for the given metric</p>
</dd></dl>

<dl class="method">
<dt id="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.get_performance_std">
<tt class="descname">get_performance_std</tt><big>(</big><em>metric</em><big>)</big><a class="reference internal" href="../../_modules/pySPACE/resources/dataset_defs/metric.html#BinaryClassificationDataset.get_performance_std"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.get_performance_std" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the average performance for the given metric</p>
</dd></dl>

<dl class="method">
<dt id="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.get_unified_confusion_matrix_performance">
<tt class="descname">get_unified_confusion_matrix_performance</tt><big>(</big><em>metric</em><big>)</big><a class="reference internal" href="../../_modules/pySPACE/resources/dataset_defs/metric.html#BinaryClassificationDataset.get_unified_confusion_matrix_performance"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.get_unified_confusion_matrix_performance" title="Permalink to this definition">¶</a></dt>
<dd><p>Confusion metrics from the splits altogether</p>
</dd></dl>

<dl class="staticmethod">
<dt id="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.calculate_metrics">
<em class="property">static </em><tt class="descname">calculate_metrics</tt><big>(</big><em>classification_results</em>, <em>calc_soft_metrics=True</em>, <em>invert_classification=False</em>, <em>ir_class='Target'</em>, <em>sec_class=None</em>, <em>loss_restriction=2.0</em>, <em>time_periods=</em>, <span class="optional">[</span><span class="optional">]</span><em>calc_AUC=True</em>, <em>calc_loss=True</em>, <em>weight=0.5</em>, <em>save_roc_points=False</em>, <em>decision_boundary=0.0</em><big>)</big><a class="reference internal" href="../../_modules/pySPACE/resources/dataset_defs/metric.html#BinaryClassificationDataset.calculate_metrics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.calculate_metrics" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate performance measures from the given classifications</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">metricdict and the ROC points if save_roc_point is True</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="staticmethod">
<dt id="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.update_confusion_matrix">
<em class="property">static </em><tt class="descname">update_confusion_matrix</tt><big>(</big><em>classification_vector</em>, <em>label</em>, <em>calc_soft_metrics=False</em>, <em>ir_class='Target'</em>, <em>sec_class='Standard'</em>, <em>confusion_matrix=defaultdict(&lt;type 'float'&gt;</em>, <em>{})</em>, <em>decision_boundary=0.0</em>, <em>scaling=5</em><big>)</big><a class="reference internal" href="../../_modules/pySPACE/resources/dataset_defs/metric.html#BinaryClassificationDataset.update_confusion_matrix"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.update_confusion_matrix" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the change in the 4 basic metrics: TP, FP, TN, FN</p>
<table border="1" class="docutils">
<colgroup>
<col width="61%" />
<col width="17%" />
<col width="22%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">class|guess</th>
<th class="head">ir</th>
<th class="head">sec</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>ir_class</td>
<td>TP</td>
<td>FN</td>
</tr>
<tr class="row-odd"><td>sec_class</td>
<td>FP</td>
<td>TN</td>
</tr>
</tbody>
</table>
<p>The change is directly written into the confusion matrix dictionary.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">confusion_matrix</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="staticmethod">
<dt id="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.scale">
<em class="property">static </em><tt class="descname">scale</tt><big>(</big><em>value</em>, <em>decision_boundary=0.0</em><big>)</big><a class="reference internal" href="../../_modules/pySPACE/resources/dataset_defs/metric.html#BinaryClassificationDataset.scale"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.scale" title="Permalink to this definition">¶</a></dt>
<dd><p>Scales the prediction output to [0,1] by simple cutting
to show there reliability
contribution in the prediction.</p>
</dd></dl>

<dl class="staticmethod">
<dt id="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.sig">
<em class="property">static </em><tt class="descname">sig</tt><big>(</big><em>value</em>, <em>decision_boundary=0.0</em><big>)</big><a class="reference internal" href="../../_modules/pySPACE/resources/dataset_defs/metric.html#BinaryClassificationDataset.sig"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.sig" title="Permalink to this definition">¶</a></dt>
<dd><p>Scales the prediction output to [0,1] SMOOTH with a sinusoid function
to show there reliability
contribution in the prediction.</p>
<p>Therefore it uses the sinusoid sigmoid function</p>
<div class="math">
<p><img src="../../_images/math/6273dc9af18c534eb0b62f8598a11ab33186a19e.png" alt="0.5\cdot (1-cos(value\cdot \pi))"/></p>
</div></dd></dl>

<dl class="staticmethod">
<dt id="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.pol">
<em class="property">static </em><tt class="descname">pol</tt><big>(</big><em>value</em>, <em>decision_boundary=0.0</em><big>)</big><a class="reference internal" href="../../_modules/pySPACE/resources/dataset_defs/metric.html#BinaryClassificationDataset.pol"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.pol" title="Permalink to this definition">¶</a></dt>
<dd><p>Scales the prediction output to [0,1] SMOOTH with a polynomial function
to show there reliability
contribution in the prediction.</p>
<p>Therefore it uses the polynomial sigmoid function</p>
<div class="math">
<p><img src="../../_images/math/24313055ee38f53966c5230e86b47412dcee27fc.png" alt="value^2 (3-2 \cdot value)"/></p>
</div></dd></dl>

<dl class="staticmethod">
<dt id="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.k_sig">
<em class="property">static </em><tt class="descname">k_sig</tt><big>(</big><em>value</em>, <em>decision_boundary=0.0</em>, <em>scaling=5</em><big>)</big><a class="reference internal" href="../../_modules/pySPACE/resources/dataset_defs/metric.html#BinaryClassificationDataset.k_sig"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.k_sig" title="Permalink to this definition">¶</a></dt>
<dd><p>Scaling as in Keerthi 2006 for smooth target function</p>
<p>&#8220;An efficient method for gradient-based adaptation of
hyperparameters in SVM models&#8221;
Keerthi, S. Sathiya; Sindhwani, Vikas; Chapelle, Olivier</p>
</dd></dl>

<dl class="staticmethod">
<dt id="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.update_loss_values">
<em class="property">static </em><tt class="descname">update_loss_values</tt><big>(</big><em>classification_vector</em>, <em>label</em>, <em>ir_class='Target'</em>, <em>sec_class='Standard'</em>, <em>loss_dict=defaultdict(&lt;function &lt;lambda&gt; at 0x10608f500&gt;</em>, <em>{})</em>, <em>loss_restriction=2.0</em><big>)</big><a class="reference internal" href="../../_modules/pySPACE/resources/dataset_defs/metric.html#BinaryClassificationDataset.update_loss_values"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.update_loss_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate classifier loss terms on test data</p>
<p>Different classifiers mapping the ir_class to 1 and the other
class to -1 try to minimize a loss term in the classification.
For some used loss terms of least squares classifiers and SVMs
the corresponding value is calculated as a metric to be later on used
for optimization.</p>
</dd></dl>

<dl class="staticmethod">
<dt id="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.calculate_confusion_metrics">
<em class="property">static </em><tt class="descname">calculate_confusion_metrics</tt><big>(</big><em>performance</em>, <em>pre=''</em>, <em>P=None</em>, <em>N=None</em>, <em>weight=0.5</em><big>)</big><a class="reference internal" href="../../_modules/pySPACE/resources/dataset_defs/metric.html#BinaryClassificationDataset.calculate_confusion_metrics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.calculate_confusion_metrics" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate each performance metric resulting from the 4 values in the confusion matrix and return it.</p>
<p>This helps to use soft metrics, generating the confusion matrix
in a different way.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Still the number of positive and negative instances
had to be used for the calculation of rates with soft metrics.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">metricdict</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">If the input is a metricdict the new calculated entries are added to it.</p>
</div>
</dd></dl>

<dl class="staticmethod">
<dt id="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.calculate_AUC">
<em class="property">static </em><tt class="descname">calculate_AUC</tt><big>(</big><em>classification_outcome</em>, <em>ir_class</em>, <em>save_roc_points</em>, <em>performance</em>, <em>inverse_ordering=False</em><big>)</big><a class="reference internal" href="../../_modules/pySPACE/resources/dataset_defs/metric.html#BinaryClassificationDataset.calculate_AUC"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.calculate_AUC" title="Permalink to this definition">¶</a></dt>
<dd><p>AUC and ROC points by an algorithm from Fawcett, &#8220;An introduction to ROC analysis&#8221;, 2005
Also possible would be to calculate the Mann-Whitney-U-Statistik</p>
<div class="math">
<p><img src="../../_images/math/2c2601b246b69f10044138c33e95e954cd2cffc8.png" alt="\sum_i^m{\sum_j^n{S(X_i,Y_i)}} \text{ with } S(X,Y) = 1 \text{ if } Y &lt; X\text{, otherwise } 0"/></p>
</div></dd></dl>

<dl class="staticmethod">
<dt id="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.mutual_information">
<em class="property">static </em><tt class="descname">mutual_information</tt><big>(</big><em>TN</em>, <em>FN</em>, <em>TP</em>, <em>FP</em><big>)</big><a class="reference internal" href="../../_modules/pySPACE/resources/dataset_defs/metric.html#BinaryClassificationDataset.mutual_information"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.mutual_information" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the mutual information metric I(T;Y) = H(T) - H(T|Y)</p>
<p>Measures the mutual information between the classifier output Y
and the target (the true label T), i.e. how many bits the classifier&#8217;s
output conveys about the target. H denotes the entropy function.</p>
</dd></dl>

<dl class="staticmethod">
<dt id="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.normalized_mutual_information">
<em class="property">static </em><tt class="descname">normalized_mutual_information</tt><big>(</big><em>TN</em>, <em>FN</em>, <em>TP</em>, <em>FP</em><big>)</big><a class="reference internal" href="../../_modules/pySPACE/resources/dataset_defs/metric.html#BinaryClassificationDataset.normalized_mutual_information"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset.normalized_mutual_information" title="Permalink to this definition">¶</a></dt>
<dd><p>Normalized mutual information IN(T;Y) = (H(T) - H(T|Y))/H(T)</p>
<p>This metric has the property that an optimal classifier will always get
value 1 while any kind of random classifier (those on the diagonal in ROC
space) get value 0.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="multinomialclassificationdataset">
<h3><a class="reference internal" href="#pySPACE.resources.dataset_defs.metric.MultinomialClassificationDataset" title="pySPACE.resources.dataset_defs.metric.MultinomialClassificationDataset"><tt class="xref py py-class docutils literal"><span class="pre">MultinomialClassificationDataset</span></tt></a><a class="headerlink" href="#multinomialclassificationdataset" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.resources.dataset_defs.metric.MultinomialClassificationDataset">
<em class="property">class </em><tt class="descclassname">pySPACE.resources.dataset_defs.metric.</tt><tt class="descname">MultinomialClassificationDataset</tt><big>(</big><em>dataset_md=None</em>, <em>dataset_pattern=None</em><big>)</big><a class="reference internal" href="../../_modules/pySPACE/resources/dataset_defs/metric.html#MultinomialClassificationDataset"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pySPACE.resources.dataset_defs.metric.MultinomialClassificationDataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset" title="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset</span></tt></a></p>
<p>Handle and store multiclass classification performance measures</p>
<p><strong>Metrics</strong></p>
<p>Balanced accuracy, accuracy and weighted accuracy are calculated as
in the Binary case.</p>
<blockquote>
<div><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Accuracy:</th><td class="field-body"><p class="first">Number of correct classifications devided by total
number of classified samples</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">Balanced_accuracy:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><p class="first">Mean of True positive rates for all classes</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">Weighted_accuracy:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><p class="first">Weighted sum of True positive rates for all classes,
using the <cite>weight</cite> parameter</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">Matthews_correlation_coefficient:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><p class="first">Pearson’s correlation coefficient between classification
and true label matrix.</p>
<ul class="simple">
<li>Paper: Comparing two K-category assignments by a K-category correlation coefficient</li>
<li>Author: J. Gorodkin</li>
<li>Page: 369</li>
<li>Webpage: <a class="reference external" href="http://dx.doi.org/10.1016/j.compbiolchem.2004.09.006">http://dx.doi.org/10.1016/j.compbiolchem.2004.09.006</a></li>
</ul>
</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">micro/macro_average_F_measure:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li>Paper: A Study on Threshold Selection for Multi-label Classification</li>
<li>Author: Rong-En Fan and Chih-Jen Lin</li>
<li>Page: 4</li>
</ul>
</td>
</tr>
</tbody>
</table>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Author:</th><td class="field-body">Mario Michael Krell</td>
</tr>
<tr class="field-even field"><th class="field-name">Created:</th><td class="field-body">2012/11/02</td>
</tr>
</tbody>
</table>
<p><strong>Class Components Summary</strong></p>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%" />
<col width="90%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="#pySPACE.resources.dataset_defs.metric.MultinomialClassificationDataset.calculate_confusion_metrics" title="pySPACE.resources.dataset_defs.metric.MultinomialClassificationDataset.calculate_confusion_metrics"><tt class="xref py py-obj docutils literal"><span class="pre">calculate_confusion_metrics</span></tt></a>(performance,&nbsp;classes)</td>
<td>Calculate metrics of multinomial confusion matrix</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#pySPACE.resources.dataset_defs.metric.MultinomialClassificationDataset.calculate_metrics" title="pySPACE.resources.dataset_defs.metric.MultinomialClassificationDataset.calculate_metrics"><tt class="xref py py-obj docutils literal"><span class="pre">calculate_metrics</span></tt></a>(classification_results[,&nbsp;...])</td>
<td>Calculate performance measures from the given classifications</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#pySPACE.resources.dataset_defs.metric.MultinomialClassificationDataset.update_confusion_matrix" title="pySPACE.resources.dataset_defs.metric.MultinomialClassificationDataset.update_confusion_matrix"><tt class="xref py py-obj docutils literal"><span class="pre">update_confusion_matrix</span></tt></a>(...[,&nbsp;confusion_matrix])</td>
<td>Calculate the change in the confusion matrix</td>
</tr>
</tbody>
</table>
<dl class="staticmethod">
<dt id="pySPACE.resources.dataset_defs.metric.MultinomialClassificationDataset.calculate_metrics">
<em class="property">static </em><tt class="descname">calculate_metrics</tt><big>(</big><em>classification_results</em>, <em>time_periods=</em>, <span class="optional">[</span><span class="optional">]</span><em>weight=None</em><big>)</big><a class="reference internal" href="../../_modules/pySPACE/resources/dataset_defs/metric.html#MultinomialClassificationDataset.calculate_metrics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pySPACE.resources.dataset_defs.metric.MultinomialClassificationDataset.calculate_metrics" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate performance measures from the given classifications</p>
</dd></dl>

<dl class="staticmethod">
<dt id="pySPACE.resources.dataset_defs.metric.MultinomialClassificationDataset.update_confusion_matrix">
<em class="property">static </em><tt class="descname">update_confusion_matrix</tt><big>(</big><em>classification_vector</em>, <em>label</em>, <em>confusion_matrix=defaultdict(&lt;type 'float'&gt;</em>, <em>{})</em><big>)</big><a class="reference internal" href="../../_modules/pySPACE/resources/dataset_defs/metric.html#MultinomialClassificationDataset.update_confusion_matrix"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pySPACE.resources.dataset_defs.metric.MultinomialClassificationDataset.update_confusion_matrix" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the change in the confusion matrix</p>
<table border="1" class="docutils">
<colgroup>
<col width="39%" />
<col width="31%" />
<col width="31%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">class|guess</th>
<th class="head">c1</th>
<th class="head">c2</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>c1</td>
<td>T:c1_P:c1</td>
<td>T:c1_P:c2</td>
</tr>
<tr class="row-odd"><td>c2</td>
<td>T:c2_P:c1</td>
<td>T:c2_P:c2</td>
</tr>
</tbody>
</table>
<p>The change is directly written into the confusion matrix dictionary.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">confusion_matrix</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="staticmethod">
<dt id="pySPACE.resources.dataset_defs.metric.MultinomialClassificationDataset.calculate_confusion_metrics">
<em class="property">static </em><tt class="descname">calculate_confusion_metrics</tt><big>(</big><em>performance</em>, <em>classes</em>, <em>weight=None</em><big>)</big><a class="reference internal" href="../../_modules/pySPACE/resources/dataset_defs/metric.html#MultinomialClassificationDataset.calculate_confusion_metrics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pySPACE.resources.dataset_defs.metric.MultinomialClassificationDataset.calculate_confusion_metrics" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate metrics of multinomial confusion matrix</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="regressiondataset">
<h3><a class="reference internal" href="#pySPACE.resources.dataset_defs.metric.RegressionDataset" title="pySPACE.resources.dataset_defs.metric.RegressionDataset"><tt class="xref py py-class docutils literal"><span class="pre">RegressionDataset</span></tt></a><a class="headerlink" href="#regressiondataset" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.resources.dataset_defs.metric.RegressionDataset">
<em class="property">class </em><tt class="descclassname">pySPACE.resources.dataset_defs.metric.</tt><tt class="descname">RegressionDataset</tt><big>(</big><em>dataset_md=None</em>, <em>dataset_pattern=None</em><big>)</big><a class="reference internal" href="../../_modules/pySPACE/resources/dataset_defs/metric.html#RegressionDataset"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pySPACE.resources.dataset_defs.metric.RegressionDataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset" title="pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.resources.dataset_defs.metric.BinaryClassificationDataset</span></tt></a></p>
<p>Calculate 1-dimensional and n-dimensional regression metrics</p>
<p>Metrics for 1-dim regression were taken from:</p>
<blockquote>
<div><ul class="simple">
<li>Book: Data mining: practical machine learning tools and techniques</li>
<li>Authors: I. H. Witten and E. Frank</li>
<li>Page: 178</li>
<li>Publisher: Morgan Kaufmann, San Francisco</li>
<li>year: 2005</li>
</ul>
</div></blockquote>
<p>n-dimensional metrics were variants derived by Mario Michael Krell:</p>
<p><strong>micro</strong></p>
<p>For the correlation coefficient, the components were treated
like single regression results.
For the other metrics, differences and means are taken element or
component wise and at the final averaging stage the mean is taken
over all components.</p>
<p><strong>component_i_metric</strong></p>
<p>For each dimension,
performance values are calculated separately.</p>
<p><strong>macro</strong></p>
<p>The component wise metrics were averaged.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Author:</th><td class="field-body">Mario Michael Krell</td>
</tr>
<tr class="field-even field"><th class="field-name">Created:</th><td class="field-body">2012/11/02</td>
</tr>
</tbody>
</table>
<p><strong>Class Components Summary</strong></p>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%" />
<col width="90%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="#pySPACE.resources.dataset_defs.metric.RegressionDataset.calculate_metrics" title="pySPACE.resources.dataset_defs.metric.RegressionDataset.calculate_metrics"><tt class="xref py py-obj docutils literal"><span class="pre">calculate_metrics</span></tt></a>(regression_results[,&nbsp;...])</td>
<td>Calculate performance measures from the given classifications</td>
</tr>
</tbody>
</table>
<dl class="staticmethod">
<dt id="pySPACE.resources.dataset_defs.metric.RegressionDataset.calculate_metrics">
<em class="property">static </em><tt class="descname">calculate_metrics</tt><big>(</big><em>regression_results</em>, <em>time_periods=</em>, <span class="optional">[</span><span class="optional">]</span><em>weight=None</em><big>)</big><a class="reference internal" href="../../_modules/pySPACE/resources/dataset_defs/metric.html#RegressionDataset.calculate_metrics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pySPACE.resources.dataset_defs.metric.RegressionDataset.calculate_metrics" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate performance measures from the given classifications</p>
</dd></dl>

</dd></dl>

</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="pySPACE.resources.dataset_defs.performance_result.html" title="performance_result"
             >next</a> |</li>
        <li class="right" >
          <a href="pySPACE.resources.dataset_defs.feature_vector.html" title="feature_vector"
             >previous</a> |</li>
        <li><a href="../../index.html">pySPACE 1.0 release documentation</a> &raquo;</li>
          <li><a href="../../content.html" >Table of Contents</a> &raquo;</li>
          <li><a href="pySPACE.html" >pySPACE Package</a> &raquo;</li>
          <li><a href="pySPACE.resources.html" >resources Package</a> &raquo;</li>
          <li><a href="pySPACE.resources.dataset_defs.html" >dataset_defs Package</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2013, pySPACE Developer Team.
      Last updated on Mar 27, 2014.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.2.2.
    </div>
  </body>
</html>