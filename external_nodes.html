<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Documentation of External and Wrapped Nodes &mdash; pySPACE 1.0 release documentation</title>
    
    <link rel="stylesheet" href="_static/pySPACE.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '1.0 release',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="shortcut icon" href="_static/pyspace-logo.ico"/>
    <link rel="top" title="pySPACE 1.0 release documentation" href="index.html" />
    <link rel="up" title="Table of Contents" href="content.html" />
    <link rel="prev" title="socket_utils" href="api/generated/pySPACE.tools.socket_utils.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="api/generated/pySPACE.tools.socket_utils.html" title="socket_utils"
             accesskey="P">previous</a> |</li>
        <li><a href="index.html">pySPACE 1.0 release documentation</a> &raquo;</li>
          <li><a href="content.html" accesskey="U">Table of Contents</a> &raquo;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="index.html">
              <img class="logo" src="_static/pyspace-logo_small.png" alt="Logo"/>
            </a></p>
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Documentation of External and Wrapped Nodes</a><ul>
<li><a class="reference internal" href="#scikit-nodes">Scikit Nodes</a><ul>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-adaboostclassifiersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.AdaBoostClassifierSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-bernoullinbsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.BernoulliNBSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-binarizersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.BinarizerSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-countvectorizersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.CountVectorizerSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-decisiontreeclassifiersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.DecisionTreeClassifierSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-decisiontreeregressorsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.DecisionTreeRegressorSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-dictvectorizersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.DictVectorizerSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-dictionarylearningsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.DictionaryLearningSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-extratreeclassifiersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.ExtraTreeClassifierSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-extratreeregressorsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.ExtraTreeRegressorSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-extratreesclassifiersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.ExtraTreesClassifierSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-extratreesregressorsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.ExtraTreesRegressorSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-factoranalysissklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.FactorAnalysisSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-featurehashersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.FeatureHasherSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-featureunionsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.FeatureUnionSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-forestregressorsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.ForestRegressorSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-gaussiannbsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.GaussianNBSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-genericunivariateselectsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.GenericUnivariateSelectSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-gradientboostingclassifiersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.GradientBoostingClassifierSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-gridsearchcvsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.GridSearchCVSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-hashingvectorizersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.HashingVectorizerSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-imputersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.ImputerSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-isomapsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.IsomapSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-isotonicregressionsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.IsotonicRegressionSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-kneighborsclassifiersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.KNeighborsClassifierSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-kernelcenterersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.KernelCentererSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-kernelpcasklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.KernelPCASklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-ldasklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.LDASklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-labelbinarizersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.LabelBinarizerSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-labelencodersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.LabelEncoderSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-labelpropagationsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.LabelPropagationSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-labelspreadingsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.LabelSpreadingSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-linearsvcsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.LinearSVCSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-locallylinearembeddingsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.LocallyLinearEmbeddingSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-logisticregressionsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.LogisticRegressionSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-minmaxscalersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.MinMaxScalerSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-minibatchdictionarylearningsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.MiniBatchDictionaryLearningSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-minibatchsparsepcasklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.MiniBatchSparsePCASklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-multinomialnbsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.MultinomialNBSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-nmfsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.NMFSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-nearestcentroidsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.NearestCentroidSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-normalizersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.NormalizerSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-nusvcsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.NuSVCSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-onehotencodersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.OneHotEncoderSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-pcasklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.PCASklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-passiveaggressiveclassifiersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.PassiveAggressiveClassifierSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-patchextractorsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.PatchExtractorSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-perceptronsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.PerceptronSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-pipelinesklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.PipelineSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-probabilisticpcasklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.ProbabilisticPCASklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-projectedgradientnmfsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.ProjectedGradientNMFSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-qdasklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.QDASklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-rfecvsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RFECVSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-rfesklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RFESklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-radiusneighborsclassifiersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RadiusNeighborsClassifierSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-randomforestclassifiersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RandomForestClassifierSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-randomforestregressorsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RandomForestRegressorSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-randomtreesembeddingsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RandomTreesEmbeddingSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-randomizedlassosklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RandomizedLassoSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-randomizedlogisticregressionsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RandomizedLogisticRegressionSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-randomizedpcasklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RandomizedPCASklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-randomizedsearchcvsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RandomizedSearchCVSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-ridgeclassifiercvsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RidgeClassifierCVSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-ridgeclassifiersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RidgeClassifierSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-sgdclassifiersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.SGDClassifierSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-sgdregressorsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.SGDRegressorSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-svcsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.SVCSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-scalersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.ScalerSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-selectfdrsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.SelectFdrSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-selectfprsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.SelectFprSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-selectfwesklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.SelectFweSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-selectkbestsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.SelectKBestSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-selectpercentilesklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.SelectPercentileSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-sparsecodersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.SparseCoderSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-sparsepcasklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.SparsePCASklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-standardscalersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.StandardScalerSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-tfidftransformersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.TfidfTransformerSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-tfidfvectorizersklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.TfidfVectorizerSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-truncatedsvdsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.TruncatedSVDSklearnNode</span></tt></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikits-nodes-wardagglomerationsklearnnode"><tt class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.WardAgglomerationSklearnNode</span></tt></a></li>
</ul>
</li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="api/generated/pySPACE.tools.socket_utils.html"
                        title="previous chapter">socket_utils</a></p>
  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="_sources/external_nodes.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="documentation-of-external-and-wrapped-nodes">
<span id="external-nodes"></span><h1>Documentation of External and Wrapped Nodes<a class="headerlink" href="#documentation-of-external-and-wrapped-nodes" title="Permalink to this headline">¶</a></h1>
<p>pySPACE comes along with wrappers to external algorithms.</p>
<p>For details on the usage of the nodes and for getting usage examples,
have a look at their documentation.
Module for external node wrapping: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.external.html#module-pySPACE.missions.nodes.external" title="pySPACE.missions.nodes.external"><tt class="xref py py-mod docutils literal"><span class="pre">pySPACE.missions.nodes.external</span></tt></a></p>
<div class="section" id="scikit-nodes">
<span id="id1"></span><h2>Scikit Nodes<a class="headerlink" href="#scikit-nodes" title="Permalink to this headline">¶</a></h2>
<p>Nodes from <a class="reference internal" href="api/generated/pySPACE.missions.nodes.scikits_nodes.html#module-pySPACE.missions.nodes.scikits_nodes" title="pySPACE.missions.nodes.scikits_nodes"><tt class="xref py py-mod docutils literal"><span class="pre">scikits</span> <span class="pre">wrapper</span></tt></a></p>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-adaboostclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.AdaBoostClassifierSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.AdaBoostClassifierSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.AdaBoostClassifierSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-adaboostclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.AdaBoostClassifierSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">AdaBoostClassifierSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.AdaBoostClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>An AdaBoost classifier.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.ensemble.weight_boosting.AdaBoostClassifier</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>An AdaBoost [1] classifier is a meta-estimator that begins by fitting a
classifier on the original dataset and then fits additional copies of the
classifier on the same dataset but where the weights of incorrectly
classified instances are adjusted such that subsequent classifiers focus
more on difficult cases.</p>
<p>This class implements the algorithm known as AdaBoost-SAMME [2].</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>base_estimator <span class="classifier-delimiter">:</span> <span class="classifier">object, optional (default=DecisionTreeClassifier)</span></dt>
<dd>The base estimator from which the boosted ensemble is built.
Support for sample weighting is required, as well as proper <cite>classes_</cite>
and <cite>n_classes_</cite> attributes.</dd>
<dt>n_estimators <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=50)</span></dt>
<dd>The maximum number of estimators at which boosting is terminated.
In case of perfect fit, the learning procedure is stopped early.</dd>
<dt>learning_rate <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1.)</span></dt>
<dd>Learning rate shrinks the contribution of each classifier by
<tt class="docutils literal"><span class="pre">learning_rate</span></tt>. There is a trade-off between <tt class="docutils literal"><span class="pre">learning_rate</span></tt> and
<tt class="docutils literal"><span class="pre">n_estimators</span></tt>.</dd>
<dt>algorithm <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;SAMME&#8217;, &#8216;SAMME.R&#8217;}, optional (default=&#8217;SAMME.R&#8217;)</span></dt>
<dd>If &#8216;SAMME.R&#8217; then use the SAMME.R real boosting algorithm.
<tt class="docutils literal"><span class="pre">base_estimator</span></tt> must support calculation of class probabilities.
If &#8216;SAMME&#8217; then use the SAMME discrete boosting algorithm.
The SAMME.R algorithm typically converges faster than SAMME,
achieving a lower test error with fewer boosting iterations.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>estimators_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">list of classifiers</span></dt>
<dd>The collection of fitted sub-estimators.</dd>
<dt><cite>classes_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_classes]</span></dt>
<dd>The classes labels.</dd>
<dt><cite>n_classes_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>The number of classes.</dd>
<dt><cite>estimator_weights_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of floats</span></dt>
<dd>Weights for each estimator in the boosted ensemble.</dd>
<dt><cite>estimator_errors_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of floats</span></dt>
<dd>Classification error for each estimator in the boosted
ensemble.</dd>
<dt><cite>feature_importances_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_features]</span></dt>
<dd>The feature importances if supported by the <tt class="docutils literal"><span class="pre">base_estimator</span></tt>.</dd>
</dl>
<p>See also</p>
<p>AdaBoostRegressor, GradientBoostingClassifier, DecisionTreeClassifier</p>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>Y. Freund, R. Schapire, &#8220;A Decision-Theoretic Generalization of
on-Line Learning and an Application to Boosting&#8221;, 1995.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td><ol class="first last upperalpha simple" start="10">
<li>Zhu, H. Zou, S. Rosset, T. Hastie, &#8220;Multi-class AdaBoost&#8221;, 2009.</li>
</ol>
</td></tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>AdaBoostClassifierSklearnNode</strong></li>
<li><strong>AdaBoostClassifierSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-bernoullinbsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.BernoulliNBSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.BernoulliNBSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.BernoulliNBSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-bernoullinbsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.BernoulliNBSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">BernoulliNBSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.BernoulliNBSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Naive Bayes classifier for multivariate Bernoulli models.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.naive_bayes.BernoulliNB</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Like MultinomialNB, this classifier is suitable for discrete data. The
difference is that while MultinomialNB works with occurrence counts,
BernoulliNB is designed for binary/boolean features.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1.0)</span></dt>
<dd>Additive (Laplace/Lidstone) smoothing parameter
(0 for no smoothing).</dd>
<dt>binarize <span class="classifier-delimiter">:</span> <span class="classifier">float or None, optional</span></dt>
<dd>Threshold for binarizing (mapping to booleans) of sample features.
If None, input is presumed to already consist of binary vectors.</dd>
<dt>fit_prior <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd>Whether to learn class prior probabilities or not.
If false, a uniform prior will be used.</dd>
<dt>class_prior <span class="classifier-delimiter">:</span> <span class="classifier">array-like, size=[n_classes,]</span></dt>
<dd>Prior probabilities of the classes. If specified the priors are not
adjusted according to the data.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>class_log_prior_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_classes]</span></dt>
<dd>Log probability of each class (smoothed).</dd>
<dt><cite>feature_log_prob_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_classes, n_features]</span></dt>
<dd>Empirical log probability of features given a class, P(x_i|y).</dd>
<dt><cite>class_count_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_classes]</span></dt>
<dd>Number of samples encountered for each class during fitting. This
value is weighted by the sample weight when provided.</dd>
<dt><cite>feature_count_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_classes, n_features]</span></dt>
<dd>Number of samples encountered for each (class, feature)
during fitting. This value is weighted by the sample weight when
provided.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">BernoulliNB</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">BernoulliNB</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="go">BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
<span class="go">[3]</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<p>C.D. Manning, P. Raghavan and H. Schütze (2008). Introduction to
Information Retrieval. Cambridge University Press, pp. 234–265.
<a class="reference external" href="http://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html">http://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html</a></p>
<p>A. McCallum and K. Nigam (1998). A comparison of event models for naive
Bayes text classification. Proc. AAAI/ICML-98 Workshop on Learning for
Text Categorization, pp. 41–48.</p>
<p>V. Metsis, I. Androutsopoulos and G. Paliouras (2006). Spam filtering with
naive Bayes &#8211; Which naive Bayes? 3rd Conf. on Email and Anti-Spam (CEAS).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>BernoulliNBSklearn</strong></li>
<li><strong>BernoulliNBSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-binarizersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.BinarizerSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.BinarizerSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.BinarizerSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-binarizersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.BinarizerSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">BinarizerSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.BinarizerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Binarize data (set feature values to 0 or 1) according to a threshold</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.preprocessing.data.Binarizer</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Values greater than the threshold map to 1, while values less than
or equal to the threshold map to 0. With the default threshold of 0,
only positive values map to 1.</p>
<p>Binarization is a common operation on text count data where the
analyst can decide to only consider the presence or absence of a
feature rather than a quantified number of occurrences for instance.</p>
<p>It can also be used as a pre-processing step for estimators that
consider boolean random variables (e.g. modelled using the Bernoulli
distribution in a Bayesian setting).</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>threshold <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (0.0 by default)</span></dt>
<dd>Feature values below or equal to this are replaced by 0, above it by 1.
Threshold may not be less than 0 for operations on sparse matrices.</dd>
<dt>copy <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default is True</span></dt>
<dd>set to False to perform inplace binarization and avoid a copy (if
the input is already a numpy array or a scipy.sparse CSR matrix).</dd>
</dl>
<p><strong>Notes</strong></p>
<p>If the input is a sparse matrix, only the non-zero values are subject
to update by the Binarizer class.</p>
<p>This estimator is stateless (besides constructor parameters), the
fit method does nothing but is useful when used in a pipeline.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>BinarizerSklearn</strong></li>
<li><strong>BinarizerSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-countvectorizersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.CountVectorizerSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.CountVectorizerSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.CountVectorizerSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-countvectorizersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.CountVectorizerSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">CountVectorizerSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.CountVectorizerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Convert a collection of text documents to a matrix of token counts</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.feature_extraction.text.CountVectorizer</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>This implementation produces a sparse representation of the counts using
scipy.sparse.coo_matrix.</p>
<p>If you do not provide an a-priori dictionary and you do not use an analyzer
that does some kind of feature selection then the number of features will
be equal to the vocabulary size found by analyzing the data.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>input <span class="classifier-delimiter">:</span> <span class="classifier">string {&#8216;filename&#8217;, &#8216;file&#8217;, &#8216;content&#8217;}</span></dt>
<dd><p class="first">If filename, the sequence passed as an argument to fit is
expected to be a list of filenames that need reading to fetch
the raw content to analyze.</p>
<p>If &#8216;file&#8217;, the sequence items must have &#8216;read&#8217; method (file-like
object) it is called to fetch the bytes in memory.</p>
<p class="last">Otherwise the input is expected to be the sequence strings or
bytes items are expected to be analyzed directly.</p>
</dd>
<dt>encoding <span class="classifier-delimiter">:</span> <span class="classifier">string, &#8216;utf-8&#8217; by default.</span></dt>
<dd>If bytes or files are given to analyze, this encoding is used to
decode.</dd>
<dt>decode_error <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;strict&#8217;, &#8216;ignore&#8217;, &#8216;replace&#8217;}</span></dt>
<dd>Instruction on what to do if a byte sequence is given to analyze that
contains characters not of the given <cite>encoding</cite>. By default, it is
&#8216;strict&#8217;, meaning that a UnicodeDecodeError will be raised. Other
values are &#8216;ignore&#8217; and &#8216;replace&#8217;.</dd>
<dt>strip_accents <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;ascii&#8217;, &#8216;unicode&#8217;, None}</span></dt>
<dd>Remove accents during the preprocessing step.
&#8216;ascii&#8217; is a fast method that only works on characters that have
an direct ASCII mapping.
&#8216;unicode&#8217; is a slightly slower method that works on any characters.
None (default) does nothing.</dd>
<dt>analyzer <span class="classifier-delimiter">:</span> <span class="classifier">string, {&#8216;word&#8217;, &#8216;char&#8217;, &#8216;char_wb&#8217;} or callable</span></dt>
<dd><p class="first">Whether the feature should be made of word or character n-grams.
Option &#8216;char_wb&#8217; creates character n-grams only from text inside
word boundaries.</p>
<p class="last">If a callable is passed it is used to extract the sequence of features
out of the raw, unprocessed input.</p>
</dd>
<dt>preprocessor <span class="classifier-delimiter">:</span> <span class="classifier">callable or None (default)</span></dt>
<dd>Override the preprocessing (string transformation) stage while
preserving the tokenizing and n-grams generation steps.</dd>
<dt>tokenizer <span class="classifier-delimiter">:</span> <span class="classifier">callable or None (default)</span></dt>
<dd>Override the string tokenization step while preserving the
preprocessing and n-grams generation steps.</dd>
<dt>ngram_range <span class="classifier-delimiter">:</span> <span class="classifier">tuple (min_n, max_n)</span></dt>
<dd>The lower and upper boundary of the range of n-values for different
n-grams to be extracted. All values of n such that min_n &lt;= n &lt;= max_n
will be used.</dd>
<dt>stop_words <span class="classifier-delimiter">:</span> <span class="classifier">string {&#8216;english&#8217;}, list, or None (default)</span></dt>
<dd><p class="first">If a string, it is passed to _check_stop_list and the appropriate stop
list is returned. &#8216;english&#8217; is currently the only supported string
value.</p>
<p>If a list, that list is assumed to contain stop words, all of which
will be removed from the resulting tokens.</p>
<p class="last">If None, no stop words will be used. max_df can be set to a value
in the range [0.7, 1.0) to automatically detect and filter stop
words based on intra corpus document frequency of terms.</p>
</dd>
<dt>lowercase <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default True</span></dt>
<dd>Convert all characters to lowercase befor tokenizing.</dd>
<dt>token_pattern <span class="classifier-delimiter">:</span> <span class="classifier">string</span></dt>
<dd>Regular expression denoting what constitutes a &#8220;token&#8221;, only used
if <cite>tokenize == &#8216;word&#8217;</cite>. The default regexp select tokens of 2
or more letters characters (punctuation is completely ignored
and always treated as a token separator).</dd>
<dt>max_df <span class="classifier-delimiter">:</span> <span class="classifier">float in range [0.0, 1.0] or int, optional, 1.0 by default</span></dt>
<dd>When building the vocabulary ignore terms that have a term frequency
strictly higher than the given threshold (corpus specific stop words).
If float, the parameter represents a proportion of documents, integer
absolute counts.
This parameter is ignored if vocabulary is not None.</dd>
<dt>min_df <span class="classifier-delimiter">:</span> <span class="classifier">float in range [0.0, 1.0] or int, optional, 1 by default</span></dt>
<dd>When building the vocabulary ignore terms that have a term frequency
strictly lower than the given threshold. This value is also called
cut-off in the literature.
If float, the parameter represents a proportion of documents, integer
absolute counts.
This parameter is ignored if vocabulary is not None.</dd>
<dt>max_features <span class="classifier-delimiter">:</span> <span class="classifier">optional, None by default</span></dt>
<dd><p class="first">If not None, build a vocabulary that only consider the top
max_features ordered by term frequency across the corpus.</p>
<p class="last">This parameter is ignored if vocabulary is not None.</p>
</dd>
<dt>vocabulary <span class="classifier-delimiter">:</span> <span class="classifier">Mapping or iterable, optional</span></dt>
<dd>Either a Mapping (e.g., a dict) where keys are terms and values are
indices in the feature matrix, or an iterable over terms. If not
given, a vocabulary is determined from the input documents.</dd>
<dt>binary <span class="classifier-delimiter">:</span> <span class="classifier">boolean, False by default.</span></dt>
<dd>If True, all non zero counts are set to 1. This is useful for discrete
probabilistic models that model binary events rather than integer
counts.</dd>
<dt>dtype <span class="classifier-delimiter">:</span> <span class="classifier">type, optional</span></dt>
<dd>Type of the matrix returned by fit_transform() or transform().</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>vocabulary_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">dict</span></dt>
<dd>A mapping of terms to feature indices.</dd>
<dt><cite>stop_words_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">set</span></dt>
<dd>Terms that were ignored because
they occurred in either too many
(<cite>max_df</cite>) or in too few (<cite>min_df</cite>) documents.
This is only available if no vocabulary was given.</dd>
</dl>
<p>See also</p>
<p>HashingVectorizer, TfidfVectorizer</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>CountVectorizerSklearn</strong></li>
<li><strong>CountVectorizerSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-decisiontreeclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.DecisionTreeClassifierSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.DecisionTreeClassifierSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.DecisionTreeClassifierSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-decisiontreeclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.DecisionTreeClassifierSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">DecisionTreeClassifierSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.DecisionTreeClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>A decision tree classifier.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.tree.tree.DecisionTreeClassifier</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>criterion <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=&#8221;gini&#8221;)</span></dt>
<dd>The function to measure the quality of a split. Supported criteria are
&#8220;gini&#8221; for the Gini impurity and &#8220;entropy&#8221; for the information gain.</dd>
<dt>max_features <span class="classifier-delimiter">:</span> <span class="classifier">int, float, string or None, optional (default=None)</span></dt>
<dd><p class="first">The number of features to consider when looking for the best split:</p>
<blockquote class="last">
<div><ul class="simple">
<li><ul class="first">
<li>If int, then consider <cite>max_features</cite> features at each split.</li>
</ul>
</li>
<li><ul class="first">
<li>If float, then <cite>max_features</cite> is a percentage and</li>
</ul>
</li>
<li><cite>int(max_features * n_features)</cite> features are considered at each</li>
<li>split.</li>
<li><ul class="first">
<li>If &#8220;auto&#8221;, then <cite>max_features=sqrt(n_features)</cite>.</li>
</ul>
</li>
<li><ul class="first">
<li>If &#8220;sqrt&#8221;, then <cite>max_features=sqrt(n_features)</cite>.</li>
</ul>
</li>
<li><ul class="first">
<li>If &#8220;log2&#8221;, then <cite>max_features=log2(n_features)</cite>.</li>
</ul>
</li>
<li><ul class="first">
<li>If None, then <cite>max_features=n_features</cite>.</li>
</ul>
</li>
</ul>
</div></blockquote>
</dd>
<dt>max_depth <span class="classifier-delimiter">:</span> <span class="classifier">integer or None, optional (default=None)</span></dt>
<dd>The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.</dd>
<dt>min_samples_split <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=2)</span></dt>
<dd>The minimum number of samples required to split an internal node.</dd>
<dt>min_samples_leaf <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=1)</span></dt>
<dd>The minimum number of samples required to be at a leaf node.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>tree_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">Tree object</span></dt>
<dd>The underlying Tree object.</dd>
<dt><cite>classes_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_classes] or a list of such arrays</span></dt>
<dd>The classes labels (single output problem),
or a list of arrays of class labels (multi-output problem).</dd>
<dt><cite>n_classes_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">int or list</span></dt>
<dd>The number of classes (for single output problems),
or a list containing the number of classes for each
output (for multi-output problems).</dd>
<dt><cite>feature_importances_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_features]</span></dt>
<dd>The feature importances. The higher, the more important the
feature. The importance of a feature is computed as the (normalized)
total reduction of the criterion brought by that feature.  It is also
known as the Gini importance <a href="#id36"><span class="problematic" id="id37"><span id="id4"></span>[4]_</span></a>.</dd>
</dl>
<p>See also</p>
<p>DecisionTreeRegressor</p>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id5" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><a class="reference external" href="http://en.wikipedia.org/wiki/Decision_tree_learning">http://en.wikipedia.org/wiki/Decision_tree_learning</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id6" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td>L. Breiman, J. Friedman, R. Olshen, and C. Stone, &#8220;Classification
and Regression Trees&#8221;, Wadsworth, Belmont, CA, 1984.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id7" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[3]</td><td>T. Hastie, R. Tibshirani and J. Friedman. &#8220;Elements of Statistical
Learning&#8221;, Springer, 2009.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id8" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[4]</td><td>L. Breiman, and A. Cutler, &#8220;Random Forests&#8221;,
<a class="reference external" href="http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm">http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm</a></td></tr>
</tbody>
</table>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">... </span>                            
<span class="gp">...</span>
<span class="go">array([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,</span>
<span class="go">        0.93...,  0.93...,  1.     ,  0.93...,  1.      ])</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>DecisionTreeClassifierSklearnNode</strong></li>
<li><strong>DecisionTreeClassifierSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-decisiontreeregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.DecisionTreeRegressorSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.DecisionTreeRegressorSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.DecisionTreeRegressorSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-decisiontreeregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.DecisionTreeRegressorSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">DecisionTreeRegressorSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.DecisionTreeRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>A tree regressor.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.tree.tree.DecisionTreeRegressor</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>criterion <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=&#8221;mse&#8221;)</span></dt>
<dd>The function to measure the quality of a split. The only supported
criterion is &#8220;mse&#8221; for the mean squared error.</dd>
<dt>max_features <span class="classifier-delimiter">:</span> <span class="classifier">int, float, string or None, optional (default=None)</span></dt>
<dd><p class="first">The number of features to consider when looking for the best split:</p>
<blockquote class="last">
<div><ul class="simple">
<li><ul class="first">
<li>If int, then consider <cite>max_features</cite> features at each split.</li>
</ul>
</li>
<li><ul class="first">
<li>If float, then <cite>max_features</cite> is a percentage and</li>
</ul>
</li>
<li><cite>int(max_features * n_features)</cite> features are considered at each</li>
<li>split.</li>
<li><ul class="first">
<li>If &#8220;auto&#8221;, then <cite>max_features=n_features</cite>.</li>
</ul>
</li>
<li><ul class="first">
<li>If &#8220;sqrt&#8221;, then <cite>max_features=sqrt(n_features)</cite>.</li>
</ul>
</li>
<li><ul class="first">
<li>If &#8220;log2&#8221;, then <cite>max_features=log2(n_features)</cite>.</li>
</ul>
</li>
<li><ul class="first">
<li>If None, then <cite>max_features=n_features</cite>.</li>
</ul>
</li>
</ul>
</div></blockquote>
</dd>
<dt>max_depth <span class="classifier-delimiter">:</span> <span class="classifier">integer or None, optional (default=None)</span></dt>
<dd>The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.</dd>
<dt>min_samples_split <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=2)</span></dt>
<dd>The minimum number of samples required to split an internal node.</dd>
<dt>min_samples_leaf <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=1)</span></dt>
<dd>The minimum number of samples required to be at a leaf node.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>tree_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">Tree object</span></dt>
<dd>The underlying Tree object.</dd>
<dt><cite>feature_importances_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_features]</span></dt>
<dd>The feature importances.
The higher, the more important the feature.
The importance of a feature is computed as the
(normalized) total reduction of the criterion brought
by that feature. It is also known as the Gini importance <a href="#id38"><span class="problematic" id="id39"><span id="id9"></span>[4]_</span></a>.</dd>
</dl>
<p>See also</p>
<p>DecisionTreeClassifier</p>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id10" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><a class="reference external" href="http://en.wikipedia.org/wiki/Decision_tree_learning">http://en.wikipedia.org/wiki/Decision_tree_learning</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id11" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td>L. Breiman, J. Friedman, R. Olshen, and C. Stone, &#8220;Classification
and Regression Trees&#8221;, Wadsworth, Belmont, CA, 1984.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id12" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[3]</td><td>T. Hastie, R. Tibshirani and J. Friedman. &#8220;Elements of Statistical
Learning&#8221;, Springer, 2009.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id13" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[4]</td><td>L. Breiman, and A. Cutler, &#8220;Random Forests&#8221;,
<a class="reference external" href="http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm">http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm</a></td></tr>
</tbody>
</table>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_boston</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">boston</span> <span class="o">=</span> <span class="n">load_boston</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regressor</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">regressor</span><span class="p">,</span> <span class="n">boston</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">boston</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">... </span>                   
<span class="gp">...</span>
<span class="go">array([ 0.61..., 0.57..., -0.34..., 0.41..., 0.75...,</span>
<span class="go">        0.07..., 0.29..., 0.33..., -1.42..., -1.77...])</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>DecisionTreeRegressorSklearn</strong></li>
<li><strong>DecisionTreeRegressorSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-dictvectorizersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.DictVectorizerSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.DictVectorizerSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.DictVectorizerSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-dictvectorizersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.DictVectorizerSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">DictVectorizerSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.DictVectorizerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Transforms lists of feature-value mappings to vectors.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.feature_extraction.dict_vectorizer.DictVectorizer</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>This transformer turns lists of mappings (dict-like objects) of feature
names to feature values into Numpy arrays or scipy.sparse matrices for use
with scikit-learn estimators.</p>
<p>When feature values are strings, this transformer will do a binary one-hot
(aka one-of-K) coding: one boolean-valued feature is constructed for each
of the possible string values that the feature can take on. For instance,
a feature &#8220;f&#8221; that can take on the values &#8220;ham&#8221; and &#8220;spam&#8221; will become two
features in the output, one signifying &#8220;f=ham&#8221;, the other &#8220;f=spam&#8221;.</p>
<p>Features that do not occur in a sample (mapping) will have a zero value
in the resulting array/matrix.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>dtype <span class="classifier-delimiter">:</span> <span class="classifier">callable, optional</span></dt>
<dd>The type of feature values. Passed to Numpy array/scipy.sparse matrix
constructors as the dtype argument.</dd>
<dt>separator: string, optional</dt>
<dd>Separator string used when constructing new features for one-hot
coding.</dd>
<dt>sparse: boolean, optional.</dt>
<dd>Whether transform should produce scipy.sparse matrices.
True by default.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>feature_names_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">list</span></dt>
<dd>A list of length n_features containing the feature names (e.g., &#8220;f=ham&#8221;
and &#8220;f=spam&#8221;).</dd>
<dt><cite>vocabulary_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">dict</span></dt>
<dd>A dictionary mapping feature names to feature indices.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction</span> <span class="kn">import</span> <span class="n">DictVectorizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">DictVectorizer</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">D</span> <span class="o">=</span> <span class="p">[{</span><span class="s">&#39;foo&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s">&#39;bar&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">},</span> <span class="p">{</span><span class="s">&#39;foo&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s">&#39;baz&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span>
<span class="go">array([[ 2.,  0.,  1.],</span>
<span class="go">       [ 0.,  1.,  3.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">==</span>         <span class="p">[{</span><span class="s">&#39;bar&#39;</span><span class="p">:</span> <span class="mf">2.0</span><span class="p">,</span> <span class="s">&#39;foo&#39;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">},</span> <span class="p">{</span><span class="s">&#39;baz&#39;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s">&#39;foo&#39;</span><span class="p">:</span> <span class="mf">3.0</span><span class="p">}]</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span><span class="o">.</span><span class="n">transform</span><span class="p">({</span><span class="s">&#39;foo&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s">&#39;unseen_feature&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">})</span>
<span class="go">array([[ 0.,  0.,  4.]])</span>
</pre></div>
</div>
<p>See also</p>
<dl class="docutils">
<dt>DictVectorizer <span class="classifier-delimiter">:</span> <span class="classifier">performs vectorization in a similar as this class,</span></dt>
<dd>but using a hash table instead of only a hash function.</dd>
<dt>sklearn.preprocessing.OneHotEncoder <span class="classifier-delimiter">:</span> <span class="classifier">handles nominal/categorical features</span></dt>
<dd>encoded as columns of integers.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>DictVectorizerSklearn</strong></li>
<li><strong>DictVectorizerSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-dictionarylearningsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.DictionaryLearningSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.DictionaryLearningSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.DictionaryLearningSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-dictionarylearningsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.DictionaryLearningSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">DictionaryLearningSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.DictionaryLearningSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Dictionary learning</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.decomposition.dict_learning.DictionaryLearning</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Finds a dictionary (a set of atoms) that can best be used to represent data
using a sparse code.</p>
<p>Solves the optimization problem:</p>
<div class="highlight-python"><div class="highlight"><pre>(U^*,V^*) = argmin 0.5 || Y - U V ||_2^2 + alpha * || U ||_1
            (U,V)
            with || V_k ||_2 = 1 for all  0 &lt;= k &lt; n_components
</pre></div>
</div>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>number of dictionary elements to extract</dd>
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>sparsity controlling parameter</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>maximum number of iterations to perform</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float,</span></dt>
<dd>tolerance for numerical error</dd>
<dt>fit_algorithm <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;lars&#8217;, &#8216;cd&#8217;}</span></dt>
<dd>lars: uses the least angle regression method to solve the lasso problem
(linear_model.lars_path)
cd: uses the coordinate descent method to compute the
Lasso solution (linear_model.Lasso). Lars will be faster if
the estimated components are sparse.</dd>
<dt>transform_algorithm <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;lasso_lars&#8217;, &#8216;lasso_cd&#8217;, &#8216;lars&#8217;, &#8216;omp&#8217;,     &#8216;threshold&#8217;}</span></dt>
<dd>Algorithm used to transform the data
lars: uses the least angle regression method (linear_model.lars_path)
lasso_lars: uses Lars to compute the Lasso solution
lasso_cd: uses the coordinate descent method to compute the
Lasso solution (linear_model.Lasso). lasso_lars will be faster if
the estimated components are sparse.
omp: uses orthogonal matching pursuit to estimate the sparse solution
threshold: squashes to zero all coefficients less than alpha from
the projection <tt class="docutils literal"><span class="pre">dictionary</span> <span class="pre">*</span> <span class="pre">X'</span></tt></dd>
<dt>transform_n_nonzero_coefs <span class="classifier-delimiter">:</span> <span class="classifier">int, <tt class="docutils literal"><span class="pre">0.1</span> <span class="pre">*</span> <span class="pre">n_features</span></tt> by default</span></dt>
<dd>Number of nonzero coefficients to target in each column of the
solution. This is only used by <cite>algorithm=&#8217;lars&#8217;</cite> and <cite>algorithm=&#8217;omp&#8217;</cite>
and is overridden by <cite>alpha</cite> in the <cite>omp</cite> case.</dd>
<dt>transform_alpha <span class="classifier-delimiter">:</span> <span class="classifier">float, 1. by default</span></dt>
<dd>If <cite>algorithm=&#8217;lasso_lars&#8217;</cite> or <cite>algorithm=&#8217;lasso_cd&#8217;</cite>, <cite>alpha</cite> is the
penalty applied to the L1 norm.
If <cite>algorithm=&#8217;threshold&#8217;</cite>, <cite>alpha</cite> is the absolute value of the
threshold below which coefficients will be squashed to zero.
If <cite>algorithm=&#8217;omp&#8217;</cite>, <cite>alpha</cite> is the tolerance parameter: the value of
the reconstruction error targeted. In this case, it overrides
<cite>n_nonzero_coefs</cite>.</dd>
<dt>split_sign <span class="classifier-delimiter">:</span> <span class="classifier">bool, False by default</span></dt>
<dd>Whether to split the sparse feature vector into the concatenation of
its negative part and its positive part. This can improve the
performance of downstream classifiers.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>number of parallel jobs to run</dd>
<dt>code_init <span class="classifier-delimiter">:</span> <span class="classifier">array of shape (n_samples, n_components),</span></dt>
<dd>initial value for the code, for warm restart</dd>
<dt>dict_init <span class="classifier-delimiter">:</span> <span class="classifier">array of shape (n_components, n_features),</span></dt>
<dd>initial values for the dictionary, for warm restart</dd>
</dl>
<p>verbose :</p>
<blockquote>
<div><ul class="simple">
<li>degree of verbosity of the printed output</li>
</ul>
</div></blockquote>
<dl class="docutils">
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int or RandomState</span></dt>
<dd>Pseudo number generator state used for random sampling.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>components_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span></dt>
<dd>dictionary atoms extracted from the data</dd>
<dt><cite>error_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array</span></dt>
<dd>vector of errors at each iteration</dd>
</dl>
<p><strong>Notes</strong></p>
<p><strong>References:</strong></p>
<p>J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning
for sparse coding (<a class="reference external" href="http://www.di.ens.fr/sierra/pdfs/icml09.pdf">http://www.di.ens.fr/sierra/pdfs/icml09.pdf</a>)</p>
<p>See also</p>
<p>SparseCoder
MiniBatchDictionaryLearning
SparsePCA
MiniBatchSparsePCA</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>DictionaryLearningSklearnNode</strong></li>
<li><strong>DictionaryLearningSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-extratreeclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.ExtraTreeClassifierSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.ExtraTreeClassifierSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.ExtraTreeClassifierSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-extratreeclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.ExtraTreeClassifierSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">ExtraTreeClassifierSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.ExtraTreeClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>An extremely randomized tree classifier.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.tree.tree.ExtraTreeClassifier</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Extra-trees differ from classic decision trees in the way they are built.
When looking for the best split to separate the samples of a node into two
groups, random splits are drawn for each of the <cite>max_features</cite> randomly
selected features and the best split among those is chosen. When
<cite>max_features</cite> is set 1, this amounts to building a totally random
decision tree.</p>
<p>Warning: Extra-trees should only be used within ensemble methods.</p>
<p>See also</p>
<p>ExtraTreeRegressor, ExtraTreesClassifier, ExtraTreesRegressor</p>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id14" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>P. Geurts, D. Ernst., and L. Wehenkel, &#8220;Extremely randomized trees&#8221;,
Machine Learning, 63(1), 3-42, 2006.</td></tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>ExtraTreeClassifierSklearnNode</strong></li>
<li><strong>ExtraTreeClassifierSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-extratreeregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.ExtraTreeRegressorSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.ExtraTreeRegressorSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.ExtraTreeRegressorSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-extratreeregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.ExtraTreeRegressorSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">ExtraTreeRegressorSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.ExtraTreeRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>An extremely randomized tree regressor.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.tree.tree.ExtraTreeRegressor</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Extra-trees differ from classic decision trees in the way they are built.
When looking for the best split to separate the samples of a node into two
groups, random splits are drawn for each of the <cite>max_features</cite> randomly
selected features and the best split among those is chosen. When
<cite>max_features</cite> is set 1, this amounts to building a totally random
decision tree.</p>
<p>Warning: Extra-trees should only be used within ensemble methods.</p>
<p>See also</p>
<p>ExtraTreeClassifier, ExtraTreesClassifier, ExtraTreesRegressor</p>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id15" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>P. Geurts, D. Ernst., and L. Wehenkel, &#8220;Extremely randomized trees&#8221;,
Machine Learning, 63(1), 3-42, 2006.</td></tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>ExtraTreeRegressorSklearn</strong></li>
<li><strong>ExtraTreeRegressorSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-extratreesclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.ExtraTreesClassifierSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.ExtraTreesClassifierSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.ExtraTreesClassifierSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-extratreesclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.ExtraTreesClassifierSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">ExtraTreesClassifierSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.ExtraTreesClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>An extra-trees classifier.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.ensemble.forest.ExtraTreesClassifier</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>This class implements a meta estimator that fits a number of
randomized decision trees (a.k.a. extra-trees) on various sub-samples
of the dataset and use averaging to improve the predictive accuracy
and control over-fitting.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_estimators <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=10)</span></dt>
<dd>The number of trees in the forest.</dd>
<dt>criterion <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=&#8221;gini&#8221;)</span></dt>
<dd>The function to measure the quality of a split. Supported criteria are
&#8220;gini&#8221; for the Gini impurity and &#8220;entropy&#8221; for the information gain.
Note: this parameter is tree-specific.</dd>
<dt>max_features <span class="classifier-delimiter">:</span> <span class="classifier">int, float, string or None, optional (default=&#8221;auto&#8221;)</span></dt>
<dd><p class="first">The number of features to consider when looking for the best split:</p>
<blockquote>
<div><ul class="simple">
<li><ul class="first">
<li>If int, then consider <cite>max_features</cite> features at each split.</li>
</ul>
</li>
<li><ul class="first">
<li>If float, then <cite>max_features</cite> is a percentage and</li>
</ul>
</li>
<li><cite>int(max_features * n_features)</cite> features are considered at each</li>
<li>split.</li>
<li><ul class="first">
<li>If &#8220;auto&#8221;, then <cite>max_features=sqrt(n_features)</cite>.</li>
</ul>
</li>
<li><ul class="first">
<li>If &#8220;sqrt&#8221;, then <cite>max_features=sqrt(n_features)</cite>.</li>
</ul>
</li>
<li><ul class="first">
<li>If &#8220;log2&#8221;, then <cite>max_features=log2(n_features)</cite>.</li>
</ul>
</li>
<li><ul class="first">
<li>If None, then <cite>max_features=n_features</cite>.</li>
</ul>
</li>
</ul>
</div></blockquote>
<p class="last">Note: this parameter is tree-specific.</p>
</dd>
<dt>max_depth <span class="classifier-delimiter">:</span> <span class="classifier">integer or None, optional (default=None)</span></dt>
<dd>The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.
Note: this parameter is tree-specific.</dd>
<dt>min_samples_split <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=2)</span></dt>
<dd>The minimum number of samples required to split an internal node.
Note: this parameter is tree-specific.</dd>
<dt>min_samples_leaf <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=1)</span></dt>
<dd>The minimum number of samples in newly created leaves.  A split is
discarded if after the split, one of the leaves would contain less then
<tt class="docutils literal"><span class="pre">min_samples_leaf</span></tt> samples.
Note: this parameter is tree-specific.</dd>
<dt>bootstrap <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=False)</span></dt>
<dd>Whether bootstrap samples are used when building trees.</dd>
<dt>oob_score <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd>Whether to use out-of-bag samples to estimate
the generalization error.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=1)</span></dt>
<dd>The number of jobs to run in parallel for both <cite>fit</cite> and <cite>predict</cite>.
If -1, then the number of jobs is set to the number of cores.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=0)</span></dt>
<dd>Controls the verbosity of the tree building process.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>estimators_</cite>: list of DecisionTreeClassifier</dt>
<dd>The collection of fitted sub-estimators.</dd>
<dt><cite>classes_</cite>: array of shape = [n_classes] or a list of such arrays</dt>
<dd>The classes labels (single output problem), or a list of arrays of
class labels (multi-output problem).</dd>
<dt><cite>n_classes_</cite>: int or list</dt>
<dd>The number of classes (single output problem), or a list containing the
number of classes for each output (multi-output problem).</dd>
<dt><cite>feature_importances_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_features]</span></dt>
<dd>The feature importances (the higher, the more important the feature).</dd>
<dt><cite>oob_score_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Score of the training dataset obtained using an out-of-bag estimate.</dd>
<dt><cite>oob_decision_function_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_samples, n_classes]</span></dt>
<dd>Decision function computed with out-of-bag estimate on the training
set. If n_estimators is small it might be possible that a data point
was never left out during the bootstrap. In this case,
<cite>oob_decision_function_</cite> might contain NaN.</dd>
</dl>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id16" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>P. Geurts, D. Ernst., and L. Wehenkel, &#8220;Extremely randomized trees&#8221;,
Machine Learning, 63(1), 3-42, 2006.</td></tr>
</tbody>
</table>
<p>See also</p>
<p>sklearn.tree.ExtraTreeClassifier : Base classifier for this ensemble.
RandomForestClassifier : Ensemble Classifier based on trees with optimal</p>
<blockquote>
<div>splits.</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>ExtraTreesClassifierSklearnNode</strong></li>
<li><strong>ExtraTreesClassifierSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-extratreesregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.ExtraTreesRegressorSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.ExtraTreesRegressorSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.ExtraTreesRegressorSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-extratreesregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.ExtraTreesRegressorSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">ExtraTreesRegressorSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.ExtraTreesRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>An extra-trees regressor.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.ensemble.forest.ExtraTreesRegressor</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>This class implements a meta estimator that fits a number of
randomized decision trees (a.k.a. extra-trees) on various sub-samples
of the dataset and use averaging to improve the predictive accuracy
and control over-fitting.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_estimators <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=10)</span></dt>
<dd>The number of trees in the forest.</dd>
<dt>criterion <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=&#8221;mse&#8221;)</span></dt>
<dd>The function to measure the quality of a split. The only supported
criterion is &#8220;mse&#8221; for the mean squared error.
Note: this parameter is tree-specific.</dd>
<dt>max_features <span class="classifier-delimiter">:</span> <span class="classifier">int, float, string or None, optional (default=&#8221;auto&#8221;)</span></dt>
<dd><p class="first">The number of features to consider when looking for the best split:</p>
<blockquote>
<div><ul class="simple">
<li><ul class="first">
<li>If int, then consider <cite>max_features</cite> features at each split.</li>
</ul>
</li>
<li><ul class="first">
<li>If float, then <cite>max_features</cite> is a percentage and</li>
</ul>
</li>
<li><cite>int(max_features * n_features)</cite> features are considered at each</li>
<li>split.</li>
<li><ul class="first">
<li>If &#8220;auto&#8221;, then <cite>max_features=n_features</cite>.</li>
</ul>
</li>
<li><ul class="first">
<li>If &#8220;sqrt&#8221;, then <cite>max_features=sqrt(n_features)</cite>.</li>
</ul>
</li>
<li><ul class="first">
<li>If &#8220;log2&#8221;, then <cite>max_features=log2(n_features)</cite>.</li>
</ul>
</li>
<li><ul class="first">
<li>If None, then <cite>max_features=n_features</cite>.</li>
</ul>
</li>
</ul>
</div></blockquote>
<p class="last">Note: this parameter is tree-specific.</p>
</dd>
<dt>max_depth <span class="classifier-delimiter">:</span> <span class="classifier">integer or None, optional (default=None)</span></dt>
<dd>The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.
Note: this parameter is tree-specific.</dd>
<dt>min_samples_split <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=2)</span></dt>
<dd>The minimum number of samples required to split an internal node.
Note: this parameter is tree-specific.</dd>
<dt>min_samples_leaf <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=1)</span></dt>
<dd>The minimum number of samples in newly created leaves.  A split is
discarded if after the split, one of the leaves would contain less then
<tt class="docutils literal"><span class="pre">min_samples_leaf</span></tt> samples.
Note: this parameter is tree-specific.</dd>
<dt>bootstrap <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=False)</span></dt>
<dd>Whether bootstrap samples are used when building trees.
Note: this parameter is tree-specific.</dd>
<dt>oob_score <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd>Whether to use out-of-bag samples to estimate
the generalization error.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=1)</span></dt>
<dd>The number of jobs to run in parallel for both <cite>fit</cite> and <cite>predict</cite>.
If -1, then the number of jobs is set to the number of cores.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=0)</span></dt>
<dd>Controls the verbosity of the tree building process.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>estimators_</cite>: list of DecisionTreeRegressor</dt>
<dd>The collection of fitted sub-estimators.</dd>
<dt><cite>feature_importances_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_features]</span></dt>
<dd>The feature importances (the higher, the more important the feature).</dd>
<dt><cite>oob_score_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Score of the training dataset obtained using an out-of-bag estimate.</dd>
<dt><cite>oob_prediction_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_samples]</span></dt>
<dd>Prediction computed with out-of-bag estimate on the training set.</dd>
</dl>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id17" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>P. Geurts, D. Ernst., and L. Wehenkel, &#8220;Extremely randomized trees&#8221;,
Machine Learning, 63(1), 3-42, 2006.</td></tr>
</tbody>
</table>
<p>See also</p>
<p>sklearn.tree.ExtraTreeRegressor: Base estimator for this ensemble.
RandomForestRegressor: Ensemble regressor using trees with optimal splits.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>ExtraTreesRegressorSklearnNode</strong></li>
<li><strong>ExtraTreesRegressorSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-factoranalysissklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.FactorAnalysisSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.FactorAnalysisSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.FactorAnalysisSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-factoranalysissklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.FactorAnalysisSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">FactorAnalysisSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.FactorAnalysisSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Factor Analysis (FA)</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.decomposition.factor_analysis.FactorAnalysis</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>A simple linear generative model with Gaussian latent variables.</p>
<p>The observations are assumed to be caused by a linear transformation of
lower dimensional latent factors and added Gaussian noise.
Without loss of generality the factors are distributed according to a
Gaussian with zero mean and unit covariance. The noise is also zero mean
and has an arbitrary diagonal covariance matrix.</p>
<p>If we would restrict the model further, by assuming that the Gaussian
noise is even isotropic (all diagonal entries are the same) we would obtain
<tt class="xref py py-class docutils literal"><span class="pre">PPCA</span></tt>.</p>
<p>FactorAnalysis performs a maximum likelihood estimate of the so-called
<cite>loading</cite> matrix, the transformation of the latent variables to the
observed ones, using expectation-maximization (EM).</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int | None</span></dt>
<dd>Dimensionality of latent space, the number of components
of <tt class="docutils literal"><span class="pre">X</span></tt> that are obtained after <tt class="docutils literal"><span class="pre">transform</span></tt>.
If None, n_components is set to the number of features.</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Stopping tolerance for EM algorithm.</dd>
<dt>copy <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd>Whether to make a copy of X. If <tt class="docutils literal"><span class="pre">False</span></tt>, the input X gets overwritten
during fitting.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Maximum number of iterations.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">int | bool</span></dt>
<dd>Print verbose output.</dd>
<dt>noise_variance_init <span class="classifier-delimiter">:</span> <span class="classifier">None | array, shape=(n_features,)</span></dt>
<dd>The initial guess of the noise variance for each feature.
If None, it defaults to np.ones(n_features)</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>components_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span></dt>
<dd>Components with maximum variance.</dd>
<dt><cite>loglike_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">list, [n_iterations]</span></dt>
<dd>The log likelihood at each iteration.</dd>
<dt><cite>noise_variance_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape=(n_features,)</span></dt>
<dd>The estimated noise variance for each feature.</dd>
</dl>
<p><strong>References</strong></p>
<p>See also</p>
<dl class="docutils">
<dt>PCA: Principal component analysis, a similar non-probabilistic</dt>
<dd>model model that can be computed in closed form.</dd>
</dl>
<p>ProbabilisticPCA: probabilistic PCA.
FastICA: Independent component analysis, a latent variable model with</p>
<blockquote>
<div>non-Gaussian latent variables.</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FactorAnalysisSklearnNode</strong></li>
<li><strong>FactorAnalysisSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-featurehashersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.FeatureHasherSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.FeatureHasherSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.FeatureHasherSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-featurehashersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.FeatureHasherSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">FeatureHasherSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.FeatureHasherSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Implements feature hashing, aka the hashing trick.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.feature_extraction.hashing.FeatureHasher</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>This class turns sequences of symbolic feature names (strings) into
scipy.sparse matrices, using a hash function to compute the matrix column
corresponding to a name. The hash function employed is the signed 32-bit
version of Murmurhash3.</p>
<p>Feature names of type byte string are used as-is. Unicode strings are
converted to UTF-8 first, but no Unicode normalization is done.</p>
<p>This class is a low-memory alternative to DictVectorizer and
CountVectorizer, intended for large-scale (online) learning and situations
where memory is tight, e.g. when running prediction code on embedded
devices.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_features <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span></dt>
<dd>The number of features (columns) in the output matrices. Small numbers
of features are likely to cause hash collisions, but large numbers
will cause larger coefficient dimensions in linear learners.</dd>
<dt>dtype <span class="classifier-delimiter">:</span> <span class="classifier">NumPy type, optional</span></dt>
<dd>The type of feature values. Passed to scipy.sparse matrix constructors
as the dtype argument. Do not set this to bool, np.boolean or any
unsigned integer type.</dd>
<dt>input_type <span class="classifier-delimiter">:</span> <span class="classifier">string, optional</span></dt>
<dd>Either &#8220;dict&#8221; (the default) to accept dictionaries over
(feature_name, value); &#8220;pair&#8221; to accept pairs of (feature_name, value);
or &#8220;string&#8221; to accept single strings.
feature_name should be a string, while value should be a number.
In the case of &#8220;string&#8221;, a value of 1 is implied.
The feature_name is hashed to find the appropriate column for the
feature. The value&#8217;s sign might be flipped in the output (but see
non_negative, below).</dd>
<dt>non_negative <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span></dt>
<dd>Whether output matrices should contain non-negative values only;
effectively calls abs on the matrix prior to returning it.
When True, output values can be interpreted as frequencies.
When False, output values will have expected value zero.</dd>
</dl>
<p>See also</p>
<p>DictVectorizer : vectorizes string-valued features using a hash table.
sklearn.preprocessing.OneHotEncoder : handles nominal/categorical features</p>
<blockquote>
<div>encoded as columns of integers.</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureHasherSklearnNode</strong></li>
<li><strong>FeatureHasherSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-featureunionsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.FeatureUnionSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.FeatureUnionSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.FeatureUnionSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-featureunionsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.FeatureUnionSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">FeatureUnionSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.FeatureUnionSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Concatenates results of multiple transformer objects.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.pipeline.FeatureUnion</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>This estimator applies a list of transformer objects in parallel to the
input data, then concatenates the results. This is useful to combine
several feature extraction mechanisms into a single transformer.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>transformer_list: list of (string, transformer) tuples</dt>
<dd>List of transformer objects to be applied to the data. The first
half of each tuple is the name of the transformer.</dd>
<dt>n_jobs: int, optional</dt>
<dd>Number of jobs to run in parallel (default 1).</dd>
<dt>transformer_weights: dict, optional</dt>
<dd>Multiplicative weights for features per transformer.
Keys are transformer names, values the weights.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureUnionSklearn</strong></li>
<li><strong>FeatureUnionSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-forestregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.ForestRegressorSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.ForestRegressorSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.ForestRegressorSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-forestregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.ForestRegressorSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">ForestRegressorSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.ForestRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Base class for forest of trees-based regressors.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.ensemble.forest.ForestRegressor</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Warning: This class should not be used directly. Use derived classes
instead.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>ForestRegressorSklearn</strong></li>
<li><strong>ForestRegressorSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-gaussiannbsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.GaussianNBSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.GaussianNBSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.GaussianNBSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-gaussiannbsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.GaussianNBSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">GaussianNBSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.GaussianNBSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Gaussian Naive Bayes (GaussianNB)</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.naive_bayes.GaussianNB</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>X <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_samples, n_features]</span></dt>
<dd>Training vector, where n_samples in the number of samples and
n_features is the number of features.</dd>
<dt>y <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_samples]</span></dt>
<dd>Target vector relative to X</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>class_prior_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_classes]</span></dt>
<dd>probability of each class.</dd>
<dt><cite>theta_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_classes, n_features]</span></dt>
<dd>mean of each feature per class</dd>
<dt><cite>sigma_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_classes, n_features]</span></dt>
<dd>variance of each feature per class</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="go">GaussianNB()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>GaussianNBSklearnNode</strong></li>
<li><strong>GaussianNBSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-genericunivariateselectsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.GenericUnivariateSelectSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.GenericUnivariateSelectSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.GenericUnivariateSelectSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-genericunivariateselectsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.GenericUnivariateSelectSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">GenericUnivariateSelectSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.GenericUnivariateSelectSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Univariate feature selector with configurable strategy.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.feature_selection.univariate_selection.GenericUnivariateSelect</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>score_func <span class="classifier-delimiter">:</span> <span class="classifier">callable</span></dt>
<dd>Function taking two arrays X and y, and returning a pair of arrays
(scores, pvalues).</dd>
<dt>mode <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;percentile&#8217;, &#8216;k_best&#8217;, &#8216;fpr&#8217;, &#8216;fdr&#8217;, &#8216;fwe&#8217;}</span></dt>
<dd>Feature selection mode.</dd>
<dt>param <span class="classifier-delimiter">:</span> <span class="classifier">float or int depending on the feature selection mode</span></dt>
<dd>Parameter of the corresponding mode.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>scores_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span></dt>
<dd>Scores of features.</dd>
<dt><cite>pvalues_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span></dt>
<dd>p-values of feature scores.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>GenericUnivariateSelectSklearn</strong></li>
<li><strong>GenericUnivariateSelectSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-gradientboostingclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.GradientBoostingClassifierSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.GradientBoostingClassifierSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.GradientBoostingClassifierSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-gradientboostingclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.GradientBoostingClassifierSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">GradientBoostingClassifierSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.GradientBoostingClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Gradient Boosting for classification.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.ensemble.gradient_boosting.GradientBoostingClassifier</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>GB builds an additive model in a
forward stage-wise fashion; it allows for the optimization of
arbitrary differentiable loss functions. In each stage <tt class="docutils literal"><span class="pre">n_classes_</span></tt>
regression trees are fit on the negative gradient of the
binomial or multinomial deviance loss function. Binary classification
is a special case where only a single regression tree is induced.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>loss <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;deviance&#8217;}, optional (default=&#8217;deviance&#8217;)</span></dt>
<dd>loss function to be optimized. &#8216;deviance&#8217; refers to
deviance (= logistic regression) for classification
with probabilistic outputs.</dd>
<dt>learning_rate <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.1)</span></dt>
<dd>learning rate shrinks the contribution of each tree by <cite>learning_rate</cite>.
There is a trade-off between learning_rate and n_estimators.</dd>
<dt>n_estimators <span class="classifier-delimiter">:</span> <span class="classifier">int (default=100)</span></dt>
<dd>The number of boosting stages to perform. Gradient boosting
is fairly robust to over-fitting so a large number usually
results in better performance.</dd>
<dt>max_depth <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=3)</span></dt>
<dd>maximum depth of the individual regression estimators. The maximum
depth limits the number of nodes in the tree. Tune this parameter
for best performance; the best value depends on the interaction
of the input variables.</dd>
<dt>min_samples_split <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=2)</span></dt>
<dd>The minimum number of samples required to split an internal node.</dd>
<dt>min_samples_leaf <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=1)</span></dt>
<dd>The minimum number of samples required to be at a leaf node.</dd>
<dt>subsample <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1.0)</span></dt>
<dd>The fraction of samples to be used for fitting the individual base
learners. If smaller than 1.0 this results in Stochastic Gradient
Boosting. <cite>subsample</cite> interacts with the parameter <cite>n_estimators</cite>.
Choosing <cite>subsample &lt; 1.0</cite> leads to a reduction of variance
and an increase in bias.</dd>
<dt>max_features <span class="classifier-delimiter">:</span> <span class="classifier">int, float, string or None, optional (default=&#8221;auto&#8221;)</span></dt>
<dd><p class="first">The number of features to consider when looking for the best split:</p>
<blockquote>
<div><ul class="simple">
<li><ul class="first">
<li>If int, then consider <cite>max_features</cite> features at each split.</li>
</ul>
</li>
<li><ul class="first">
<li>If float, then <cite>max_features</cite> is a percentage and</li>
</ul>
</li>
<li><cite>int(max_features * n_features)</cite> features are considered at each</li>
<li>split.</li>
<li><ul class="first">
<li>If &#8220;auto&#8221;, then <cite>max_features=sqrt(n_features)</cite>.</li>
</ul>
</li>
<li><ul class="first">
<li>If &#8220;sqrt&#8221;, then <cite>max_features=sqrt(n_features)</cite>.</li>
</ul>
</li>
<li><ul class="first">
<li>If &#8220;log2&#8221;, then <cite>max_features=log2(n_features)</cite>.</li>
</ul>
</li>
<li><ul class="first">
<li>If None, then <cite>max_features=n_features</cite>.</li>
</ul>
</li>
</ul>
</div></blockquote>
<p class="last">Choosing <cite>max_features &lt; n_features</cite> leads to a reduction of variance
and an increase in bias.</p>
</dd>
<dt>init <span class="classifier-delimiter">:</span> <span class="classifier">BaseEstimator, None, optional (default=None)</span></dt>
<dd>An estimator object that is used to compute the initial
predictions. <tt class="docutils literal"><span class="pre">init</span></tt> has to provide <tt class="docutils literal"><span class="pre">fit</span></tt> and <tt class="docutils literal"><span class="pre">predict</span></tt>.
If None it uses <tt class="docutils literal"><span class="pre">loss.init_estimator</span></tt>.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">int, default: 0</span></dt>
<dd>Enable verbose output. If 1 then it prints progress and performance
once in a while (the more trees the lower the frequency).
If greater than 1 then it prints progress and performance for every tree.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>feature_importances_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_features]</span></dt>
<dd>The feature importances (the higher, the more important the feature).</dd>
<dt><cite>oob_improvement_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_estimators]</span></dt>
<dd>The improvement in loss (= deviance) on the out-of-bag samples
relative to the previous iteration.
<tt class="docutils literal"><span class="pre">oob_improvement_[0]</span></tt> is the improvement in
loss of the first stage over the <tt class="docutils literal"><span class="pre">init</span></tt> estimator.</dd>
<dt><cite>oob_score_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_estimators]</span></dt>
<dd>Score of the training dataset obtained using an out-of-bag estimate.
The i-th score <tt class="docutils literal"><span class="pre">oob_score_[i]</span></tt> is the deviance (= loss) of the
model at iteration <tt class="docutils literal"><span class="pre">i</span></tt> on the out-of-bag sample.
Deprecated: use <cite>oob_improvement_</cite> instead.</dd>
<dt><cite>train_score_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_estimators]</span></dt>
<dd>The i-th score <tt class="docutils literal"><span class="pre">train_score_[i]</span></tt> is the deviance (= loss) of the
model at iteration <tt class="docutils literal"><span class="pre">i</span></tt> on the in-bag sample.
If <tt class="docutils literal"><span class="pre">subsample</span> <span class="pre">==</span> <span class="pre">1</span></tt> this is the deviance on the training data.</dd>
<dt><cite>loss_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">LossFunction</span></dt>
<dd>The concrete <tt class="docutils literal"><span class="pre">LossFunction</span></tt> object.</dd>
<dt><cite>init</cite> <span class="classifier-delimiter">:</span> <span class="classifier">BaseEstimator</span></dt>
<dd>The estimator that provides the initial predictions.
Set via the <tt class="docutils literal"><span class="pre">init</span></tt> argument or <tt class="docutils literal"><span class="pre">loss.init_estimator</span></tt>.</dd>
<dt><cite>estimators_</cite>: list of DecisionTreeRegressor</dt>
<dd>The collection of fitted sub-estimators.</dd>
</dl>
<p>See also</p>
<p>sklearn.tree.DecisionTreeClassifier, RandomForestClassifier</p>
<p><strong>References</strong></p>
<p>J. Friedman, Greedy Function Approximation: A Gradient Boosting
Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.</p>
<ol class="upperalpha simple" start="10">
<li>Friedman, Stochastic Gradient Boosting, 1999</li>
</ol>
<p>T. Hastie, R. Tibshirani and J. Friedman.
Elements of Statistical Learning Ed. 2, Springer, 2009.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>GradientBoostingClassifierSklearn</strong></li>
<li><strong>GradientBoostingClassifierSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-gridsearchcvsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.GridSearchCVSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.GridSearchCVSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.GridSearchCVSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-gridsearchcvsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.GridSearchCVSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">GridSearchCVSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.GridSearchCVSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Exhaustive search over specified parameter values for an estimator.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.grid_search.GridSearchCV</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Important members are fit, predict.</p>
<p>GridSearchCV implements a &#8220;fit&#8221; method and a &#8220;predict&#8221; method like
any classifier except that the parameters of the classifier
used to predict is optimized by cross-validation.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>estimator <span class="classifier-delimiter">:</span> <span class="classifier">object type that implements the &#8220;fit&#8221; and &#8220;predict&#8221; methods</span></dt>
<dd>A object of that type is instantiated for each grid point.</dd>
<dt>param_grid <span class="classifier-delimiter">:</span> <span class="classifier">dict or list of dictionaries</span></dt>
<dd>Dictionary with parameters names (string) as keys and lists of
parameter settings to try as values, or a list of such
dictionaries, in which case the grids spanned by each dictionary
in the list are explored. This enables searching over any sequence
of parameter settings.</dd>
<dt>scoring <span class="classifier-delimiter">:</span> <span class="classifier">string, callable or None, optional, default: None</span></dt>
<dd>A string (see model evaluation documentation) or
a scorer callable object / function with signature
<tt class="docutils literal"><span class="pre">scorer(estimator,</span> <span class="pre">X,</span> <span class="pre">y)</span></tt>.</dd>
<dt>fit_params <span class="classifier-delimiter">:</span> <span class="classifier">dict, optional</span></dt>
<dd>Parameters to pass to the fit method.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>Number of jobs to run in parallel (default 1).</dd>
<dt>pre_dispatch <span class="classifier-delimiter">:</span> <span class="classifier">int, or string, optional</span></dt>
<dd><p class="first">Controls the number of jobs that get dispatched during parallel
execution. Reducing this number can be useful to avoid an
explosion of memory consumption when more jobs get dispatched
than CPUs can process. This parameter can be:</p>
<blockquote class="last">
<div><ul class="simple">
<li>None, in which case all the jobs are immediately
created and spawned. Use this for lightweight and
fast-running jobs, to avoid delays due to on-demand
spawning of the jobs</li>
<li>An int, giving the exact number of total jobs that are
spawned</li>
<li>A string, giving an expression as a function of n_jobs,
as in &#8216;2*n_jobs&#8217;</li>
</ul>
</div></blockquote>
</dd>
<dt>iid <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span></dt>
<dd>If True, the data is assumed to be identically distributed across
the folds, and the loss minimized is the total loss per sample,
and not the mean loss across the folds.</dd>
<dt>cv <span class="classifier-delimiter">:</span> <span class="classifier">integer or cross-validation generator, optional</span></dt>
<dd>If an integer is passed, it is the number of folds (default 3).
Specific cross-validation objects can be passed, see
sklearn.cross_validation module for the list of possible objects</dd>
<dt>refit <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd>Refit the best estimator with the entire dataset.
If &#8220;False&#8221;, it is impossible to make predictions using
this GridSearchCV instance after fitting.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">integer</span></dt>
<dd>Controls the verbosity: the higher, the more messages.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span><span class="p">,</span> <span class="n">grid_search</span><span class="p">,</span> <span class="n">datasets</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s">&#39;kernel&#39;</span><span class="p">:(</span><span class="s">&#39;linear&#39;</span><span class="p">,</span> <span class="s">&#39;rbf&#39;</span><span class="p">),</span> <span class="s">&#39;C&#39;</span><span class="p">:[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">]}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">svr</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">grid_search</span><span class="o">.</span><span class="n">GridSearchCV</span><span class="p">(</span><span class="n">svr</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
<span class="gp">... </span>                            
<span class="go">GridSearchCV(cv=None,</span>
<span class="go">       estimator=SVC(C=1.0, cache_size=..., class_weight=..., coef0=..., degree=..., gamma=...,</span>
<span class="go">   kernel=&#39;rbf&#39;, max_iter=-1, probability=False, random_state=None,</span>
<span class="go">   shrinking=True, tol=..., verbose=False),</span>
<span class="go">       fit_params={}, iid=..., loss_func=..., n_jobs=1,</span>
<span class="go">       param_grid=..., pre_dispatch=..., refit=..., score_func=...,</span>
<span class="go">       scoring=..., verbose=...)</span>
</pre></div>
</div>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>grid_scores_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">list of named tuples</span></dt>
<dd><p class="first">Contains scores for all parameter combinations in param_grid.
Each entry corresponds to one parameter setting.
Each named tuple has the attributes:</p>
<blockquote class="last">
<div><ul class="simple">
<li><tt class="docutils literal"><span class="pre">parameters</span></tt>, a dict of parameter settings</li>
<li><tt class="docutils literal"><span class="pre">mean_validation_score</span></tt>, the mean score over the
cross-validation folds</li>
<li><tt class="docutils literal"><span class="pre">cv_validation_scores</span></tt>, the list of scores for each fold</li>
</ul>
</div></blockquote>
</dd>
<dt><cite>best_estimator_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">estimator</span></dt>
<dd>Estimator that was chosen by the search, i.e. estimator
which gave highest score (or smallest loss if specified)
on the left out data.</dd>
<dt><cite>best_score_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Score of best_estimator on the left out data.</dd>
<dt><cite>best_params_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">dict</span></dt>
<dd>Parameter setting that gave the best results on the hold out data.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>The parameters selected are those that maximize the score of the left out
data, unless an explicit score is passed in which case it is used instead.</p>
<p>If <cite>n_jobs</cite> was set to a value higher than one, the data is copied for each
point in the grid (and not <cite>n_jobs</cite> times). This is done for efficiency
reasons if individual jobs take very little time, but may raise errors if
the dataset is large and not enough memory is available.  A workaround in
this case is to set <cite>pre_dispatch</cite>. Then, the memory is copied only
<cite>pre_dispatch</cite> many times. A reasonable value for <cite>pre_dispatch</cite> is <cite>2 *
n_jobs</cite>.</p>
<p>See Also</p>
<p><tt class="xref py py-class docutils literal"><span class="pre">ParameterGrid</span></tt>:</p>
<blockquote>
<div><ul class="simple">
<li>generates all the combinations of a an hyperparameter grid.</li>
</ul>
</div></blockquote>
<p><tt class="xref py py-func docutils literal"><span class="pre">sklearn.cross_validation.train_test_split()</span></tt>:</p>
<blockquote>
<div><ul class="simple">
<li>utility function to split the data into a development set usable</li>
<li>for fitting a GridSearchCV instance and an evaluation set for</li>
<li>its final evaluation.</li>
</ul>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>GridSearchCVSklearnNode</strong></li>
<li><strong>GridSearchCVSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-hashingvectorizersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.HashingVectorizerSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.HashingVectorizerSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.HashingVectorizerSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-hashingvectorizersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.HashingVectorizerSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">HashingVectorizerSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.HashingVectorizerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Convert a collection of text documents to a matrix of token occurrences</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.feature_extraction.text.HashingVectorizer</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>It turns a collection of text documents into a scipy.sparse matrix holding
token occurrence counts (or binary occurrence information), possibly
normalized as token frequencies if norm=&#8217;l1&#8217; or projected on the euclidean
unit sphere if norm=&#8217;l2&#8217;.</p>
<p>This text vectorizer implementation uses the hashing trick to find the
token string name to feature integer index mapping.</p>
<p>This strategy has several advantage:</p>
<ul class="simple">
<li>it is very low memory scalable to large datasets as there is no need to
store a vocabulary dictionary in memory</li>
<li>it is fast to pickle and un-pickle has it holds no state besides the
constructor parameters</li>
<li>it can be used in a streaming (partial fit) or parallel pipeline as there
is no state computed during fit.</li>
</ul>
<p>There are also a couple of cons (vs using a CountVectorizer with an
in-memory vocabulary):</p>
<ul class="simple">
<li>there is no way to compute the inverse transform (from feature indices to
string feature names) which can be a problem when trying to introspect
which features are most important to a model.</li>
<li>there can be collisions: distinct tokens can be mapped to the same
feature index. However in practice this is rarely an issue if n_features
is large enough (e.g. 2 ** 18 for text classification problems).</li>
<li>no IDF weighting as this would render the transformer stateful.</li>
</ul>
<p>The hash function employed is the signed 32-bit version of Murmurhash3.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>input: string {&#8216;filename&#8217;, &#8216;file&#8217;, &#8216;content&#8217;}</dt>
<dd><p class="first">If filename, the sequence passed as an argument to fit is
expected to be a list of filenames that need reading to fetch
the raw content to analyze.</p>
<p>If &#8216;file&#8217;, the sequence items must have &#8216;read&#8217; method (file-like
object) it is called to fetch the bytes in memory.</p>
<p class="last">Otherwise the input is expected to be the sequence strings or
bytes items are expected to be analyzed directly.</p>
</dd>
<dt>encoding <span class="classifier-delimiter">:</span> <span class="classifier">string, &#8216;utf-8&#8217; by default.</span></dt>
<dd>If bytes or files are given to analyze, this encoding is used to
decode.</dd>
<dt>decode_error <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;strict&#8217;, &#8216;ignore&#8217;, &#8216;replace&#8217;}</span></dt>
<dd>Instruction on what to do if a byte sequence is given to analyze that
contains characters not of the given <cite>encoding</cite>. By default, it is
&#8216;strict&#8217;, meaning that a UnicodeDecodeError will be raised. Other
values are &#8216;ignore&#8217; and &#8216;replace&#8217;.</dd>
<dt>strip_accents: {&#8216;ascii&#8217;, &#8216;unicode&#8217;, None}</dt>
<dd>Remove accents during the preprocessing step.
&#8216;ascii&#8217; is a fast method that only works on characters that have
an direct ASCII mapping.
&#8216;unicode&#8217; is a slightly slower method that works on any characters.
None (default) does nothing.</dd>
<dt>analyzer: string, {&#8216;word&#8217;, &#8216;char&#8217;, &#8216;char_wb&#8217;} or callable</dt>
<dd><p class="first">Whether the feature should be made of word or character n-grams.
Option &#8216;char_wb&#8217; creates character n-grams only from text inside
word boundaries.</p>
<p class="last">If a callable is passed it is used to extract the sequence of features
out of the raw, unprocessed input.</p>
</dd>
<dt>preprocessor: callable or None (default)</dt>
<dd>Override the preprocessing (string transformation) stage while
preserving the tokenizing and n-grams generation steps.</dd>
<dt>tokenizer: callable or None (default)</dt>
<dd>Override the string tokenization step while preserving the
preprocessing and n-grams generation steps.</dd>
<dt>ngram_range: tuple (min_n, max_n)</dt>
<dd>The lower and upper boundary of the range of n-values for different
n-grams to be extracted. All values of n such that min_n &lt;= n &lt;= max_n
will be used.</dd>
<dt>stop_words: string {&#8216;english&#8217;}, list, or None (default)</dt>
<dd><p class="first">If a string, it is passed to _check_stop_list and the appropriate stop
list is returned. &#8216;english&#8217; is currently the only supported string
value.</p>
<p class="last">If a list, that list is assumed to contain stop words, all of which
will be removed from the resulting tokens.</p>
</dd>
<dt>lowercase: boolean, default True</dt>
<dd>Convert all characters to lowercase before tokenizing.</dd>
<dt>token_pattern: string</dt>
<dd>Regular expression denoting what constitutes a &#8220;token&#8221;, only used
if <cite>tokenize == &#8216;word&#8217;</cite>. The default regexp select tokens of 2
or more letters characters (punctuation is completely ignored
and always treated as a token separator).</dd>
<dt>n_features <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional, (2 ** 20) by default</span></dt>
<dd>The number of features (columns) in the output matrices. Small numbers
of features are likely to cause hash collisions, but large numbers
will cause larger coefficient dimensions in linear learners.</dd>
<dt>norm <span class="classifier-delimiter">:</span> <span class="classifier">&#8216;l1&#8217;, &#8216;l2&#8217; or None, optional</span></dt>
<dd>Norm used to normalize term vectors. None for no normalization.</dd>
<dt>binary: boolean, False by default.</dt>
<dd>If True, all non zero counts are set to 1. This is useful for discrete
probabilistic models that model binary events rather than integer
counts.</dd>
<dt>dtype: type, optional</dt>
<dd>Type of the matrix returned by fit_transform() or transform().</dd>
<dt>non_negative <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span></dt>
<dd>Whether output matrices should contain non-negative values only;
effectively calls abs on the matrix prior to returning it.
When True, output values can be interpreted as frequencies.
When False, output values will have expected value zero.</dd>
</dl>
<p>See also</p>
<p>CountVectorizer, TfidfVectorizer</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>HashingVectorizerSklearn</strong></li>
<li><strong>HashingVectorizerSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-imputersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.ImputerSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.ImputerSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.ImputerSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-imputersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.ImputerSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">ImputerSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.ImputerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Imputation transformer for completing missing values.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.preprocessing.imputation.Imputer</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>missing_values <span class="classifier-delimiter">:</span> <span class="classifier">integer or string, optional (default=&#8221;NaN&#8221;)</span></dt>
<dd>The placeholder for the missing values. All occurences of
<cite>missing_values</cite> will be imputed. For missing values encoded as np.nan,
use the string value &#8220;NaN&#8221;.</dd>
<dt>strategy <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=&#8221;mean&#8221;)</span></dt>
<dd><dl class="first last docutils">
<dt>The imputation strategy.</dt>
<dd><ul class="first last simple">
<li>If &#8220;mean&#8221;, then replace missing values using the mean along
the axis.</li>
<li>If &#8220;median&#8221;, then replace missing values using the median along
the axis.</li>
<li>If &#8220;most_frequent&#8221;, then replace missing using the most frequent
value along the axis.</li>
</ul>
</dd>
</dl>
</dd>
<dt>axis <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=0)</span></dt>
<dd><dl class="first last docutils">
<dt>The axis along which to impute.</dt>
<dd><ul class="first last simple">
<li>If <cite>axis=0</cite>, then impute along columns.</li>
<li>If <cite>axis=1</cite>, then impute along rows.</li>
</ul>
</dd>
</dl>
</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=0)</span></dt>
<dd>Controls the verbosity of the imputer.</dd>
<dt>copy <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=True)</span></dt>
<dd>If True, a copy of X will be created. If False, imputation will
be done in-place.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>statistics_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape (n_features,) or (n_samples,)</span></dt>
<dd>The statistics along the imputation axis.</dd>
</dl>
<p><strong>Notes</strong></p>
<ul class="simple">
<li>When <tt class="docutils literal"><span class="pre">axis=0</span></tt>, columns which only contained missing values at <cite>fit</cite>
are discarded upon <cite>transform</cite>.</li>
<li>When <tt class="docutils literal"><span class="pre">axis=1</span></tt>, an exception is raised if there are rows for which it is
not possible to fill in the missing values (e.g., because they only
contain missing values).</li>
</ul>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>ImputerSklearn</strong></li>
<li><strong>ImputerSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-isomapsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.IsomapSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.IsomapSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.IsomapSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-isomapsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.IsomapSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">IsomapSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.IsomapSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Isomap Embedding</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.manifold.isomap.Isomap</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Non-linear dimensionality reduction through Isometric Mapping</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_neighbors <span class="classifier-delimiter">:</span> <span class="classifier">integer</span></dt>
<dd>number of neighbors to consider for each point.</dd>
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">integer</span></dt>
<dd>number of coordinates for the manifold</dd>
<dt>eigen_solver <span class="classifier-delimiter">:</span> <span class="classifier">[&#8216;auto&#8217;|&#8217;arpack&#8217;|&#8217;dense&#8217;]</span></dt>
<dd><dl class="first last docutils">
<dt>&#8216;auto&#8217; <span class="classifier-delimiter">:</span> <span class="classifier">Attempt to choose the most efficient solver</span></dt>
<dd>for the given problem.</dd>
<dt>&#8216;arpack&#8217; <span class="classifier-delimiter">:</span> <span class="classifier">Use Arnoldi decomposition to find the eigenvalues</span></dt>
<dd>and eigenvectors.</dd>
<dt>&#8216;dense&#8217; <span class="classifier-delimiter">:</span> <span class="classifier">Use a direct solver (i.e. LAPACK)</span></dt>
<dd>for the eigenvalue decomposition.</dd>
</dl>
</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Convergence tolerance passed to arpack or lobpcg.
not used if eigen_solver == &#8216;dense&#8217;.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">integer</span></dt>
<dd>Maximum number of iterations for the arpack solver.
not used if eigen_solver == &#8216;dense&#8217;.</dd>
<dt>path_method <span class="classifier-delimiter">:</span> <span class="classifier">string [&#8216;auto&#8217;|&#8217;FW&#8217;|&#8217;D&#8217;]</span></dt>
<dd>Method to use in finding shortest path.
&#8216;auto&#8217; : attempt to choose the best algorithm automatically
&#8216;FW&#8217; : Floyd-Warshall algorithm
&#8216;D&#8217; : Dijkstra algorithm with Fibonacci Heaps</dd>
<dt>neighbors_algorithm <span class="classifier-delimiter">:</span> <span class="classifier">string [&#8216;auto&#8217;|&#8217;brute&#8217;|&#8217;kd_tree&#8217;|&#8217;ball_tree&#8217;]</span></dt>
<dd>Algorithm to use for nearest neighbors search,
passed to neighbors.NearestNeighbors instance.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>embedding_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_samples, n_components)</span></dt>
<dd>Stores the embedding vectors.</dd>
<dt><cite>kernel_pca_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">object</span></dt>
<dd><cite>KernelPCA</cite> object used to implement the embedding.</dd>
<dt><cite>training_data_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_samples, n_features)</span></dt>
<dd>Stores the training data.</dd>
<dt><cite>nbrs_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">sklearn.neighbors.NearestNeighbors instance</span></dt>
<dd>Stores nearest neighbors instance, including BallTree or KDtree
if applicable.</dd>
<dt><cite>dist_matrix_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_samples, n_samples)</span></dt>
<dd>Stores the geodesic distance matrix of training data.</dd>
</dl>
<p><strong>References</strong></p>
<dl class="docutils">
<dt>[1] Tenenbaum, J.B.; De Silva, V.; &amp; Langford, J.C. A global geometric</dt>
<dd>framework for nonlinear dimensionality reduction. Science 290 (5500)</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>IsomapSklearnNode</strong></li>
<li><strong>IsomapSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-isotonicregressionsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.IsotonicRegressionSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.IsotonicRegressionSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.IsotonicRegressionSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-isotonicregressionsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.IsotonicRegressionSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">IsotonicRegressionSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.IsotonicRegressionSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Isotonic regression model.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.isotonic.IsotonicRegression</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>The isotonic regression optimization problem is defined by:</p>
<div class="highlight-python"><div class="highlight"><pre>min sum w_i (y[i] - y_[i]) ** 2

subject to y_[i] &lt;= y_[j] whenever X[i] &lt;= X[j]
and min(y_) = y_min, max(y_) = y_max
</pre></div>
</div>
<p>where:</p>
<blockquote>
<div><ul class="simple">
<li><ul class="first">
<li><tt class="docutils literal"><span class="pre">y[i]</span></tt> are inputs (real numbers)</li>
</ul>
</li>
<li><ul class="first">
<li><tt class="docutils literal"><span class="pre">y_[i]</span></tt> are fitted</li>
</ul>
</li>
<li><ul class="first">
<li><tt class="docutils literal"><span class="pre">X</span></tt> specifies the order.</li>
</ul>
</li>
<li>If <tt class="docutils literal"><span class="pre">X</span></tt> is non-decreasing then <tt class="docutils literal"><span class="pre">y_</span></tt> is non-decreasing.</li>
<li><ul class="first">
<li><tt class="docutils literal"><span class="pre">w[i]</span></tt> are optional strictly positive weights (default to 1.0)</li>
</ul>
</li>
</ul>
</div></blockquote>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>y_min <span class="classifier-delimiter">:</span> <span class="classifier">optional, default: None</span></dt>
<dd>If not None, set the lowest value of the fit to y_min.</dd>
<dt>y_max <span class="classifier-delimiter">:</span> <span class="classifier">optional, default: None</span></dt>
<dd>If not None, set the highest value of the fit to y_max.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>X_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">ndarray (n_samples, )</span></dt>
<dd>A copy of the input X.</dd>
<dt><cite>y_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">ndarray (n_samples, )</span></dt>
<dd>Isotonic fit of y.</dd>
</dl>
<p><strong>References</strong></p>
<p>Isotonic Median Regression: A Linear Programming Approach
Nilotpal Chakravarti
Mathematics of Operations Research
Vol. 14, No. 2 (May, 1989), pp. 303-308</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>IsotonicRegressionSklearn</strong></li>
<li><strong>IsotonicRegressionSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-kneighborsclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.KNeighborsClassifierSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.KNeighborsClassifierSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.KNeighborsClassifierSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-kneighborsclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.KNeighborsClassifierSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">KNeighborsClassifierSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.KNeighborsClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Classifier implementing the k-nearest neighbors vote.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.neighbors.classification.KNeighborsClassifier</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_neighbors <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default = 5)</span></dt>
<dd>Number of neighbors to use by default for <tt class="xref py py-meth docutils literal"><span class="pre">k_neighbors()</span></tt> queries.</dd>
<dt>weights <span class="classifier-delimiter">:</span> <span class="classifier">str or callable</span></dt>
<dd><p class="first">weight function used in prediction.  Possible values:</p>
<ul class="simple">
<li>&#8216;uniform&#8217; : uniform weights.  All points in each neighborhood
are weighted equally.</li>
<li>&#8216;distance&#8217; : weight points by the inverse of their distance.
in this case, closer neighbors of a query point will have a
greater influence than neighbors which are further away.</li>
<li>[callable] : a user-defined function which accepts an
array of distances, and returns an array of the same shape
containing the weights.</li>
</ul>
<p class="last">Uniform weights are used by default.</p>
</dd>
<dt>algorithm <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;auto&#8217;, &#8216;ball_tree&#8217;, &#8216;kd_tree&#8217;, &#8216;brute&#8217;}, optional</span></dt>
<dd><p class="first">Algorithm used to compute the nearest neighbors:</p>
<ul class="simple">
<li>&#8216;ball_tree&#8217; will use <tt class="xref py py-class docutils literal"><span class="pre">BallTree</span></tt></li>
<li>&#8216;kd_tree&#8217; will use <tt class="xref py py-class docutils literal"><span class="pre">KDTree</span></tt></li>
<li>&#8216;brute&#8217; will use a brute-force search.</li>
<li>&#8216;auto&#8217; will attempt to decide the most appropriate algorithm
based on the values passed to <tt class="xref py py-meth docutils literal"><span class="pre">fit()</span></tt> method.</li>
</ul>
<p class="last">Note: fitting on sparse input will override the setting of
this parameter, using brute force.</p>
</dd>
<dt>leaf_size <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default = 30)</span></dt>
<dd>Leaf size passed to BallTree or KDTree.  This can affect the
speed of the construction and query, as well as the memory
required to store the tree.  The optimal value depends on the
nature of the problem.</dd>
<dt>metric <span class="classifier-delimiter">:</span> <span class="classifier">string or DistanceMetric object (default=&#8217;minkowski&#8217;)</span></dt>
<dd>the distance metric to use for the tree.  The default metric is
minkowski, and with p=2 is equivalent to the standard Euclidean
metric. See the documentation of the DistanceMetric class for a
list of available metrics.</dd>
<dt>p <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default = 2)</span></dt>
<dd>Power parameter for the Minkowski metric. When p = 1, this is
equivalent to using manhattan_distance (l1), and euclidean_distance
(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.</dd>
</dl>
<p><a href="#id18"><span class="problematic" id="id19">**</span></a>kwargs :</p>
<blockquote>
<div><ul class="simple">
<li>additional keyword arguments are passed to the distance function as</li>
<li>additional arguments.</li>
</ul>
</div></blockquote>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neigh</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neigh</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">KNeighborsClassifier(...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">neigh</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mf">1.1</span><span class="p">]]))</span>
<span class="go">[0]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">neigh</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">([[</span><span class="mf">0.9</span><span class="p">]]))</span>
<span class="go">[[ 0.66666667  0.33333333]]</span>
</pre></div>
</div>
<p>See also</p>
<p>RadiusNeighborsClassifier
KNeighborsRegressor
RadiusNeighborsRegressor
NearestNeighbors</p>
<p><strong>Notes</strong></p>
<p>See <em class="xref std std-ref">Nearest Neighbors</em> in the online documentation
for a discussion of the choice of <tt class="docutils literal"><span class="pre">algorithm</span></tt> and <tt class="docutils literal"><span class="pre">leaf_size</span></tt>.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Regarding the Nearest Neighbors algorithms, if it is found that two
neighbors, neighbor <cite>k+1</cite> and <cite>k</cite>, have identical distances but
but different labels, the results will depend on the ordering of the
training data.</p>
</div>
<p><a class="reference external" href="http://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm">http://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm</a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>KNeighborsClassifierSklearnNode</strong></li>
<li><strong>KNeighborsClassifierSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-kernelcenterersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.KernelCentererSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.KernelCentererSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.KernelCentererSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-kernelcenterersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.KernelCentererSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">KernelCentererSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.KernelCentererSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Center a kernel matrix</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.preprocessing.data.KernelCenterer</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Let K(x, z) be a kernel defined by phi(x)^T phi(z), where phi is a
function mapping x to a Hilbert space. KernelCenterer centers (i.e.,
normalize to have zero mean) the data without explicitly computing phi(x).
It is equivalent to centering phi(x) with
sklearn.preprocessing.StandardScaler(with_std=False).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>KernelCentererSklearnNode</strong></li>
<li><strong>KernelCentererSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-kernelpcasklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.KernelPCASklearnNode" title="pySPACE.missions.nodes.scikits_nodes.KernelPCASklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.KernelPCASklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-kernelpcasklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.KernelPCASklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">KernelPCASklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.KernelPCASklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Kernel Principal component analysis (KPCA)</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.decomposition.kernel_pca.KernelPCA</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Non-linear dimensionality reduction through the use of kernels.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components: int or None</dt>
<dd>Number of components. If None, all non-zero components are kept.</dd>
<dt>kernel: &#8220;linear&#8221; | &#8220;poly&#8221; | &#8220;rbf&#8221; | &#8220;sigmoid&#8221; | &#8220;cosine&#8221; | &#8220;precomputed&#8221;</dt>
<dd>Kernel.
Default: &#8220;linear&#8221;</dd>
<dt>degree <span class="classifier-delimiter">:</span> <span class="classifier">int, default=3</span></dt>
<dd>Degree for poly, rbf and sigmoid kernels. Ignored by other kernels.</dd>
<dt>gamma <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Kernel coefficient for rbf and poly kernels. Default: 1/n_features.
Ignored by other kernels.</dd>
<dt>coef0 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Independent term in poly and sigmoid kernels.
Ignored by other kernels.</dd>
<dt>kernel_params <span class="classifier-delimiter">:</span> <span class="classifier">mapping of string to any, optional</span></dt>
<dd>Parameters (keyword arguments) and values for kernel passed as
callable object. Ignored by other kernels.</dd>
<dt>alpha: int</dt>
<dd>Hyperparameter of the ridge regression that learns the
inverse transform (when fit_inverse_transform=True).
Default: 1.0</dd>
<dt>fit_inverse_transform: bool</dt>
<dd>Learn the inverse transform for non-precomputed kernels.
(i.e. learn to find the pre-image of a point)
Default: False</dd>
<dt>eigen_solver: string [&#8216;auto&#8217;|&#8217;dense&#8217;|&#8217;arpack&#8217;]</dt>
<dd>Select eigensolver to use.  If n_components is much less than
the number of training samples, arpack may be more efficient
than the dense eigensolver.</dd>
<dt>tol: float</dt>
<dd>convergence tolerance for arpack.
Default: 0 (optimal value will be chosen by arpack)</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>maximum number of iterations for arpack
Default: None (optimal value will be chosen by arpack)</dd>
<dt>remove_zero_eig <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default=True</span></dt>
<dd>If True, then all components with zero eigenvalues are removed, so
that the number of components in the output may be &lt; n_components
(and sometimes even zero due to numerical instability).
When n_components is None, this parameter is ignored and components
with zero eigenvalues are removed regardless.</dd>
</dl>
<p><strong>Attributes</strong></p>
<p><cite>lambdas_</cite>, <cite>alphas_</cite>:</p>
<blockquote>
<div><ul class="simple">
<li>Eigenvalues and eigenvectors of the centered kernel matrix</li>
</ul>
</div></blockquote>
<p><cite>dual_coef_</cite>:</p>
<blockquote>
<div><ul class="simple">
<li>Inverse transform matrix</li>
</ul>
</div></blockquote>
<p><cite>X_transformed_fit_</cite>:</p>
<blockquote>
<div><ul class="simple">
<li>Projection of the fitted data on the kernel principal components</li>
</ul>
</div></blockquote>
<p><strong>References</strong></p>
<p>Kernel PCA was introduced in:</p>
<blockquote>
<div><ul class="simple">
<li>Bernhard Schoelkopf, Alexander J. Smola,</li>
<li>and Klaus-Robert Mueller. 1999. Kernel principal</li>
<li>component analysis. In Advances in kernel methods,</li>
<li>MIT Press, Cambridge, MA, USA 327-352.</li>
</ul>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>KernelPCASklearn</strong></li>
<li><strong>KernelPCASklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-ldasklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.LDASklearnNode" title="pySPACE.missions.nodes.scikits_nodes.LDASklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.LDASklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-ldasklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.LDASklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">LDASklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.LDASklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Linear Discriminant Analysis (LDA)</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.lda.LDA</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>A classifier with a linear decision boundary, generated
by fitting class conditional densities to the data
and using Bayes&#8217; rule.</p>
<p>The model fits a Gaussian density to each class, assuming that
all classes share the same covariance matrix.</p>
<p>The fitted model can also be used to reduce the dimensionality
of the input, by projecting it to the most discriminative
directions.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components: int</dt>
<dd>Number of components (&lt; n_classes - 1) for dimensionality reduction</dd>
<dt>priors <span class="classifier-delimiter">:</span> <span class="classifier">array, optional, shape = [n_classes]</span></dt>
<dd>Priors on classes</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>coef_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [rank, n_classes - 1]</span></dt>
<dd>Coefficients of the features in the linear decision
function. rank is min(rank_features, n_classes) where
rank_features is the dimensionality of the spaces spanned
by the features (i.e. n_features excluding redundant features).</dd>
<dt><cite>covariance_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_features, n_features]</span></dt>
<dd>Covariance matrix (shared by all classes).</dd>
<dt><cite>means_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_classes, n_features]</span></dt>
<dd>Class means.</dd>
<dt><cite>priors_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_classes]</span></dt>
<dd>Class priors (sum to 1).</dd>
<dt><cite>scalings_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [rank, n_classes - 1]</span></dt>
<dd>Scaling of the features in the space spanned by the class
centroids.</dd>
<dt><cite>xbar_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">float, shape = [n_features]</span></dt>
<dd>Overall mean.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.lda</span> <span class="kn">import</span> <span class="n">LDA</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">LDA</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">LDA(n_components=None, priors=None)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
<p>See also</p>
<p>sklearn.qda.QDA: Quadratic discriminant analysis</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>LDASklearnNode</strong></li>
<li><strong>LDASklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-labelbinarizersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.LabelBinarizerSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.LabelBinarizerSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.LabelBinarizerSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-labelbinarizersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.LabelBinarizerSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">LabelBinarizerSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.LabelBinarizerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Binarize labels in a one-vs-all fashion</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.preprocessing.label.LabelBinarizer</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Several regression and binary classification algorithms are
available in the scikit. A simple way to extend these algorithms
to the multi-class classification case is to use the so-called
one-vs-all scheme.</p>
<p>At learning time, this simply consists in learning one regressor
or binary classifier per class. In doing so, one needs to convert
multi-class labels to binary labels (belong or does not belong
to the class). LabelBinarizer makes this process easy with the
transform method.</p>
<p>At prediction time, one assigns the class for which the corresponding
model gave the greatest confidence. LabelBinarizer makes this easy
with the inverse_transform method.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>neg_label <span class="classifier-delimiter">:</span> <span class="classifier">int (default: 0)</span></dt>
<dd>Value with which negative labels must be encoded.</dd>
<dt>pos_label <span class="classifier-delimiter">:</span> <span class="classifier">int (default: 1)</span></dt>
<dd>Value with which positive labels must be encoded.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>classes_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape [n_class]</span></dt>
<dd>Holds the label for each class.</dd>
<dt><cite>multilabel_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd>True if the transformer was fitted on a multilabel rather than a
multiclass set of labels.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">preprocessing</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lb</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">LabelBinarizer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lb</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="go">LabelBinarizer(neg_label=0, pos_label=1)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lb</span><span class="o">.</span><span class="n">classes_</span>
<span class="go">array([1, 2, 4, 6])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lb</span><span class="o">.</span><span class="n">multilabel_</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lb</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="go">array([[1, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 1]])</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">lb</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">([(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)])</span>
<span class="go">array([[1, 1, 0],</span>
<span class="go">       [0, 0, 1]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lb</span><span class="o">.</span><span class="n">classes_</span>
<span class="go">array([1, 2, 3])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lb</span><span class="o">.</span><span class="n">multilabel_</span>
<span class="go">True</span>
</pre></div>
</div>
<p>See also</p>
<dl class="docutils">
<dt>label_binarize <span class="classifier-delimiter">:</span> <span class="classifier">function to perform the transform operation of</span></dt>
<dd>LabelBinarizer with fixed classes.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>LabelBinarizerSklearn</strong></li>
<li><strong>LabelBinarizerSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-labelencodersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.LabelEncoderSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.LabelEncoderSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.LabelEncoderSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-labelencodersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.LabelEncoderSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">LabelEncoderSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.LabelEncoderSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Encode labels with value between 0 and n_classes-1.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.preprocessing.label.LabelEncoder</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>classes_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape (n_class,)</span></dt>
<dd>Holds the label for each class.</dd>
</dl>
<p><strong>Examples</strong></p>
<p><cite>LabelEncoder</cite> can be used to normalize labels.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">preprocessing</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">le</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">LabelEncoder</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">le</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="go">LabelEncoder()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">le</span><span class="o">.</span><span class="n">classes_</span>
<span class="go">array([1, 2, 6])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">le</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span> 
<span class="go">array([0, 0, 1, 2]...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">le</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="go">array([1, 1, 2, 6])</span>
</pre></div>
</div>
<p>It can also be used to transform non-numerical labels (as long as they are
hashable and comparable) to numerical labels.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">le</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">LabelEncoder</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">le</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="s">&quot;paris&quot;</span><span class="p">,</span> <span class="s">&quot;paris&quot;</span><span class="p">,</span> <span class="s">&quot;tokyo&quot;</span><span class="p">,</span> <span class="s">&quot;amsterdam&quot;</span><span class="p">])</span>
<span class="go">LabelEncoder()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">le</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>
<span class="go">[&#39;amsterdam&#39;, &#39;paris&#39;, &#39;tokyo&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">le</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="s">&quot;tokyo&quot;</span><span class="p">,</span> <span class="s">&quot;tokyo&quot;</span><span class="p">,</span> <span class="s">&quot;paris&quot;</span><span class="p">])</span> 
<span class="go">array([2, 2, 1]...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">le</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
<span class="go">[&#39;tokyo&#39;, &#39;tokyo&#39;, &#39;paris&#39;]</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>LabelEncoderSklearn</strong></li>
<li><strong>LabelEncoderSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-labelpropagationsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.LabelPropagationSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.LabelPropagationSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.LabelPropagationSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-labelpropagationsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.LabelPropagationSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">LabelPropagationSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.LabelPropagationSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Label Propagation classifier</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.semi_supervised.label_propagation.LabelPropagation</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>kernel <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;knn&#8217;, &#8216;rbf&#8217;}</span></dt>
<dd>String identifier for kernel function to use.
Only &#8216;rbf&#8217; and &#8216;knn&#8217; kernels are currently supported..</dd>
<dt>gamma <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>parameter for rbf kernel</dd>
<dt>n_neighbors <span class="classifier-delimiter">:</span> <span class="classifier">integer &gt; 0</span></dt>
<dd>parameter for knn kernel</dd>
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>clamping factor</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>change maximum number of iterations allowed</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Convergence tolerance: threshold to consider the system at steady
state</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.semi_supervised</span> <span class="kn">import</span> <span class="n">LabelPropagation</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">label_prop_model</span> <span class="o">=</span> <span class="n">LabelPropagation</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">random_unlabeled_points</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random_integers</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span><span class="p">[</span><span class="n">random_unlabeled_points</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">label_prop_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="gp">... </span>
<span class="go">LabelPropagation(...)</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<p>Xiaojin Zhu and Zoubin Ghahramani. Learning from labeled and unlabeled data
with label propagation. Technical Report CMU-CALD-02-107, Carnegie Mellon
University, 2002 <a class="reference external" href="http://pages.cs.wisc.edu/~jerryzhu/pub/CMU-CALD-02-107.pdf">http://pages.cs.wisc.edu/~jerryzhu/pub/CMU-CALD-02-107.pdf</a></p>
<p>See Also</p>
<p>LabelSpreading : Alternate label propagation strategy more robust to noise</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>LabelPropagationSklearn</strong></li>
<li><strong>LabelPropagationSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-labelspreadingsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.LabelSpreadingSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.LabelSpreadingSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.LabelSpreadingSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-labelspreadingsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.LabelSpreadingSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">LabelSpreadingSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.LabelSpreadingSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>LabelSpreading model for semi-supervised learning</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.semi_supervised.label_propagation.LabelSpreading</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>This model is similar to the basic Label Propgation algorithm,
but uses affinity matrix based on the normalized graph Laplacian
and soft clamping across the labels.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>kernel <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;knn&#8217;, &#8216;rbf&#8217;}</span></dt>
<dd>String identifier for kernel function to use.
Only &#8216;rbf&#8217; and &#8216;knn&#8217; kernels are currently supported.</dd>
<dt>gamma <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>parameter for rbf kernel</dd>
<dt>n_neighbors <span class="classifier-delimiter">:</span> <span class="classifier">integer &gt; 0</span></dt>
<dd>parameter for knn kernel</dd>
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>clamping factor</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>maximum number of iterations allowed</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Convergence tolerance: threshold to consider the system at steady
state</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.semi_supervised</span> <span class="kn">import</span> <span class="n">LabelSpreading</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">label_prop_model</span> <span class="o">=</span> <span class="n">LabelSpreading</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">random_unlabeled_points</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random_integers</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span><span class="p">[</span><span class="n">random_unlabeled_points</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">label_prop_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="gp">... </span>
<span class="go">LabelSpreading(...)</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<p>Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston,
Bernhard Schölkopf. Learning with local and global consistency (2004)
<a class="reference external" href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.115.3219">http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.115.3219</a></p>
<p>See Also</p>
<p>LabelPropagation : Unregularized graph based semi-supervised learning</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>LabelSpreadingSklearnNode</strong></li>
<li><strong>LabelSpreadingSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-linearsvcsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.LinearSVCSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.LinearSVCSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.LinearSVCSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-linearsvcsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.LinearSVCSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">LinearSVCSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.LinearSVCSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Linear Support Vector Classification.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.svm.classes.LinearSVC</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Similar to SVC with parameter kernel=&#8217;linear&#8217;, but implemented in terms of
liblinear rather than libsvm, so it has more flexibility in the choice of
penalties and loss functions and should scale better (to large numbers of
samples).</p>
<p>This class supports both dense and sparse input and the multiclass support
is handled according to a one-vs-the-rest scheme.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>C <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1.0)</span></dt>
<dd>Penalty parameter C of the error term.</dd>
<dt>loss <span class="classifier-delimiter">:</span> <span class="classifier">string, &#8216;l1&#8217; or &#8216;l2&#8217; (default=&#8217;l2&#8217;)</span></dt>
<dd>Specifies the loss function. &#8216;l1&#8217; is the hinge loss (standard SVM)
while &#8216;l2&#8217; is the squared hinge loss.</dd>
<dt>penalty <span class="classifier-delimiter">:</span> <span class="classifier">string, &#8216;l1&#8217; or &#8216;l2&#8217; (default=&#8217;l2&#8217;)</span></dt>
<dd>Specifies the norm used in the penalization. The &#8216;l2&#8217;
penalty is the standard used in SVC. The &#8216;l1&#8217; leads to <cite>coef_</cite>
vectors that are sparse.</dd>
<dt>dual <span class="classifier-delimiter">:</span> <span class="classifier">bool, (default=True)</span></dt>
<dd>Select the algorithm to either solve the dual or primal
optimization problem. Prefer dual=False when n_samples &gt; n_features.</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1e-4)</span></dt>
<dd>Tolerance for stopping criteria</dd>
<dt>multi_class: string, &#8216;ovr&#8217; or &#8216;crammer_singer&#8217; (default=&#8217;ovr&#8217;)</dt>
<dd>Determines the multi-class strategy if <cite>y</cite> contains more than
two classes.
<cite>ovr</cite> trains n_classes one-vs-rest classifiers, while <cite>crammer_singer</cite>
optimizes a joint objective over all classes.
While <cite>crammer_singer</cite> is interesting from an theoretical perspective
as it is consistent it is seldom used in practice and rarely leads to
better accuracy and is more expensive to compute.
If <cite>crammer_singer</cite> is chosen, the options loss, penalty and dual will
be ignored.</dd>
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=True)</span></dt>
<dd>Whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>intercept_scaling <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1)</span></dt>
<dd>when self.fit_intercept is True, instance vector x becomes
[x, self.intercept_scaling],
i.e. a &#8220;synthetic&#8221; feature with constant value equals to
intercept_scaling is appended to the instance vector.
The intercept becomes intercept_scaling * synthetic feature weight
Note! the synthetic feature weight is subject to l1/l2 regularization
as all other features.
To lessen the effect of regularization on synthetic feature weight
(and therefore on the intercept) intercept_scaling has to be increased</dd>
<dt>class_weight <span class="classifier-delimiter">:</span> <span class="classifier">{dict, &#8216;auto&#8217;}, optional</span></dt>
<dd>Set the parameter C of class i to class_weight[i]*C for
SVC. If not given, all classes are supposed to have
weight one. The &#8216;auto&#8217; mode uses the values of y to
automatically adjust weights inversely proportional to
class frequencies.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">int, default: 0</span></dt>
<dd>Enable verbose output. Note that this setting takes advantage of a
per-process runtime setting in liblinear that, if enabled, may not work
properly in a multithreaded context.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int seed, RandomState instance, or None (default)</span></dt>
<dd>The seed of the pseudo random number generator to use when
shuffling the data.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>coef_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_features] if n_classes == 2             else [n_classes, n_features]</span></dt>
<dd><p class="first">Weights asigned to the features (coefficients in the primal
problem). This is only available in the case of linear kernel.</p>
<p class="last"><cite>coef_</cite> is readonly property derived from <cite>raw_coef_</cite> that         follows the internal memory layout of liblinear.</p>
</dd>
<dt><cite>intercept_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [1] if n_classes == 2 else [n_classes]</span></dt>
<dd>Constants in decision function.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>The underlying C implementation uses a random number generator to
select features when fitting the model. It is thus not uncommon,
to have slightly different results for the same input data. If
that happens, try with a smaller tol parameter.</p>
<p>The underlying implementation (liblinear) uses a sparse internal
representation for the data that will incur a memory copy.</p>
<p><strong>References:</strong>
<a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/liblinear/">LIBLINEAR: A Library for Large Linear Classification</a></p>
<p>See also</p>
<dl class="docutils">
<dt>SVC</dt>
<dd><p class="first">Implementation of Support Vector Machine classifier using libsvm:</p>
<ul class="simple">
<li>the kernel can be non-linear but its SMO algorithm does not</li>
<li>scale to large number of samples as LinearSVC does.</li>
</ul>
<p>Furthermore SVC multi-class mode is implemented using one
vs one scheme while LinearSVC uses one vs the rest. It is
possible to implement one vs the rest with SVC by using the
<tt class="xref py py-class docutils literal"><span class="pre">sklearn.multiclass.OneVsRestClassifier</span></tt> wrapper.</p>
<p class="last">Finally SVC can fit dense data without memory copy if the input
is C-contiguous. Sparse data will still incur memory copy though.</p>
</dd>
<dt>sklearn.linear_model.SGDClassifier</dt>
<dd><p class="first">SGDClassifier can optimize the same cost function as LinearSVC
by adjusting the penalty and loss parameters. Furthermore
SGDClassifier is scalable to large number of samples as it uses
a Stochastic Gradient Descent optimizer.</p>
<p class="last">Finally SGDClassifier can fit both dense and sparse data without
memory copy if the input is C-contiguous or CSR.</p>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>LinearSVCSklearn</strong></li>
<li><strong>LinearSVCSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-locallylinearembeddingsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.LocallyLinearEmbeddingSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.LocallyLinearEmbeddingSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.LocallyLinearEmbeddingSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-locallylinearembeddingsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.LocallyLinearEmbeddingSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">LocallyLinearEmbeddingSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.LocallyLinearEmbeddingSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Locally Linear Embedding</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.manifold.locally_linear.LocallyLinearEmbedding</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_neighbors <span class="classifier-delimiter">:</span> <span class="classifier">integer</span></dt>
<dd>number of neighbors to consider for each point.</dd>
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">integer</span></dt>
<dd>number of coordinates for the manifold</dd>
<dt>reg <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>regularization constant, multiplies the trace of the local covariance
matrix of the distances.</dd>
<dt>eigen_solver <span class="classifier-delimiter">:</span> <span class="classifier">string, {&#8216;auto&#8217;, &#8216;arpack&#8217;, &#8216;dense&#8217;}</span></dt>
<dd><p class="first">auto : algorithm will attempt to choose the best method for input data</p>
<dl class="last docutils">
<dt>arpack <span class="classifier-delimiter">:</span> <span class="classifier">use arnoldi iteration in shift-invert mode.</span></dt>
<dd>For this method, M may be a dense matrix, sparse matrix,
or general linear operator.
Warning: ARPACK can be unstable for some problems.  It is
best to try several random seeds in order to check results.</dd>
<dt>dense <span class="classifier-delimiter">:</span> <span class="classifier">use standard dense matrix operations for the eigenvalue</span></dt>
<dd>decomposition.  For this method, M must be an array
or matrix type.  This method should be avoided for
large problems.</dd>
</dl>
</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Tolerance for &#8216;arpack&#8217; method
Not used if eigen_solver==&#8217;dense&#8217;.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">integer</span></dt>
<dd>maximum number of iterations for the arpack solver.
Not used if eigen_solver==&#8217;dense&#8217;.</dd>
<dt>method <span class="classifier-delimiter">:</span> <span class="classifier">string (&#8216;standard&#8217;, &#8216;hessian&#8217;, &#8216;modified&#8217; or &#8216;ltsa&#8217;)</span></dt>
<dd><dl class="first last docutils">
<dt>standard <span class="classifier-delimiter">:</span> <span class="classifier">use the standard locally linear embedding algorithm.  see</span></dt>
<dd>reference [1]</dd>
<dt>hessian <span class="classifier-delimiter">:</span> <span class="classifier">use the Hessian eigenmap method. This method requires</span></dt>
<dd><tt class="docutils literal"><span class="pre">n_neighbors</span> <span class="pre">&gt;</span> <span class="pre">n_components</span> <span class="pre">*</span> <span class="pre">(1</span> <span class="pre">+</span> <span class="pre">(n_components</span> <span class="pre">+</span> <span class="pre">1)</span> <span class="pre">/</span> <span class="pre">2</span></tt>
see reference [2]</dd>
<dt>modified <span class="classifier-delimiter">:</span> <span class="classifier">use the modified locally linear embedding algorithm.</span></dt>
<dd>see reference [3]</dd>
<dt>ltsa <span class="classifier-delimiter">:</span> <span class="classifier">use local tangent space alignment algorithm</span></dt>
<dd>see reference [4]</dd>
</dl>
</dd>
<dt>hessian_tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Tolerance for Hessian eigenmapping method.
Only used if <tt class="docutils literal"><span class="pre">method</span> <span class="pre">==</span> <span class="pre">'hessian'</span></tt></dd>
<dt>modified_tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Tolerance for modified LLE method.
Only used if <tt class="docutils literal"><span class="pre">method</span> <span class="pre">==</span> <span class="pre">'modified'</span></tt></dd>
<dt>neighbors_algorithm <span class="classifier-delimiter">:</span> <span class="classifier">string [&#8216;auto&#8217;|&#8217;brute&#8217;|&#8217;kd_tree&#8217;|&#8217;ball_tree&#8217;]</span></dt>
<dd>algorithm to use for nearest neighbors search,
passed to neighbors.NearestNeighbors instance</dd>
<dt>random_state: numpy.RandomState or int, optional</dt>
<dd>The generator or seed used to determine the starting vector for arpack
iterations.  Defaults to numpy.random.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>embedding_vectors_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape [n_components, n_samples]</span></dt>
<dd>Stores the embedding vectors</dd>
<dt><cite>reconstruction_error_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Reconstruction error associated with <cite>embedding_vectors_</cite></dd>
<dt><cite>nbrs_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">NearestNeighbors object</span></dt>
<dd>Stores nearest neighbors instance, including BallTree or KDtree
if applicable.</dd>
</dl>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id20" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><cite>Roweis, S. &amp; Saul, L. Nonlinear dimensionality reduction
by locally linear embedding.  Science 290:2323 (2000).</cite></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id21" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td><cite>Donoho, D. &amp; Grimes, C. Hessian eigenmaps: Locally
linear embedding techniques for high-dimensional data.
Proc Natl Acad Sci U S A.  100:5591 (2003).</cite></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id22" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[3]</td><td><cite>Zhang, Z. &amp; Wang, J. MLLE: Modified Locally Linear
Embedding Using Multiple Weights.</cite>
<a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382">http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id23" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[4]</td><td><cite>Zhang, Z. &amp; Zha, H. Principal manifolds and nonlinear
dimensionality reduction via tangent space alignment.
Journal of Shanghai Univ.  8:406 (2004)</cite></td></tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>LocallyLinearEmbeddingSklearnNode</strong></li>
<li><strong>LocallyLinearEmbeddingSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-logisticregressionsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.LogisticRegressionSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.LogisticRegressionSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.LogisticRegressionSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-logisticregressionsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.LogisticRegressionSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">LogisticRegressionSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.LogisticRegressionSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Logistic Regression (aka logit, MaxEnt) classifier.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.linear_model.logistic.LogisticRegression</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>In the multiclass case, the training algorithm uses a one-vs.-all (OvA)
scheme, rather than the &#8220;true&#8221; multinomial LR.</p>
<p>This class implements L1 and L2 regularized logistic regression using the
<cite>liblinear</cite> library. It can handle both dense and sparse input. Use
C-ordered arrays or CSR matrices containing 64-bit floats for optimal
performance; any other input format will be converted (and copied).</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>penalty <span class="classifier-delimiter">:</span> <span class="classifier">string, &#8216;l1&#8217; or &#8216;l2&#8217;</span></dt>
<dd>Used to specify the norm used in the penalization.</dd>
<dt>dual <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd>Dual or primal formulation. Dual formulation is only
implemented for l2 penalty. Prefer dual=False when
n_samples &gt; n_features.</dd>
<dt>C <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1.0)</span></dt>
<dd>Inverse of regularization strength; must be a positive float.
Like in support vector machines, smaller values specify stronger
regularization.</dd>
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">bool, default: True</span></dt>
<dd>Specifies if a constant (a.k.a. bias or intercept) should be
added the decision function.</dd>
<dt>intercept_scaling <span class="classifier-delimiter">:</span> <span class="classifier">float, default: 1</span></dt>
<dd>when self.fit_intercept is True, instance vector x becomes
[x, self.intercept_scaling],
i.e. a &#8220;synthetic&#8221; feature with constant value equals to
intercept_scaling is appended to the instance vector.
The intercept becomes intercept_scaling * synthetic feature weight
Note! the synthetic feature weight is subject to l1/l2 regularization
as all other features.
To lessen the effect of regularization on synthetic feature weight
(and therefore on the intercept) intercept_scaling has to be increased</dd>
<dt>class_weight <span class="classifier-delimiter">:</span> <span class="classifier">{dict, &#8216;auto&#8217;}, optional</span></dt>
<dd>Over-/undersamples the samples of each class according to the given
weights. If not given, all classes are supposed to have weight one.
The &#8216;auto&#8217; mode selects weights inversely proportional to class
frequencies in the training set.</dd>
<dt>tol: float, optional</dt>
<dd>Tolerance for stopping criteria.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>coef_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_classes-1, n_features]</span></dt>
<dd><p class="first">Coefficient of the features in the decision function.</p>
<p class="last"><cite>coef_</cite> is readonly property derived from <cite>raw_coef_</cite> that         follows the internal memory layout of liblinear.</p>
</dd>
<dt><cite>intercept_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_classes-1]</span></dt>
<dd>Intercept (a.k.a. bias) added to the decision function.
It is available only when parameter intercept is set to True.</dd>
<dt>random_state: int seed, RandomState instance, or None (default)</dt>
<dd>The seed of the pseudo random number generator to use when
shuffling the data.</dd>
</dl>
<p>See also</p>
<p>LinearSVC</p>
<p><strong>Notes</strong></p>
<p>The underlying C implementation uses a random number generator to
select features when fitting the model. It is thus not uncommon,
to have slightly different results for the same input data. If
that happens, try with a smaller tol parameter.</p>
<p>References:</p>
<dl class="docutils">
<dt>LIBLINEAR &#8211; A Library for Large Linear Classification</dt>
<dd><a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/liblinear/">http://www.csie.ntu.edu.tw/~cjlin/liblinear/</a></dd>
<dt>Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent</dt>
<dd>methods for logistic regression and maximum entropy models.
Machine Learning 85(1-2):41-75.
<a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf">http://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf</a></dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>LogisticRegressionSklearn</strong></li>
<li><strong>LogisticRegressionSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-minmaxscalersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.MinMaxScalerSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.MinMaxScalerSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.MinMaxScalerSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-minmaxscalersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.MinMaxScalerSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">MinMaxScalerSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.MinMaxScalerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Standardizes features by scaling each feature to a given range.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.preprocessing.data.MinMaxScaler</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>This estimator scales and translates each feature individually such
that it is in the given range on the training set, i.e. between
zero and one.</p>
<p>The standardization is given by:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="o">-</span> <span class="n">X_std</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
<span class="o">-</span> <span class="n">X_scaled</span> <span class="o">=</span> <span class="n">X_std</span> <span class="o">*</span> <span class="p">(</span><span class="nb">max</span> <span class="o">-</span> <span class="nb">min</span><span class="p">)</span> <span class="o">+</span> <span class="nb">min</span>
</pre></div>
</div>
<p>where min, max = feature_range.</p>
<p>This standardization is often used as an alternative to zero mean,
unit variance scaling.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>feature_range: tuple (min, max), default=(0, 1)</dt>
<dd>Desired range of transformed data.</dd>
<dt>copy <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default is True</span></dt>
<dd>Set to False to perform inplace row normalization and avoid a
copy (if the input is already a numpy array).</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>min_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">ndarray, shape (n_features,)</span></dt>
<dd>Per feature adjustment for minimum.</dd>
<dt><cite>scale_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">ndarray, shape (n_features,)</span></dt>
<dd>Per feature relative scaling of the data.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>MinMaxScalerSklearnNode</strong></li>
<li><strong>MinMaxScalerSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-minibatchdictionarylearningsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.MiniBatchDictionaryLearningSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.MiniBatchDictionaryLearningSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.MiniBatchDictionaryLearningSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-minibatchdictionarylearningsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.MiniBatchDictionaryLearningSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">MiniBatchDictionaryLearningSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.MiniBatchDictionaryLearningSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Mini-batch dictionary learning</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.decomposition.dict_learning.MiniBatchDictionaryLearning</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Finds a dictionary (a set of atoms) that can best be used to represent data
using a sparse code.</p>
<p>Solves the optimization problem:</p>
<div class="highlight-python"><div class="highlight"><pre>(U^*,V^*) = argmin 0.5 || Y - U V ||_2^2 + alpha * || U ||_1
             (U,V)
             with || V_k ||_2 = 1 for all  0 &lt;= k &lt; n_components
</pre></div>
</div>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>number of dictionary elements to extract</dd>
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>sparsity controlling parameter</dd>
<dt>n_iter <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>total number of iterations to perform</dd>
<dt>fit_algorithm <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;lars&#8217;, &#8216;cd&#8217;}</span></dt>
<dd>lars: uses the least angle regression method to solve the lasso problem
(linear_model.lars_path)
cd: uses the coordinate descent method to compute the
Lasso solution (linear_model.Lasso). Lars will be faster if
the estimated components are sparse.</dd>
<dt>transform_algorithm <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;lasso_lars&#8217;, &#8216;lasso_cd&#8217;, &#8216;lars&#8217;, &#8216;omp&#8217;,     &#8216;threshold&#8217;}</span></dt>
<dd>Algorithm used to transform the data.
lars: uses the least angle regression method (linear_model.lars_path)
lasso_lars: uses Lars to compute the Lasso solution
lasso_cd: uses the coordinate descent method to compute the
Lasso solution (linear_model.Lasso). lasso_lars will be faster if
the estimated components are sparse.
omp: uses orthogonal matching pursuit to estimate the sparse solution
threshold: squashes to zero all coefficients less than alpha from
the projection dictionary * X&#8217;</dd>
<dt>transform_n_nonzero_coefs <span class="classifier-delimiter">:</span> <span class="classifier">int, <tt class="docutils literal"><span class="pre">0.1</span> <span class="pre">*</span> <span class="pre">n_features</span></tt> by default</span></dt>
<dd>Number of nonzero coefficients to target in each column of the
solution. This is only used by <cite>algorithm=&#8217;lars&#8217;</cite> and <cite>algorithm=&#8217;omp&#8217;</cite>
and is overridden by <cite>alpha</cite> in the <cite>omp</cite> case.</dd>
<dt>transform_alpha <span class="classifier-delimiter">:</span> <span class="classifier">float, 1. by default</span></dt>
<dd>If <cite>algorithm=&#8217;lasso_lars&#8217;</cite> or <cite>algorithm=&#8217;lasso_cd&#8217;</cite>, <cite>alpha</cite> is the
penalty applied to the L1 norm.
If <cite>algorithm=&#8217;threshold&#8217;</cite>, <cite>alpha</cite> is the absolute value of the
threshold below which coefficients will be squashed to zero.
If <cite>algorithm=&#8217;omp&#8217;</cite>, <cite>alpha</cite> is the tolerance parameter: the value of
the reconstruction error targeted. In this case, it overrides
<cite>n_nonzero_coefs</cite>.</dd>
<dt>split_sign <span class="classifier-delimiter">:</span> <span class="classifier">bool, False by default</span></dt>
<dd>Whether to split the sparse feature vector into the concatenation of
its negative part and its positive part. This can improve the
performance of downstream classifiers.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>number of parallel jobs to run</dd>
<dt>dict_init <span class="classifier-delimiter">:</span> <span class="classifier">array of shape (n_components, n_features),</span></dt>
<dd>initial value of the dictionary for warm restart scenarios</dd>
</dl>
<p>verbose :</p>
<blockquote>
<div><ul class="simple">
<li>degree of verbosity of the printed output</li>
</ul>
</div></blockquote>
<dl class="docutils">
<dt>batch_size <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>number of samples in each mini-batch</dd>
<dt>shuffle <span class="classifier-delimiter">:</span> <span class="classifier">bool,</span></dt>
<dd>whether to shuffle the samples before forming batches</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int or RandomState</span></dt>
<dd>Pseudo number generator state used for random sampling.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>components_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span></dt>
<dd>components extracted from the data</dd>
</dl>
<p><strong>Notes</strong></p>
<p><strong>References:</strong></p>
<p>J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning
for sparse coding (<a class="reference external" href="http://www.di.ens.fr/sierra/pdfs/icml09.pdf">http://www.di.ens.fr/sierra/pdfs/icml09.pdf</a>)</p>
<p>See also</p>
<p>SparseCoder
DictionaryLearning
SparsePCA
MiniBatchSparsePCA</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>MiniBatchDictionaryLearningSklearn</strong></li>
<li><strong>MiniBatchDictionaryLearningSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-minibatchsparsepcasklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.MiniBatchSparsePCASklearnNode" title="pySPACE.missions.nodes.scikits_nodes.MiniBatchSparsePCASklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.MiniBatchSparsePCASklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-minibatchsparsepcasklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.MiniBatchSparsePCASklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">MiniBatchSparsePCASklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.MiniBatchSparsePCASklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Mini-batch Sparse Principal Components Analysis</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.decomposition.sparse_pca.MiniBatchSparsePCA</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Finds the set of sparse components that can optimally reconstruct
the data.  The amount of sparseness is controllable by the coefficient
of the L1 penalty, given by the parameter alpha.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>number of sparse atoms to extract</dd>
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>Sparsity controlling parameter. Higher values lead to sparser
components.</dd>
<dt>ridge_alpha <span class="classifier-delimiter">:</span> <span class="classifier">float,</span></dt>
<dd>Amount of ridge shrinkage to apply in order to improve
conditioning when calling the transform method.</dd>
<dt>n_iter <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>number of iterations to perform for each mini batch</dd>
<dt>callback <span class="classifier-delimiter">:</span> <span class="classifier">callable,</span></dt>
<dd>callable that gets invoked every five iterations</dd>
<dt>batch_size <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>the number of features to take in each mini batch</dd>
</dl>
<p>verbose :</p>
<blockquote>
<div><ul class="simple">
<li>degree of output the procedure will print</li>
</ul>
</div></blockquote>
<dl class="docutils">
<dt>shuffle <span class="classifier-delimiter">:</span> <span class="classifier">boolean,</span></dt>
<dd>whether to shuffle the data before splitting it in batches</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>number of parallel jobs to run, or -1 to autodetect.</dd>
<dt>method <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;lars&#8217;, &#8216;cd&#8217;}</span></dt>
<dd>lars: uses the least angle regression method to solve the lasso problem
(linear_model.lars_path)
cd: uses the coordinate descent method to compute the
Lasso solution (linear_model.Lasso). Lars will be faster if
the estimated components are sparse.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int or RandomState</span></dt>
<dd>Pseudo number generator state used for random sampling.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>components_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span></dt>
<dd>Sparse components extracted from the data.</dd>
<dt><cite>error_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array</span></dt>
<dd>Vector of errors at each iteration.</dd>
</dl>
<p>See also</p>
<p>PCA
SparsePCA
DictionaryLearning</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>MiniBatchSparsePCASklearnNode</strong></li>
<li><strong>MiniBatchSparsePCASklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-multinomialnbsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.MultinomialNBSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.MultinomialNBSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.MultinomialNBSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-multinomialnbsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.MultinomialNBSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">MultinomialNBSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.MultinomialNBSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Naive Bayes classifier for multinomial models</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.naive_bayes.MultinomialNB</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>The multinomial Naive Bayes classifier is suitable for classification with
discrete features (e.g., word counts for text classification). The
multinomial distribution normally requires integer feature counts. However,
in practice, fractional counts such as tf-idf may also work.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1.0)</span></dt>
<dd>Additive (Laplace/Lidstone) smoothing parameter
(0 for no smoothing).</dd>
<dt>fit_prior <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd>Whether to learn class prior probabilities or not.
If false, a uniform prior will be used.</dd>
<dt>class_prior <span class="classifier-delimiter">:</span> <span class="classifier">array-like, size (n_classes,)</span></dt>
<dd>Prior probabilities of the classes. If specified the priors are not
adjusted according to the data.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>class_log_prior_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_classes, )</span></dt>
<dd>Smoothed empirical log probability for each class.</dd>
<dt><cite>intercept_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">property</span></dt>
<dd>Mirrors <tt class="docutils literal"><span class="pre">class_log_prior_</span></tt> for interpreting MultinomialNB
as a linear model.</dd>
<dt><cite>feature_log_prob_</cite>: array, shape (n_classes, n_features)</dt>
<dd>Empirical log probability of features
given a class, <tt class="docutils literal"><span class="pre">P(x_i|y)</span></tt>.</dd>
<dt><cite>coef_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">property</span></dt>
<dd>Mirrors <tt class="docutils literal"><span class="pre">feature_log_prob_</span></tt> for interpreting MultinomialNB
as a linear model.</dd>
<dt><cite>class_count_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_classes,)</span></dt>
<dd>Number of samples encountered for each class during fitting. This
value is weighted by the sample weight when provided.</dd>
<dt><cite>feature_count_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_classes, n_features)</span></dt>
<dd>Number of samples encountered for each (class, feature)
during fitting. This value is weighted by the sample weight when
provided.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">MultinomialNB</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">MultinomialNB</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
<span class="go">[3]</span>
</pre></div>
</div>
<p><strong>Notes</strong></p>
<p>For the rationale behind the names <cite>coef_</cite> and <cite>intercept_</cite>, i.e.
naive Bayes as a linear classifier, see J. Rennie et al. (2003),
Tackling the poor assumptions of naive Bayes text classifiers, ICML.</p>
<p><strong>References</strong></p>
<p>C.D. Manning, P. Raghavan and H. Schütze (2008). Introduction to
Information Retrieval. Cambridge University Press, pp. 234–265.
<a class="reference external" href="http://nlp.stanford.edu/IR-book/html/htmledition/">http://nlp.stanford.edu/IR-book/html/htmledition/</a></p>
<blockquote>
<div>naive-bayes-text-classification-1.html</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>MultinomialNBSklearn</strong></li>
<li><strong>MultinomialNBSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-nmfsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.NMFSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.NMFSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.NMFSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-nmfsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.NMFSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">NMFSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.NMFSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Non-Negative matrix factorization by Projected Gradient (NMF)</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.decomposition.nmf.NMF</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int or None</span></dt>
<dd>Number of components, if n_components is not set all components
are kept</dd>
<dt>init <span class="classifier-delimiter">:</span> <span class="classifier">&#8216;nndsvd&#8217; |  &#8216;nndsvda&#8217; | &#8216;nndsvdar&#8217; | &#8216;random&#8217;</span></dt>
<dd><p class="first">Method used to initialize the procedure.
Default: &#8216;nndsvdar&#8217; if n_components &lt; n_features, otherwise random.
Valid options:</p>
<div class="last highlight-python"><div class="highlight"><pre>&#39;nndsvd&#39;: Nonnegative Double Singular Value Decomposition (NNDSVD)
    initialization (better for sparseness)
&#39;nndsvda&#39;: NNDSVD with zeros filled with the average of X
    (better when sparsity is not desired)
&#39;nndsvdar&#39;: NNDSVD with zeros filled with small random values
    (generally faster, less accurate alternative to NNDSVDa
    for when sparsity is not desired)
&#39;random&#39;: non-negative random matrices
</pre></div>
</div>
</dd>
<dt>sparseness <span class="classifier-delimiter">:</span> <span class="classifier">&#8216;data&#8217; | &#8216;components&#8217; | None, default: None</span></dt>
<dd>Where to enforce sparsity in the model.</dd>
<dt>beta <span class="classifier-delimiter">:</span> <span class="classifier">double, default: 1</span></dt>
<dd>Degree of sparseness, if sparseness is not None. Larger values mean
more sparseness.</dd>
<dt>eta <span class="classifier-delimiter">:</span> <span class="classifier">double, default: 0.1</span></dt>
<dd>Degree of correctness to maintain, if sparsity is not None. Smaller
values mean larger error.</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">double, default: 1e-4</span></dt>
<dd>Tolerance value used in stopping conditions.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, default: 200</span></dt>
<dd>Number of iterations to compute.</dd>
<dt>nls_max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, default: 2000</span></dt>
<dd>Number of iterations in NLS subproblem.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int or RandomState</span></dt>
<dd>Random number generator seed control.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>components_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span></dt>
<dd>Non-negative components of the data.</dd>
<dt><cite>reconstruction_err_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">number</span></dt>
<dd>Frobenius norm of the matrix difference between
the training data and the reconstructed data from
the fit produced by the model. <tt class="docutils literal"><span class="pre">||</span> <span class="pre">X</span> <span class="pre">-</span> <span class="pre">WH</span> <span class="pre">||_2</span></tt></dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">ProjectedGradientNMF</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ProjectedGradientNMF</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s">&#39;random&#39;</span><span class="p">,</span>
<span class="gp">... </span>                             <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> 
<span class="go">ProjectedGradientNMF(beta=1, eta=0.1, init=&#39;random&#39;, max_iter=200,</span>
<span class="go">        n_components=2, nls_max_iter=2000, random_state=0, sparseness=None,</span>
<span class="go">        tol=0.0001)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">components_</span>
<span class="go">array([[ 0.77032744,  0.11118662],</span>
<span class="go">       [ 0.38526873,  0.38228063]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">reconstruction_err_</span> 
<span class="go">0.00746...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ProjectedGradientNMF</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="gp">... </span>             <span class="n">sparseness</span><span class="o">=</span><span class="s">&#39;components&#39;</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s">&#39;random&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> 
<span class="go">ProjectedGradientNMF(beta=1, eta=0.1, init=&#39;random&#39;, max_iter=200,</span>
<span class="go">            n_components=2, nls_max_iter=2000, random_state=0,</span>
<span class="go">            sparseness=&#39;components&#39;, tol=0.0001)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">components_</span>
<span class="go">array([[ 1.67481991,  0.29614922],</span>
<span class="go">       [ 0.        ,  0.4681982 ]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">reconstruction_err_</span> 
<span class="go">0.513...</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<p>This implements</p>
<p>C.-J. Lin. Projected gradient methods
for non-negative matrix factorization. Neural
Computation, 19(2007), 2756-2779.
<a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/nmf/">http://www.csie.ntu.edu.tw/~cjlin/nmf/</a></p>
<p>P. Hoyer. Non-negative Matrix Factorization with
Sparseness Constraints. Journal of Machine Learning
Research 2004.</p>
<p>NNDSVD is introduced in</p>
<p>C. Boutsidis, E. Gallopoulos: SVD based
initialization: A head start for nonnegative
matrix factorization - Pattern Recognition, 2008
<a class="reference external" href="http://tinyurl.com/nndsvd">http://tinyurl.com/nndsvd</a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>NMFSklearn</strong></li>
<li><strong>NMFSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-nearestcentroidsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.NearestCentroidSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.NearestCentroidSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.NearestCentroidSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-nearestcentroidsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.NearestCentroidSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">NearestCentroidSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.NearestCentroidSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Nearest centroid classifier.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.neighbors.nearest_centroid.NearestCentroid</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Each class is represented by its centroid, with test samples classified to
the class with the nearest centroid.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>metric: string, or callable</dt>
<dd>The metric to use when calculating distance between instances in a
feature array. If metric is a string or callable, it must be one of
the options allowed by metrics.pairwise.pairwise_distances for its
metric parameter.</dd>
<dt>shrink_threshold <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default = None)</span></dt>
<dd>Threshold for shrinking centroids to remove features.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>centroids_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_classes, n_features]</span></dt>
<dd>Centroid of each class</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors.nearest_centroid</span> <span class="kn">import</span> <span class="n">NearestCentroid</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">NearestCentroid</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">NearestCentroid(metric=&#39;euclidean&#39;, shrink_threshold=None)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
<p>See also</p>
<p>sklearn.neighbors.KNeighborsClassifier: nearest neighbors classifier</p>
<p><strong>Notes</strong></p>
<p>When used for text classification with tf–idf vectors, this classifier is
also known as the Rocchio classifier.</p>
<p><strong>References</strong></p>
<p>Tibshirani, R., Hastie, T., Narasimhan, B., &amp; Chu, G. (2002). Diagnosis of
multiple cancer types by shrunken centroids of gene expression. Proceedings
of the National Academy of Sciences of the United States of America,
99(10), 6567-6572. The National Academy of Sciences.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>NearestCentroidSklearnNode</strong></li>
<li><strong>NearestCentroidSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-normalizersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.NormalizerSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.NormalizerSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.NormalizerSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-normalizersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.NormalizerSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">NormalizerSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.NormalizerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Normalize samples individually to unit norm</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.preprocessing.data.Normalizer</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Each sample (i.e. each row of the data matrix) with at least one
non zero component is rescaled independently of other samples so
that its norm (l1 or l2) equals one.</p>
<p>This transformer is able to work both with dense numpy arrays and
scipy.sparse matrix (use CSR format if you want to avoid the burden of
a copy / conversion).</p>
<p>Scaling inputs to unit norms is a common operation for text
classification or clustering for instance. For instance the dot
product of two l2-normalized TF-IDF vectors is the cosine similarity
of the vectors and is the base similarity metric for the Vector
Space Model commonly used by the Information Retrieval community.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>norm <span class="classifier-delimiter">:</span> <span class="classifier">&#8216;l1&#8217; or &#8216;l2&#8217;, optional (&#8216;l2&#8217; by default)</span></dt>
<dd>The norm to use to normalize each non zero sample.</dd>
<dt>copy <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default is True</span></dt>
<dd>set to False to perform inplace row normalization and avoid a
copy (if the input is already a numpy array or a scipy.sparse
CSR matrix).</dd>
</dl>
<p><strong>Notes</strong></p>
<p>This estimator is stateless (besides constructor parameters), the
fit method does nothing but is useful when used in a pipeline.</p>
<p>See also</p>
<p><tt class="xref py py-func docutils literal"><span class="pre">sklearn.preprocessing.normalize()</span></tt> equivalent function
without the object oriented API</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>NormalizerSklearn</strong></li>
<li><strong>NormalizerSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-nusvcsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.NuSVCSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.NuSVCSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.NuSVCSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-nusvcsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.NuSVCSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">NuSVCSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.NuSVCSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Nu-Support Vector Classification.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.svm.classes.NuSVC</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Similar to SVC but uses a parameter to control the number of support
vectors.</p>
<p>The implementation is based on libsvm.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>nu <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.5)</span></dt>
<dd>An upper bound on the fraction of training errors and a lower
bound of the fraction of support vectors. Should be in the
interval (0, 1].</dd>
<dt>kernel <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=&#8217;rbf&#8217;)</span></dt>
<dd>Specifies the kernel type to be used in the algorithm.
It must be one of &#8216;linear&#8217;, &#8216;poly&#8217;, &#8216;rbf&#8217;, &#8216;sigmoid&#8217;, &#8216;precomputed&#8217; or
a callable.
If none is given, &#8216;rbf&#8217; will be used. If a callable is given it is
used to precompute the kernel matrix.</dd>
<dt>degree <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=3)</span></dt>
<dd>degree of kernel function
is significant only in poly, rbf, sigmoid</dd>
<dt>gamma <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.0)</span></dt>
<dd>kernel coefficient for rbf and poly, if gamma is 0.0 then 1/n_features
will be taken.</dd>
<dt>coef0 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.0)</span></dt>
<dd>independent term in kernel function. It is only significant
in poly/sigmoid.</dd>
<dt>probability: boolean, optional (default=False)</dt>
<dd>Whether to enable probability estimates. This must be enabled prior
to calling <cite>fit</cite>, and will slow down that method.</dd>
<dt>shrinking: boolean, optional (default=True)</dt>
<dd>Whether to use the shrinking heuristic.</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1e-3)</span></dt>
<dd>Tolerance for stopping criterion.</dd>
<dt>cache_size <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Specify the size of the kernel cache (in MB)</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">bool, default: False</span></dt>
<dd>Enable verbose output. Note that this setting takes advantage of a
per-process runtime setting in libsvm that, if enabled, may not work
properly in a multithreaded context.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=-1)</span></dt>
<dd>Hard limit on iterations within solver, or -1 for no limit.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int seed, RandomState instance, or None (default)</span></dt>
<dd>The seed of the pseudo random number generator to use when
shuffling the data for probability estimation.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>support_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_SV]</span></dt>
<dd>Index of support vectors.</dd>
<dt><cite>support_vectors_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_SV, n_features]</span></dt>
<dd>Support vectors.</dd>
<dt><cite>n_support_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, dtype=int32, shape = [n_class]</span></dt>
<dd>number of support vector for each class.</dd>
<dt><cite>dual_coef_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_class-1, n_SV]</span></dt>
<dd>Coefficients of the support vector in the decision function.         For multiclass, coefficient for all 1-vs-1 classifiers.         The layout of the coefficients in the multiclass case is somewhat         non-trivial. See the section about multi-class classification in         the SVM section of the User Guide for details.</dd>
<dt><cite>coef_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_class-1, n_features]</span></dt>
<dd><p class="first">Weights asigned to the features (coefficients in the primal
problem). This is only available in the case of linear kernel.</p>
<p class="last"><cite>coef_</cite> is readonly property derived from <cite>dual_coef_</cite> and
<cite>support_vectors_</cite></p>
</dd>
<dt><cite>intercept_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_class * (n_class-1) / 2]</span></dt>
<dd>Constants in decision function.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">NuSVC</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">NuSVC</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">NuSVC(cache_size=200, coef0=0.0, degree=3, gamma=0.0, kernel=&#39;rbf&#39;,</span>
<span class="go">      max_iter=-1, nu=0.5, probability=False, random_state=None,</span>
<span class="go">      shrinking=True, tol=0.001, verbose=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
<p>See also</p>
<dl class="docutils">
<dt>SVC</dt>
<dd>Support Vector Machine for classification using libsvm.</dd>
<dt>LinearSVC</dt>
<dd>Scalable linear Support Vector Machine for classification using
liblinear.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>NuSVCSklearnNode</strong></li>
<li><strong>NuSVCSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-onehotencodersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.OneHotEncoderSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.OneHotEncoderSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.OneHotEncoderSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-onehotencodersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.OneHotEncoderSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">OneHotEncoderSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.OneHotEncoderSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Encode categorical integer features using a one-hot aka one-of-K scheme.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.preprocessing.data.OneHotEncoder</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>The input to this transformer should be a matrix of integers, denoting
the values taken on by categorical (discrete) features. The output will be
a sparse matrix were each column corresponds to one possible value of one
feature. It is assumed that input features take on values in the range
[0, n_values).</p>
<p>This encoding is needed for feeding categorical data to many scikit-learn
estimators, notably linear models and SVMs with the standard kernels.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_values <span class="classifier-delimiter">:</span> <span class="classifier">&#8216;auto&#8217;, int or array of ints</span></dt>
<dd><p class="first">Number of values per feature.</p>
<ul class="last simple">
<li>&#8216;auto&#8217; : determine value range from training data.</li>
<li>int : maximum value for all features.</li>
<li>array : maximum value per feature.</li>
</ul>
</dd>
<dt>categorical_features: &#8220;all&#8221; or array of indices or mask</dt>
<dd><p class="first">Specify what features are treated as categorical.</p>
<ul class="simple">
<li>&#8216;all&#8217; (default): All features are treated as categorical.</li>
<li>array of indices: Array of categorical feature indices.</li>
<li>mask: Array of length n_features and with dtype=bool.</li>
</ul>
<p class="last">Non-categorical features are always stacked to the right of the matrix.</p>
</dd>
<dt>dtype <span class="classifier-delimiter">:</span> <span class="classifier">number type, default=np.float</span></dt>
<dd>Desired dtype of output.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>active_features_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array</span></dt>
<dd>Indices for active features, meaning values that actually occur
in the training set. Only available when n_values is <tt class="docutils literal"><span class="pre">'auto'</span></tt>.</dd>
<dt><cite>feature_indices_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape (n_features,)</span></dt>
<dd>Indices to feature ranges.
Feature <tt class="docutils literal"><span class="pre">i</span></tt> in the original data is mapped to features
from <tt class="docutils literal"><span class="pre">feature_indices_[i]</span></tt> to <tt class="docutils literal"><span class="pre">feature_indices_[i+1]</span></tt>
(and then potentially masked by <cite>active_features_</cite> afterwards)</dd>
<dt><cite>n_values_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape (n_features,)</span></dt>
<dd>Maximum number of values per feature.</dd>
</dl>
<p><strong>Examples</strong></p>
<p>Given a dataset with three features and two samples, we let the encoder
find the maximum value per feature and transform the data to a binary
one-hot encoding.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">enc</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">enc</span><span class="o">.</span><span class="n">fit</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>  
<span class="go">OneHotEncoder(categorical_features=&#39;all&#39;, dtype=&lt;... &#39;float&#39;&gt;,</span>
<span class="go">       n_values=&#39;auto&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">enc</span><span class="o">.</span><span class="n">n_values_</span>
<span class="go">array([2, 3, 4])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">enc</span><span class="o">.</span><span class="n">feature_indices_</span>
<span class="go">array([0, 2, 5, 9])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">enc</span><span class="o">.</span><span class="n">transform</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="go">array([[ 1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.]])</span>
</pre></div>
</div>
<p>See also</p>
<dl class="docutils">
<dt>sklearn.feature_extraction.DictVectorizer <span class="classifier-delimiter">:</span> <span class="classifier">performs a one-hot encoding of</span></dt>
<dd>dictionary items (also handles string-valued features).</dd>
<dt>sklearn.feature_extraction.FeatureHasher <span class="classifier-delimiter">:</span> <span class="classifier">performs an approximate one-hot</span></dt>
<dd>encoding of dictionary items or strings.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>OneHotEncoderSklearnNode</strong></li>
<li><strong>OneHotEncoderSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-pcasklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.PCASklearnNode" title="pySPACE.missions.nodes.scikits_nodes.PCASklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.PCASklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-pcasklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.PCASklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">PCASklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.PCASklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Principal component analysis (PCA)</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.decomposition.pca.PCA</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Linear dimensionality reduction using Singular Value Decomposition of the
data and keeping only the most significant singular vectors to project the
data to a lower dimensional space.</p>
<p>This implementation uses the scipy.linalg implementation of the singular
value decomposition. It only works for dense arrays and is not scalable to
large dimensional data.</p>
<p>The time complexity of this implementation is <tt class="docutils literal"><span class="pre">O(n</span> <span class="pre">**</span> <span class="pre">3)</span></tt> assuming
n ~ n_samples ~ n_features.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int, None or string</span></dt>
<dd><p class="first">Number of components to keep.
if n_components is not set all components are kept:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">n_components</span> <span class="o">==</span> <span class="nb">min</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>
</pre></div>
</div>
<p class="last">if n_components == &#8216;mle&#8217;, Minka&#8217;s MLE is used to guess the dimension
if <tt class="docutils literal"><span class="pre">0</span> <span class="pre">&lt;</span> <span class="pre">n_components</span> <span class="pre">&lt;</span> <span class="pre">1</span></tt>, select the number of components such that
the amount of variance that needs to be explained is greater than the
percentage specified by n_components</p>
</dd>
<dt>copy <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd>If False, data passed to fit are overwritten and running
fit(X).transform(X) will not yield the expected results,
use fit_transform(X) instead.</dd>
<dt>whiten <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span></dt>
<dd><p class="first">When True (False by default) the <cite>components_</cite> vectors are divided
by n_samples times singular values to ensure uncorrelated outputs
with unit component-wise variances.</p>
<p class="last">Whitening will remove some information from the transformed signal
(the relative variance scales of the components) but can sometime
improve the predictive accuracy of the downstream estimators by
making there data respect some hard-wired assumptions.</p>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>components_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span></dt>
<dd>Components with maximum variance.</dd>
<dt><cite>explained_variance_ratio_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components]</span></dt>
<dd>Percentage of variance explained by each of the selected components.         k is not set then all components are stored and the sum of explained         variances is equal to 1.0</dd>
</dl>
<p><strong>Notes</strong></p>
<p>For n_components=&#8217;mle&#8217;, this class uses the method of <a href="#id24"><span class="problematic" id="id25">`</span></a>Thomas P. Minka:</p>
<p>Automatic Choice of Dimensionality for PCA. NIPS 2000: 598-604`</p>
<p>Due to implementation subtleties of the Singular Value Decomposition (SVD),
which is used in this implementation, running fit twice on the same matrix
can lead to principal components with signs flipped (change in direction).
For this reason, it is important to always use the same estimator object to
transform data in a consistent fashion.</p>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">PCA(copy=True, n_components=2, whiten=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">)</span> 
<span class="go">[ 0.99244...  0.00755...]</span>
</pre></div>
</div>
<p>See also</p>
<p>ProbabilisticPCA
RandomizedPCA
KernelPCA
SparsePCA
TruncatedSVD</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>PCASklearn</strong></li>
<li><strong>PCASklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-passiveaggressiveclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.PassiveAggressiveClassifierSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.PassiveAggressiveClassifierSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.PassiveAggressiveClassifierSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-passiveaggressiveclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.PassiveAggressiveClassifierSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">PassiveAggressiveClassifierSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.PassiveAggressiveClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Passive Aggressive Classifier</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>C <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Maximum step size (regularization). Defaults to 1.0.</dd>
<dt>fit_intercept: bool</dt>
<dd>Whether the intercept should be estimated or not. If False, the
data is assumed to be already centered. Defaults to True.</dd>
<dt>n_iter: int, optional</dt>
<dd>The number of passes over the training data (aka epochs).
Defaults to 5.</dd>
<dt>shuffle: bool, optional</dt>
<dd>Whether or not the training data should be shuffled after each epoch.
Defaults to False.</dd>
<dt>random_state: int seed, RandomState instance, or None (default)</dt>
<dd>The seed of the pseudo random number generator to use when
shuffling the data.</dd>
<dt>verbose: integer, optional</dt>
<dd>The verbosity level</dd>
<dt>n_jobs: integer, optional</dt>
<dd>The number of CPUs to use to do the OVA (One Versus All, for
multi-class problems) computation. -1 means &#8216;all CPUs&#8217;. Defaults
to 1.</dd>
<dt>loss <span class="classifier-delimiter">:</span> <span class="classifier">string, optional</span></dt>
<dd><p class="first">The loss function to be used:</p>
<ul class="last simple">
<li>hinge: equivalent to PA-I in the reference paper.</li>
<li>squared_hinge: equivalent to PA-II in the reference paper.</li>
</ul>
</dd>
<dt>warm_start <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span></dt>
<dd>When set to True, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.</dd>
</dl>
<p><strong>Attributes</strong></p>
<p><cite>coef_</cite> : array, shape = [1, n_features] if n_classes == 2 else [n_classes,
n_features]</p>
<blockquote>
<div>Weights assigned to the features.</div></blockquote>
<dl class="docutils">
<dt><cite>intercept_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [1] if n_classes == 2 else [n_classes]</span></dt>
<dd>Constants in decision function.</dd>
</dl>
<p>See also</p>
<p>SGDClassifier
Perceptron</p>
<p><strong>References</strong></p>
<p>Online Passive-Aggressive Algorithms
&lt;<a class="reference external" href="http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf">http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf</a>&gt;
K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006)</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>PassiveAggressiveClassifierSklearn</strong></li>
<li><strong>PassiveAggressiveClassifierSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-patchextractorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.PatchExtractorSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.PatchExtractorSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.PatchExtractorSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-patchextractorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.PatchExtractorSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">PatchExtractorSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.PatchExtractorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Extracts patches from a collection of images</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.feature_extraction.image.PatchExtractor</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>patch_size: tuple of ints (patch_height, patch_width)</dt>
<dd>the dimensions of one patch</dd>
<dt>max_patches: integer or float, optional default is None</dt>
<dd>The maximum number of patches per image to extract. If max_patches is a
float in (0, 1), it is taken to mean a proportion of the total number
of patches.</dd>
<dt>random_state: int or RandomState</dt>
<dd>Pseudo number generator state used for random sampling.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>PatchExtractorSklearn</strong></li>
<li><strong>PatchExtractorSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-perceptronsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.PerceptronSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.PerceptronSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.PerceptronSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-perceptronsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.PerceptronSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">PerceptronSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.PerceptronSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Perceptron</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.linear_model.perceptron.Perceptron</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>penalty <span class="classifier-delimiter">:</span> <span class="classifier">None, &#8216;l2&#8217; or &#8216;l1&#8217; or &#8216;elasticnet&#8217;</span></dt>
<dd>The penalty (aka regularization term) to be used. Defaults to None.</dd>
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Constant that multiplies the regularization term if regularization is
used. Defaults to 0.0001</dd>
<dt>fit_intercept: bool</dt>
<dd>Whether the intercept should be estimated or not. If False, the
data is assumed to be already centered. Defaults to True.</dd>
<dt>n_iter: int, optional</dt>
<dd>The number of passes over the training data (aka epochs).
Defaults to 5.</dd>
<dt>shuffle: bool, optional</dt>
<dd>Whether or not the training data should be shuffled after each epoch.
Defaults to False.</dd>
<dt>random_state: int seed, RandomState instance, or None (default)</dt>
<dd>The seed of the pseudo random number generator to use when
shuffling the data.</dd>
<dt>verbose: integer, optional</dt>
<dd>The verbosity level</dd>
<dt>n_jobs: integer, optional</dt>
<dd>The number of CPUs to use to do the OVA (One Versus All, for
multi-class problems) computation. -1 means &#8216;all CPUs&#8217;. Defaults
to 1.</dd>
<dt>eta0 <span class="classifier-delimiter">:</span> <span class="classifier">double</span></dt>
<dd>Constant by which the updates are multiplied. Defaults to 1.</dd>
<dt>class_weight <span class="classifier-delimiter">:</span> <span class="classifier">dict, {class_label</span> <span class="classifier-delimiter">:</span> <span class="classifier">weight} or &#8220;auto&#8221; or None, optional</span></dt>
<dd><p class="first">Preset for the class_weight fit parameter.</p>
<p>Weights associated with classes. If not given, all classes
are supposed to have weight one.</p>
<p class="last">The &#8220;auto&#8221; mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies.</p>
</dd>
<dt>warm_start <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span></dt>
<dd>When set to True, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.</dd>
</dl>
<p><strong>Attributes</strong></p>
<p><cite>coef_</cite> : array, shape = [1, n_features] if n_classes == 2 else [n_classes,
n_features]</p>
<blockquote>
<div>Weights assigned to the features.</div></blockquote>
<dl class="docutils">
<dt><cite>intercept_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [1] if n_classes == 2 else [n_classes]</span></dt>
<dd>Constants in decision function.</dd>
</dl>
<p><strong>Notes</strong></p>
<p><cite>Perceptron</cite> and <cite>SGDClassifier</cite> share the same underlying implementation.
In fact, <cite>Perceptron()</cite> is equivalent to <cite>SGDClassifier(loss=&#8221;perceptron&#8221;,
eta0=1, learning_rate=&#8221;constant&#8221;, penalty=None)</cite>.</p>
<p>See also</p>
<p>SGDClassifier</p>
<p><strong>References</strong></p>
<p><a class="reference external" href="http://en.wikipedia.org/wiki/Perceptron">http://en.wikipedia.org/wiki/Perceptron</a> and references therein.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>PerceptronSklearn</strong></li>
<li><strong>PerceptronSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-pipelinesklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.PipelineSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.PipelineSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.PipelineSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-pipelinesklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.PipelineSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">PipelineSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.PipelineSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Pipeline of transforms with a final estimator.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.pipeline.Pipeline</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Sequentially apply a list of transforms and a final estimator.
Intermediate steps of the pipeline must be &#8216;transforms&#8217;, that is, they
must implements fit and transform methods.
The final estimator needs only implements fit.</p>
<p>The purpose of the pipeline is to assemble several steps that can be
cross-validated together while setting different parameters.
For this, it enables setting parameters of the various steps using their
names and the parameter name separated by a &#8216;__&#8217;, as in the example below.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>steps: list</dt>
<dd>List of (name, transform) tuples (implementing fit/transform) that are
chained, in the order in which they are chained, with the last object
an estimator.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">samples_generator</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SelectKBest</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">f_regression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="c"># generate some data to play with</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">samples_generator</span><span class="o">.</span><span class="n">make_classification</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">n_informative</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_redundant</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="c"># ANOVA SVM-C</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">anova_filter</span> <span class="o">=</span> <span class="n">SelectKBest</span><span class="p">(</span><span class="n">f_regression</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">&#39;linear&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">anova_svm</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([(</span><span class="s">&#39;anova&#39;</span><span class="p">,</span> <span class="n">anova_filter</span><span class="p">),</span> <span class="p">(</span><span class="s">&#39;svc&#39;</span><span class="p">,</span> <span class="n">clf</span><span class="p">)])</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="c"># You can set the parameters using the names issued</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c"># For instance, fit using a k of 10 in the SelectKBest</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c"># and a parameter &#39;C&#39; of the svn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">anova_svm</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">anova__k</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">svc__C</span><span class="o">=.</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">... </span>                                             
<span class="go">Pipeline(steps=[...])</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">prediction</span> <span class="o">=</span> <span class="n">anova_svm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">anova_svm</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">0.75</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>PipelineSklearn</strong></li>
<li><strong>PipelineSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-probabilisticpcasklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.ProbabilisticPCASklearnNode" title="pySPACE.missions.nodes.scikits_nodes.ProbabilisticPCASklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.ProbabilisticPCASklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-probabilisticpcasklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.ProbabilisticPCASklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">ProbabilisticPCASklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.ProbabilisticPCASklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Additional layer on top of PCA that adds a probabilistic evaluationPrincipal component analysis (PCA)</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.decomposition.pca.ProbabilisticPCA</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Linear dimensionality reduction using Singular Value Decomposition of the
data and keeping only the most significant singular vectors to project the
data to a lower dimensional space.</p>
<p>This implementation uses the scipy.linalg implementation of the singular
value decomposition. It only works for dense arrays and is not scalable to
large dimensional data.</p>
<p>The time complexity of this implementation is <tt class="docutils literal"><span class="pre">O(n</span> <span class="pre">**</span> <span class="pre">3)</span></tt> assuming
n ~ n_samples ~ n_features.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int, None or string</span></dt>
<dd><p class="first">Number of components to keep.
if n_components is not set all components are kept:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">n_components</span> <span class="o">==</span> <span class="nb">min</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>
</pre></div>
</div>
<p class="last">if n_components == &#8216;mle&#8217;, Minka&#8217;s MLE is used to guess the dimension
if <tt class="docutils literal"><span class="pre">0</span> <span class="pre">&lt;</span> <span class="pre">n_components</span> <span class="pre">&lt;</span> <span class="pre">1</span></tt>, select the number of components such that
the amount of variance that needs to be explained is greater than the
percentage specified by n_components</p>
</dd>
<dt>copy <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd>If False, data passed to fit are overwritten and running
fit(X).transform(X) will not yield the expected results,
use fit_transform(X) instead.</dd>
<dt>whiten <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span></dt>
<dd><p class="first">When True (False by default) the <cite>components_</cite> vectors are divided
by n_samples times singular values to ensure uncorrelated outputs
with unit component-wise variances.</p>
<p class="last">Whitening will remove some information from the transformed signal
(the relative variance scales of the components) but can sometime
improve the predictive accuracy of the downstream estimators by
making there data respect some hard-wired assumptions.</p>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>components_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span></dt>
<dd>Components with maximum variance.</dd>
<dt><cite>explained_variance_ratio_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components]</span></dt>
<dd>Percentage of variance explained by each of the selected components.         k is not set then all components are stored and the sum of explained         variances is equal to 1.0</dd>
</dl>
<p><strong>Notes</strong></p>
<p>For n_components=&#8217;mle&#8217;, this class uses the method of <a href="#id26"><span class="problematic" id="id27">`</span></a>Thomas P. Minka:</p>
<p>Automatic Choice of Dimensionality for PCA. NIPS 2000: 598-604`</p>
<p>Due to implementation subtleties of the Singular Value Decomposition (SVD),
which is used in this implementation, running fit twice on the same matrix
can lead to principal components with signs flipped (change in direction).
For this reason, it is important to always use the same estimator object to
transform data in a consistent fashion.</p>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">PCA(copy=True, n_components=2, whiten=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">)</span> 
<span class="go">[ 0.99244...  0.00755...]</span>
</pre></div>
</div>
<p>See also</p>
<p>ProbabilisticPCA
RandomizedPCA
KernelPCA
SparsePCA
TruncatedSVD</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>ProbabilisticPCASklearnNode</strong></li>
<li><strong>ProbabilisticPCASklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-projectedgradientnmfsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.ProjectedGradientNMFSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.ProjectedGradientNMFSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.ProjectedGradientNMFSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-projectedgradientnmfsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.ProjectedGradientNMFSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">ProjectedGradientNMFSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.ProjectedGradientNMFSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Non-Negative matrix factorization by Projected Gradient (NMF)</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.decomposition.nmf.ProjectedGradientNMF</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int or None</span></dt>
<dd>Number of components, if n_components is not set all components
are kept</dd>
<dt>init <span class="classifier-delimiter">:</span> <span class="classifier">&#8216;nndsvd&#8217; |  &#8216;nndsvda&#8217; | &#8216;nndsvdar&#8217; | &#8216;random&#8217;</span></dt>
<dd><p class="first">Method used to initialize the procedure.
Default: &#8216;nndsvdar&#8217; if n_components &lt; n_features, otherwise random.
Valid options:</p>
<div class="last highlight-python"><div class="highlight"><pre>&#39;nndsvd&#39;: Nonnegative Double Singular Value Decomposition (NNDSVD)
    initialization (better for sparseness)
&#39;nndsvda&#39;: NNDSVD with zeros filled with the average of X
    (better when sparsity is not desired)
&#39;nndsvdar&#39;: NNDSVD with zeros filled with small random values
    (generally faster, less accurate alternative to NNDSVDa
    for when sparsity is not desired)
&#39;random&#39;: non-negative random matrices
</pre></div>
</div>
</dd>
<dt>sparseness <span class="classifier-delimiter">:</span> <span class="classifier">&#8216;data&#8217; | &#8216;components&#8217; | None, default: None</span></dt>
<dd>Where to enforce sparsity in the model.</dd>
<dt>beta <span class="classifier-delimiter">:</span> <span class="classifier">double, default: 1</span></dt>
<dd>Degree of sparseness, if sparseness is not None. Larger values mean
more sparseness.</dd>
<dt>eta <span class="classifier-delimiter">:</span> <span class="classifier">double, default: 0.1</span></dt>
<dd>Degree of correctness to maintain, if sparsity is not None. Smaller
values mean larger error.</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">double, default: 1e-4</span></dt>
<dd>Tolerance value used in stopping conditions.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, default: 200</span></dt>
<dd>Number of iterations to compute.</dd>
<dt>nls_max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, default: 2000</span></dt>
<dd>Number of iterations in NLS subproblem.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int or RandomState</span></dt>
<dd>Random number generator seed control.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>components_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span></dt>
<dd>Non-negative components of the data.</dd>
<dt><cite>reconstruction_err_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">number</span></dt>
<dd>Frobenius norm of the matrix difference between
the training data and the reconstructed data from
the fit produced by the model. <tt class="docutils literal"><span class="pre">||</span> <span class="pre">X</span> <span class="pre">-</span> <span class="pre">WH</span> <span class="pre">||_2</span></tt></dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">ProjectedGradientNMF</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ProjectedGradientNMF</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s">&#39;random&#39;</span><span class="p">,</span>
<span class="gp">... </span>                             <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> 
<span class="go">ProjectedGradientNMF(beta=1, eta=0.1, init=&#39;random&#39;, max_iter=200,</span>
<span class="go">        n_components=2, nls_max_iter=2000, random_state=0, sparseness=None,</span>
<span class="go">        tol=0.0001)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">components_</span>
<span class="go">array([[ 0.77032744,  0.11118662],</span>
<span class="go">       [ 0.38526873,  0.38228063]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">reconstruction_err_</span> 
<span class="go">0.00746...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ProjectedGradientNMF</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="gp">... </span>             <span class="n">sparseness</span><span class="o">=</span><span class="s">&#39;components&#39;</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s">&#39;random&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> 
<span class="go">ProjectedGradientNMF(beta=1, eta=0.1, init=&#39;random&#39;, max_iter=200,</span>
<span class="go">            n_components=2, nls_max_iter=2000, random_state=0,</span>
<span class="go">            sparseness=&#39;components&#39;, tol=0.0001)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">components_</span>
<span class="go">array([[ 1.67481991,  0.29614922],</span>
<span class="go">       [ 0.        ,  0.4681982 ]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">reconstruction_err_</span> 
<span class="go">0.513...</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<p>This implements</p>
<p>C.-J. Lin. Projected gradient methods
for non-negative matrix factorization. Neural
Computation, 19(2007), 2756-2779.
<a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/nmf/">http://www.csie.ntu.edu.tw/~cjlin/nmf/</a></p>
<p>P. Hoyer. Non-negative Matrix Factorization with
Sparseness Constraints. Journal of Machine Learning
Research 2004.</p>
<p>NNDSVD is introduced in</p>
<p>C. Boutsidis, E. Gallopoulos: SVD based
initialization: A head start for nonnegative
matrix factorization - Pattern Recognition, 2008
<a class="reference external" href="http://tinyurl.com/nndsvd">http://tinyurl.com/nndsvd</a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>ProjectedGradientNMFSklearnNode</strong></li>
<li><strong>ProjectedGradientNMFSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-qdasklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.QDASklearnNode" title="pySPACE.missions.nodes.scikits_nodes.QDASklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.QDASklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-qdasklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.QDASklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">QDASklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.QDASklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Quadratic Discriminant Analysis (QDA)</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.qda.QDA</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>A classifier with a quadratic decision boundary, generated
by fitting class conditional densities to the data
and using Bayes&#8217; rule.</p>
<p>The model fits a Gaussian density to each class.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>priors <span class="classifier-delimiter">:</span> <span class="classifier">array, optional, shape = [n_classes]</span></dt>
<dd>Priors on classes</dd>
<dt>reg_param <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Regularizes the covariance estimate as
<tt class="docutils literal"><span class="pre">(1-reg_param)*Sigma</span> <span class="pre">+</span> <span class="pre">reg_param*np.eye(n_features)</span></tt></dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>covariances_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">list of array-like, shape = [n_features, n_features]</span></dt>
<dd>Covariance matrices of each class.</dd>
<dt><cite>means_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_classes, n_features]</span></dt>
<dd>Class means.</dd>
<dt><cite>priors_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_classes]</span></dt>
<dd>Class priors (sum to 1).</dd>
<dt><cite>rotations_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">list of arrays</span></dt>
<dd>For each class an array of shape [n_samples, n_samples], the
rotation of the Gaussian distribution, i.e. its principal axis.</dd>
<dt><cite>scalings_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_classes, n_features]</span></dt>
<dd>Contains the scaling of the Gaussian
distributions along the principal axes for each
class, i.e. the variance in the rotated coordinate system.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.qda</span> <span class="kn">import</span> <span class="n">QDA</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">QDA</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">QDA(priors=None, reg_param=0.0)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
<p>See also</p>
<p>sklearn.lda.LDA: Linear discriminant analysis</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>QDASklearn</strong></li>
<li><strong>QDASklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-rfecvsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.RFECVSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.RFECVSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RFECVSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-rfecvsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.RFECVSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">RFECVSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.RFECVSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Feature ranking with recursive feature elimination and cross-validated
selection of the best number of features.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.feature_selection.rfe.RFECV</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>estimator <span class="classifier-delimiter">:</span> <span class="classifier">object</span></dt>
<dd><p class="first">A supervised learning estimator with a <cite>fit</cite> method that updates a
<cite>coef_</cite> attribute that holds the fitted parameters. Important features
must correspond to high absolute values in the <cite>coef_</cite> array.</p>
<p class="last">For instance, this is the case for most supervised learning
algorithms such as Support Vector Classifiers and Generalized
Linear Models from the <cite>svm</cite> and <cite>linear_model</cite> modules.</p>
</dd>
<dt>step <span class="classifier-delimiter">:</span> <span class="classifier">int or float, optional (default=1)</span></dt>
<dd>If greater than or equal to 1, then <cite>step</cite> corresponds to the (integer)
number of features to remove at each iteration.
If within (0.0, 1.0), then <cite>step</cite> corresponds to the percentage
(rounded down) of features to remove at each iteration.</dd>
<dt>cv <span class="classifier-delimiter">:</span> <span class="classifier">int or cross-validation generator, optional (default=None)</span></dt>
<dd>If int, it is the number of folds.
If None, 3-fold cross-validation is performed by default.
Specific cross-validation objects can also be passed, see
<cite>sklearn.cross_validation module</cite> for details.</dd>
<dt>scoring <span class="classifier-delimiter">:</span> <span class="classifier">string, callable or None, optional, default: None</span></dt>
<dd>A string (see model evaluation documentation) or
a scorer callable object / function with signature
<tt class="docutils literal"><span class="pre">scorer(estimator,</span> <span class="pre">X,</span> <span class="pre">y)</span></tt>.</dd>
<dt>estimator_params <span class="classifier-delimiter">:</span> <span class="classifier">dict</span></dt>
<dd>Parameters for the external estimator.
Useful for doing grid searches.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">int, default=0</span></dt>
<dd>Controls verbosity of output.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>n_features_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>The number of selected features with cross-validation.</dd>
<dt><cite>support_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape [n_features]</span></dt>
<dd>The mask of selected features.</dd>
<dt><cite>ranking_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape [n_features]</span></dt>
<dd>The feature ranking, such that <cite>ranking_[i]</cite>
corresponds to the ranking
position of the i-th feature.
Selected (i.e., estimated best)
features are assigned rank 1.</dd>
<dt><cite>grid_scores_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape [n_subsets_of_features]</span></dt>
<dd>The cross-validation scores such that
<cite>grid_scores_[i]</cite> corresponds to
the CV score of the i-th subset of features.</dd>
<dt><cite>estimator_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">object</span></dt>
<dd>The external estimator fit on the reduced dataset.</dd>
</dl>
<p><strong>Examples</strong></p>
<p>The following example shows how to retrieve the a-priori not known 5
informative features in the Friedman #1 dataset.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_friedman1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">RFECV</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVR</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_friedman1</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">estimator</span> <span class="o">=</span> <span class="n">SVR</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">&quot;linear&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selector</span> <span class="o">=</span> <span class="n">RFECV</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selector</span> <span class="o">=</span> <span class="n">selector</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selector</span><span class="o">.</span><span class="n">support_</span> 
<span class="go">array([ True,  True,  True,  True,  True,</span>
<span class="go">        False, False, False, False, False], dtype=bool)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selector</span><span class="o">.</span><span class="n">ranking_</span>
<span class="go">array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id28" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>Guyon, I., Weston, J., Barnhill, S., &amp; Vapnik, V., &#8220;Gene selection
for cancer classification using support vector machines&#8221;,
Mach. Learn., 46(1-3), 389&#8211;422, 2002.</td></tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>RFECVSklearn</strong></li>
<li><strong>RFECVSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-rfesklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.RFESklearnNode" title="pySPACE.missions.nodes.scikits_nodes.RFESklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RFESklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-rfesklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.RFESklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">RFESklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.RFESklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Feature ranking with recursive feature elimination.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.feature_selection.rfe.RFE</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Given an external estimator that assigns weights to features (e.g., the
coefficients of a linear model), the goal of recursive feature elimination
(RFE) is to select features by recursively considering smaller and smaller
sets of features. First, the estimator is trained on the initial set of
features and weights are assigned to each one of them. Then, features whose
absolute weights are the smallest are pruned from the current set features.
That procedure is recursively repeated on the pruned set until the desired
number of features to select is eventually reached.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>estimator <span class="classifier-delimiter">:</span> <span class="classifier">object</span></dt>
<dd><p class="first">A supervised learning estimator with a <cite>fit</cite> method that updates a
<cite>coef_</cite> attribute that holds the fitted parameters. Important features
must correspond to high absolute values in the <cite>coef_</cite> array.</p>
<p class="last">For instance, this is the case for most supervised learning
algorithms such as Support Vector Classifiers and Generalized
Linear Models from the <cite>svm</cite> and <cite>linear_model</cite> modules.</p>
</dd>
<dt>n_features_to_select <span class="classifier-delimiter">:</span> <span class="classifier">int or None (default=None)</span></dt>
<dd>The number of features to select. If <cite>None</cite>, half of the features
are selected.</dd>
<dt>step <span class="classifier-delimiter">:</span> <span class="classifier">int or float, optional (default=1)</span></dt>
<dd>If greater than or equal to 1, then <cite>step</cite> corresponds to the (integer)
number of features to remove at each iteration.
If within (0.0, 1.0), then <cite>step</cite> corresponds to the percentage
(rounded down) of features to remove at each iteration.</dd>
<dt>estimator_params <span class="classifier-delimiter">:</span> <span class="classifier">dict</span></dt>
<dd>Parameters for the external estimator.
Useful for doing grid searches.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>n_features_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>The number of selected features.</dd>
<dt><cite>support_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape [n_features]</span></dt>
<dd>The mask of selected features.</dd>
<dt><cite>ranking_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape [n_features]</span></dt>
<dd>The feature ranking, such that <cite>ranking_[i]</cite> corresponds to the         ranking position of the i-th feature. Selected (i.e., estimated         best) features are assigned rank 1.</dd>
<dt><cite>estimator_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">object</span></dt>
<dd>The external estimator fit on the reduced dataset.</dd>
</dl>
<p><strong>Examples</strong></p>
<p>The following example shows how to retrieve the 5 right informative
features in the Friedman #1 dataset.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_friedman1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">RFE</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVR</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_friedman1</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">estimator</span> <span class="o">=</span> <span class="n">SVR</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">&quot;linear&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selector</span> <span class="o">=</span> <span class="n">RFE</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selector</span> <span class="o">=</span> <span class="n">selector</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selector</span><span class="o">.</span><span class="n">support_</span> 
<span class="go">array([ True,  True,  True,  True,  True,</span>
<span class="go">        False, False, False, False, False], dtype=bool)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selector</span><span class="o">.</span><span class="n">ranking_</span>
<span class="go">array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id29" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>Guyon, I., Weston, J., Barnhill, S., &amp; Vapnik, V., &#8220;Gene selection
for cancer classification using support vector machines&#8221;,
Mach. Learn., 46(1-3), 389&#8211;422, 2002.</td></tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>RFESklearn</strong></li>
<li><strong>RFESklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-radiusneighborsclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.RadiusNeighborsClassifierSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.RadiusNeighborsClassifierSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RadiusNeighborsClassifierSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-radiusneighborsclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.RadiusNeighborsClassifierSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">RadiusNeighborsClassifierSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.RadiusNeighborsClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Classifier implementing a vote among neighbors within a given radius</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.neighbors.classification.RadiusNeighborsClassifier</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>radius <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default = 1.0)</span></dt>
<dd>Range of parameter space to use by default for :meth`radius_neighbors`
queries.</dd>
<dt>weights <span class="classifier-delimiter">:</span> <span class="classifier">str or callable</span></dt>
<dd><p class="first">weight function used in prediction.  Possible values:</p>
<ul class="simple">
<li>&#8216;uniform&#8217; : uniform weights.  All points in each neighborhood
are weighted equally.</li>
<li>&#8216;distance&#8217; : weight points by the inverse of their distance.
in this case, closer neighbors of a query point will have a
greater influence than neighbors which are further away.</li>
<li>[callable] : a user-defined function which accepts an
array of distances, and returns an array of the same shape
containing the weights.</li>
</ul>
<p class="last">Uniform weights are used by default.</p>
</dd>
<dt>algorithm <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;auto&#8217;, &#8216;ball_tree&#8217;, &#8216;kd_tree&#8217;, &#8216;brute&#8217;}, optional</span></dt>
<dd><p class="first">Algorithm used to compute the nearest neighbors:</p>
<ul class="simple">
<li>&#8216;ball_tree&#8217; will use <tt class="xref py py-class docutils literal"><span class="pre">BallTree</span></tt></li>
<li>&#8216;kd_tree&#8217; will use <tt class="xref py py-class docutils literal"><span class="pre">KDtree</span></tt></li>
<li>&#8216;brute&#8217; will use a brute-force search.</li>
<li>&#8216;auto&#8217; will attempt to decide the most appropriate algorithm
based on the values passed to <tt class="xref py py-meth docutils literal"><span class="pre">fit()</span></tt> method.</li>
</ul>
<p class="last">Note: fitting on sparse input will override the setting of
this parameter, using brute force.</p>
</dd>
<dt>leaf_size <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default = 30)</span></dt>
<dd>Leaf size passed to BallTree or KDTree.  This can affect the
speed of the construction and query, as well as the memory
required to store the tree.  The optimal value depends on the
nature of the problem.</dd>
<dt>metric <span class="classifier-delimiter">:</span> <span class="classifier">string or DistanceMetric object (default=&#8217;minkowski&#8217;)</span></dt>
<dd>the distance metric to use for the tree.  The default metric is
minkowski, and with p=2 is equivalent to the standard Euclidean
metric. See the documentation of the DistanceMetric class for a
list of available metrics.</dd>
<dt>p <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default = 2)</span></dt>
<dd>Power parameter for the Minkowski metric. When p = 1, this is
equivalent to using manhattan_distance (l1), and euclidean_distance
(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.</dd>
<dt>outlier_label <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default = None)</span></dt>
<dd>Label, which is given for outlier samples (samples with no
neighbors on given radius).
If set to None, ValueError is raised, when outlier is detected.</dd>
</dl>
<p><a href="#id30"><span class="problematic" id="id31">**</span></a>kwargs :</p>
<blockquote>
<div><ul class="simple">
<li>additional keyword arguments are passed to the distance function as</li>
<li>additional arguments.</li>
</ul>
</div></blockquote>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">RadiusNeighborsClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neigh</span> <span class="o">=</span> <span class="n">RadiusNeighborsClassifier</span><span class="p">(</span><span class="n">radius</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neigh</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">RadiusNeighborsClassifier(...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">neigh</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mf">1.5</span><span class="p">]]))</span>
<span class="go">[0]</span>
</pre></div>
</div>
<p>See also</p>
<p>KNeighborsClassifier
RadiusNeighborsRegressor
KNeighborsRegressor
NearestNeighbors</p>
<p><strong>Notes</strong></p>
<p>See <em class="xref std std-ref">Nearest Neighbors</em> in the online documentation
for a discussion of the choice of <tt class="docutils literal"><span class="pre">algorithm</span></tt> and <tt class="docutils literal"><span class="pre">leaf_size</span></tt>.</p>
<p><a class="reference external" href="http://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm">http://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm</a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>RadiusNeighborsClassifierSklearnNode</strong></li>
<li><strong>RadiusNeighborsClassifierSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-randomforestclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.RandomForestClassifierSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.RandomForestClassifierSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RandomForestClassifierSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-randomforestclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.RandomForestClassifierSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">RandomForestClassifierSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.RandomForestClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>A random forest classifier.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.ensemble.forest.RandomForestClassifier</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>A random forest is a meta estimator that fits a number of decision tree
classifiers on various sub-samples of the dataset and use averaging to
improve the predictive accuracy and control over-fitting.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_estimators <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=10)</span></dt>
<dd>The number of trees in the forest.</dd>
<dt>criterion <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=&#8221;gini&#8221;)</span></dt>
<dd>The function to measure the quality of a split. Supported criteria are
&#8220;gini&#8221; for the Gini impurity and &#8220;entropy&#8221; for the information gain.
Note: this parameter is tree-specific.</dd>
<dt>max_features <span class="classifier-delimiter">:</span> <span class="classifier">int, float, string or None, optional (default=&#8221;auto&#8221;)</span></dt>
<dd><p class="first">The number of features to consider when looking for the best split:</p>
<blockquote>
<div><ul class="simple">
<li><ul class="first">
<li>If int, then consider <cite>max_features</cite> features at each split.</li>
</ul>
</li>
<li><ul class="first">
<li>If float, then <cite>max_features</cite> is a percentage and</li>
</ul>
</li>
<li><cite>int(max_features * n_features)</cite> features are considered at each</li>
<li>split.</li>
<li><ul class="first">
<li>If &#8220;auto&#8221;, then <cite>max_features=sqrt(n_features)</cite>.</li>
</ul>
</li>
<li><ul class="first">
<li>If &#8220;sqrt&#8221;, then <cite>max_features=sqrt(n_features)</cite>.</li>
</ul>
</li>
<li><ul class="first">
<li>If &#8220;log2&#8221;, then <cite>max_features=log2(n_features)</cite>.</li>
</ul>
</li>
<li><ul class="first">
<li>If None, then <cite>max_features=n_features</cite>.</li>
</ul>
</li>
</ul>
</div></blockquote>
<p class="last">Note: this parameter is tree-specific.</p>
</dd>
<dt>max_depth <span class="classifier-delimiter">:</span> <span class="classifier">integer or None, optional (default=None)</span></dt>
<dd>The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.
Note: this parameter is tree-specific.</dd>
<dt>min_samples_split <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=2)</span></dt>
<dd>The minimum number of samples required to split an internal node.
Note: this parameter is tree-specific.</dd>
<dt>min_samples_leaf <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=1)</span></dt>
<dd>The minimum number of samples in newly created leaves.  A split is
discarded if after the split, one of the leaves would contain less then
<tt class="docutils literal"><span class="pre">min_samples_leaf</span></tt> samples.
Note: this parameter is tree-specific.</dd>
<dt>bootstrap <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=True)</span></dt>
<dd>Whether bootstrap samples are used when building trees.</dd>
<dt>oob_score <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd>Whether to use out-of-bag samples to estimate
the generalization error.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=1)</span></dt>
<dd>The number of jobs to run in parallel for both <cite>fit</cite> and <cite>predict</cite>.
If -1, then the number of jobs is set to the number of cores.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=0)</span></dt>
<dd>Controls the verbosity of the tree building process.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>estimators_</cite>: list of DecisionTreeClassifier</dt>
<dd>The collection of fitted sub-estimators.</dd>
<dt><cite>classes_</cite>: array of shape = [n_classes] or a list of such arrays</dt>
<dd>The classes labels (single output problem), or a list of arrays of
class labels (multi-output problem).</dd>
<dt><cite>n_classes_</cite>: int or list</dt>
<dd>The number of classes (single output problem), or a list containing the
number of classes for each output (multi-output problem).</dd>
<dt><cite>feature_importances_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_features]</span></dt>
<dd>The feature importances (the higher, the more important the feature).</dd>
<dt><cite>oob_score_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Score of the training dataset obtained using an out-of-bag estimate.</dd>
<dt><cite>oob_decision_function_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_samples, n_classes]</span></dt>
<dd>Decision function computed with out-of-bag estimate on the training
set. If n_estimators is small it might be possible that a data point
was never left out during the bootstrap. In this case,
<cite>oob_decision_function_</cite> might contain NaN.</dd>
</dl>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id32" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><ol class="first last upperalpha simple" start="12">
<li>Breiman, &#8220;Random Forests&#8221;, Machine Learning, 45(1), 5-32, 2001.</li>
</ol>
</td></tr>
</tbody>
</table>
<p>See also</p>
<p>DecisionTreeClassifier, ExtraTreesClassifier</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>RandomForestClassifierSklearn</strong></li>
<li><strong>RandomForestClassifierSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-randomforestregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.RandomForestRegressorSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.RandomForestRegressorSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RandomForestRegressorSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-randomforestregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.RandomForestRegressorSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">RandomForestRegressorSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.RandomForestRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>A random forest regressor.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.ensemble.forest.RandomForestRegressor</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>A random forest is a meta estimator that fits a number of classifying
decision trees on various sub-samples of the dataset and use averaging
to improve the predictive accuracy and control over-fitting.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_estimators <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=10)</span></dt>
<dd>The number of trees in the forest.</dd>
<dt>criterion <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=&#8221;mse&#8221;)</span></dt>
<dd>The function to measure the quality of a split. The only supported
criterion is &#8220;mse&#8221; for the mean squared error.
Note: this parameter is tree-specific.</dd>
<dt>max_features <span class="classifier-delimiter">:</span> <span class="classifier">int, float, string or None, optional (default=&#8221;auto&#8221;)</span></dt>
<dd><p class="first">The number of features to consider when looking for the best split:</p>
<blockquote>
<div><ul class="simple">
<li><ul class="first">
<li>If int, then consider <cite>max_features</cite> features at each split.</li>
</ul>
</li>
<li><ul class="first">
<li>If float, then <cite>max_features</cite> is a percentage and</li>
</ul>
</li>
<li><cite>int(max_features * n_features)</cite> features are considered at each</li>
<li>split.</li>
<li><ul class="first">
<li>If &#8220;auto&#8221;, then <cite>max_features=n_features</cite>.</li>
</ul>
</li>
<li><ul class="first">
<li>If &#8220;sqrt&#8221;, then <cite>max_features=sqrt(n_features)</cite>.</li>
</ul>
</li>
<li><ul class="first">
<li>If &#8220;log2&#8221;, then <cite>max_features=log2(n_features)</cite>.</li>
</ul>
</li>
<li><ul class="first">
<li>If None, then <cite>max_features=n_features</cite>.</li>
</ul>
</li>
</ul>
</div></blockquote>
<p class="last">Note: this parameter is tree-specific.</p>
</dd>
<dt>max_depth <span class="classifier-delimiter">:</span> <span class="classifier">integer or None, optional (default=None)</span></dt>
<dd>The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.
Note: this parameter is tree-specific.</dd>
<dt>min_samples_split <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=2)</span></dt>
<dd>The minimum number of samples required to split an internal node.
Note: this parameter is tree-specific.</dd>
<dt>min_samples_leaf <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=1)</span></dt>
<dd>The minimum number of samples in newly created leaves.  A split is
discarded if after the split, one of the leaves would contain less then
<tt class="docutils literal"><span class="pre">min_samples_leaf</span></tt> samples.
Note: this parameter is tree-specific.</dd>
<dt>bootstrap <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=True)</span></dt>
<dd>Whether bootstrap samples are used when building trees.</dd>
<dt>oob_score <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd>whether to use out-of-bag samples to estimate
the generalization error.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=1)</span></dt>
<dd>The number of jobs to run in parallel for both <cite>fit</cite> and <cite>predict</cite>.
If -1, then the number of jobs is set to the number of cores.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=0)</span></dt>
<dd>Controls the verbosity of the tree building process.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>estimators_</cite>: list of DecisionTreeRegressor</dt>
<dd>The collection of fitted sub-estimators.</dd>
<dt><cite>feature_importances_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_features]</span></dt>
<dd>The feature importances (the higher, the more important the feature).</dd>
<dt><cite>oob_score_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Score of the training dataset obtained using an out-of-bag estimate.</dd>
<dt><cite>oob_prediction_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_samples]</span></dt>
<dd>Prediction computed with out-of-bag estimate on the training set.</dd>
</dl>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id33" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><ol class="first last upperalpha simple" start="12">
<li>Breiman, &#8220;Random Forests&#8221;, Machine Learning, 45(1), 5-32, 2001.</li>
</ol>
</td></tr>
</tbody>
</table>
<p>See also</p>
<p>DecisionTreeRegressor, ExtraTreesRegressor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>RandomForestRegressorSklearn</strong></li>
<li><strong>RandomForestRegressorSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-randomtreesembeddingsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.RandomTreesEmbeddingSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.RandomTreesEmbeddingSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RandomTreesEmbeddingSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-randomtreesembeddingsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.RandomTreesEmbeddingSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">RandomTreesEmbeddingSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.RandomTreesEmbeddingSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>An ensemble of totally random trees.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.ensemble.forest.RandomTreesEmbedding</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>An unsupervised transformation of a dataset to a high-dimensional
sparse representation. A datapoint is coded according to which leaf of
each tree it is sorted into. Using a one-hot encoding of the leaves,
this leads to a binary coding with as many ones as trees in the forest.</p>
<p>The dimensionality of the resulting representation is approximately
<tt class="docutils literal"><span class="pre">n_estimators</span> <span class="pre">*</span> <span class="pre">2</span> <span class="pre">**</span> <span class="pre">max_depth</span></tt>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_estimators <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Number of trees in the forest.</dd>
<dt>max_depth <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Maximum depth of each tree.</dd>
<dt>min_samples_split <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=2)</span></dt>
<dd>The minimum number of samples required to split an internal node.
Note: this parameter is tree-specific.</dd>
<dt>min_samples_leaf <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=1)</span></dt>
<dd>The minimum number of samples in newly created leaves.  A split is
discarded if after the split, one of the leaves would contain less then
<tt class="docutils literal"><span class="pre">min_samples_leaf</span></tt> samples.
Note: this parameter is tree-specific.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=1)</span></dt>
<dd>The number of jobs to run in parallel for both <cite>fit</cite> and <cite>predict</cite>.
If -1, then the number of jobs is set to the number of cores.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=0)</span></dt>
<dd>Controls the verbosity of the tree building process.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>estimators_</cite>: list of DecisionTreeClassifier</dt>
<dd>The collection of fitted sub-estimators.</dd>
</dl>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id34" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>P. Geurts, D. Ernst., and L. Wehenkel, &#8220;Extremely randomized trees&#8221;,
Machine Learning, 63(1), 3-42, 2006.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id35" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td>Moosmann, F. and Triggs, B. and Jurie, F.  &#8220;Fast discriminative
visual codebooks using randomized clustering forests&#8221;
NIPS 2007</td></tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>RandomTreesEmbeddingSklearnNode</strong></li>
<li><strong>RandomTreesEmbeddingSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-randomizedlassosklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.RandomizedLassoSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.RandomizedLassoSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RandomizedLassoSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-randomizedlassosklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.RandomizedLassoSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">RandomizedLassoSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.RandomizedLassoSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Randomized Lasso.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.linear_model.randomized_l1.RandomizedLasso</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Randomized Lasso works by resampling the train data and computing
a Lasso on each resampling. In short, the features selected more
often are good features. It is also known as stability selection.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float, &#8216;aic&#8217;, or &#8216;bic&#8217;, optional</span></dt>
<dd>The regularization parameter alpha parameter in the Lasso.
Warning: this is not the alpha parameter in the stability selection
article which is scaling.</dd>
<dt>scaling <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>The alpha parameter in the stability selection article used to
randomly scale the features. Should be between 0 and 1.</dd>
<dt>sample_fraction <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>The fraction of samples to be used in each randomized design.
Should be between 0 and 1. If 1, all samples are used.</dd>
<dt>n_resampling <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>Number of randomized models.</dd>
<dt>selection_threshold: float, optional</dt>
<dd>The score above which features should be selected.</dd>
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span></dt>
<dd>whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">boolean or integer, optional</span></dt>
<dd>Sets the verbosity amount</dd>
<dt>normalize <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span></dt>
<dd>If True, the regressors X will be normalized before regression.</dd>
<dt>precompute <span class="classifier-delimiter">:</span> <span class="classifier">True | False | &#8216;auto&#8217;</span></dt>
<dd>Whether to use a precomputed Gram matrix to speed up
calculations. If set to &#8216;auto&#8217; let us decide. The Gram
matrix can also be passed as argument.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span></dt>
<dd>Maximum number of iterations to perform in the Lars algorithm.</dd>
<dt>eps <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>The machine-precision regularization in the computation of the
Cholesky diagonal factors. Increase this for very ill-conditioned
systems. Unlike the &#8216;tol&#8217; parameter in some iterative
optimization-based algorithms, this parameter does not control
the tolerance of the optimization.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span></dt>
<dd>Number of CPUs to use during the resampling. If &#8216;-1&#8217;, use
all the CPUs</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>pre_dispatch <span class="classifier-delimiter">:</span> <span class="classifier">int, or string, optional</span></dt>
<dd><p class="first">Controls the number of jobs that get dispatched during parallel
execution. Reducing this number can be useful to avoid an
explosion of memory consumption when more jobs get dispatched
than CPUs can process. This parameter can be:</p>
<blockquote class="last">
<div><ul class="simple">
<li>None, in which case all the jobs are immediately
created and spawned. Use this for lightweight and
fast-running jobs, to avoid delays due to on-demand
spawning of the jobs</li>
<li>An int, giving the exact number of total jobs that are
spawned</li>
<li>A string, giving an expression as a function of n_jobs,
as in &#8216;2*n_jobs&#8217;</li>
</ul>
</div></blockquote>
</dd>
<dt>memory <span class="classifier-delimiter">:</span> <span class="classifier">Instance of joblib.Memory or string</span></dt>
<dd>Used for internal caching. By default, no caching is done.
If a string is given, it is thepath to the caching directory.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>scores_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_features]</span></dt>
<dd>Feature scores between 0 and 1.</dd>
<dt><cite>all_scores_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_features, n_reg_parameter]</span></dt>
<dd>Feature scores between 0 and 1 for all values of the regularization         parameter. The reference article suggests <tt class="docutils literal"><span class="pre">scores_</span></tt> is the max of         <tt class="docutils literal"><span class="pre">all_scores_</span></tt>.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">RandomizedLasso</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">randomized_lasso</span> <span class="o">=</span> <span class="n">RandomizedLasso</span><span class="p">()</span>
</pre></div>
</div>
<p><strong>Notes</strong></p>
<p>See examples/linear_model/plot_sparse_recovery.py for an example.</p>
<p><strong>References</strong></p>
<p>Stability selection
Nicolai Meinshausen, Peter Buhlmann
Journal of the Royal Statistical Society: Series B
Volume 72, Issue 4, pages 417-473, September 2010
DOI: 10.1111/j.1467-9868.2010.00740.x</p>
<p>See also</p>
<p>RandomizedLogisticRegression, LogisticRegression</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>RandomizedLassoSklearn</strong></li>
<li><strong>RandomizedLassoSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-randomizedlogisticregressionsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.RandomizedLogisticRegressionSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.RandomizedLogisticRegressionSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RandomizedLogisticRegressionSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-randomizedlogisticregressionsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.RandomizedLogisticRegressionSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">RandomizedLogisticRegressionSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.RandomizedLogisticRegressionSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Randomized Logistic Regression</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.linear_model.randomized_l1.RandomizedLogisticRegression</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Randomized Regression works by resampling the train data and computing
a LogisticRegression on each resampling. In short, the features selected
more often are good features. It is also known as stability selection.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>C <span class="classifier-delimiter">:</span> <span class="classifier">float, optional, default=1</span></dt>
<dd>The regularization parameter C in the LogisticRegression.</dd>
<dt>scaling <span class="classifier-delimiter">:</span> <span class="classifier">float, optional, default=0.5</span></dt>
<dd>The alpha parameter in the stability selection article used to
randomly scale the features. Should be between 0 and 1.</dd>
<dt>sample_fraction <span class="classifier-delimiter">:</span> <span class="classifier">float, optional, default=0.75</span></dt>
<dd>The fraction of samples to be used in each randomized design.
Should be between 0 and 1. If 1, all samples are used.</dd>
<dt>n_resampling <span class="classifier-delimiter">:</span> <span class="classifier">int, optional, default=200</span></dt>
<dd>Number of randomized models.</dd>
<dt>selection_threshold: float, optional, default=0.25</dt>
<dd>The score above which features should be selected.</dd>
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default=True</span></dt>
<dd>whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">boolean or integer, optional</span></dt>
<dd>Sets the verbosity amount</dd>
<dt>normalize <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default=False</span></dt>
<dd>If True, the regressors X will be normalized before regression.</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional, default=1e-3</span></dt>
<dd>tolerance for stopping criteria of LogisticRegression</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span></dt>
<dd>Number of CPUs to use during the resampling. If &#8216;-1&#8217;, use
all the CPUs</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
<dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>pre_dispatch <span class="classifier-delimiter">:</span> <span class="classifier">int, or string, optional</span></dt>
<dd><p class="first">Controls the number of jobs that get dispatched during parallel
execution. Reducing this number can be useful to avoid an
explosion of memory consumption when more jobs get dispatched
than CPUs can process. This parameter can be:</p>
<blockquote class="last">
<div><ul class="simple">
<li>None, in which case all the jobs are immediately
created and spawned. Use this for lightweight and
fast-running jobs, to avoid delays due to on-demand
spawning of the jobs</li>
<li>An int, giving the exact number of total jobs that are
spawned</li>
<li>A string, giving an expression as a function of n_jobs,
as in &#8216;2*n_jobs&#8217;</li>
</ul>
</div></blockquote>
</dd>
<dt>memory <span class="classifier-delimiter">:</span> <span class="classifier">Instance of joblib.Memory or string</span></dt>
<dd>Used for internal caching. By default, no caching is done.
If a string is given, it is thepath to the caching directory.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>scores_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_features]</span></dt>
<dd>Feature scores between 0 and 1.</dd>
<dt><cite>all_scores_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_features, n_reg_parameter]</span></dt>
<dd>Feature scores between 0 and 1 for all values of the regularization         parameter. The reference article suggests <tt class="docutils literal"><span class="pre">scores_</span></tt> is the max         of <tt class="docutils literal"><span class="pre">all_scores_</span></tt>.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">RandomizedLogisticRegression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">randomized_logistic</span> <span class="o">=</span> <span class="n">RandomizedLogisticRegression</span><span class="p">()</span>
</pre></div>
</div>
<p><strong>Notes</strong></p>
<p>See examples/linear_model/plot_sparse_recovery.py for an example.</p>
<p><strong>References</strong></p>
<p>Stability selection
Nicolai Meinshausen, Peter Buhlmann
Journal of the Royal Statistical Society: Series B
Volume 72, Issue 4, pages 417-473, September 2010
DOI: 10.1111/j.1467-9868.2010.00740.x</p>
<p>See also</p>
<p>RandomizedLasso, Lasso, ElasticNet</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>RandomizedLogisticRegressionSklearnNode</strong></li>
<li><strong>RandomizedLogisticRegressionSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-randomizedpcasklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.RandomizedPCASklearnNode" title="pySPACE.missions.nodes.scikits_nodes.RandomizedPCASklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RandomizedPCASklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-randomizedpcasklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.RandomizedPCASklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">RandomizedPCASklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.RandomizedPCASklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Principal component analysis (PCA) using randomized SVD</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.decomposition.pca.RandomizedPCA</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Linear dimensionality reduction using approximated Singular Value
Decomposition of the data and keeping only the most significant
singular vectors to project the data to a lower dimensional space.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>Maximum number of components to keep. When not given or None, this
is set to n_features (the second dimension of the training data).</dd>
<dt>copy <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd>If False, data passed to fit are overwritten and running
fit(X).transform(X) will not yield the expected results,
use fit_transform(X) instead.</dd>
<dt>iterated_power <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>Number of iterations for the power method. 3 by default.</dd>
<dt>whiten <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span></dt>
<dd><p class="first">When True (False by default) the <cite>components_</cite> vectors are divided
by the singular values to ensure uncorrelated outputs with unit
component-wise variances.</p>
<p class="last">Whitening will remove some information from the transformed signal
(the relative variance scales of the components) but can sometime
improve the predictive accuracy of the downstream estimators by
making their data respect some hard-wired assumptions.</p>
</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int or RandomState instance or None (default)</span></dt>
<dd>Pseudo Random Number generator seed control. If None, use the
numpy.random singleton.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>components_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span></dt>
<dd>Components with maximum variance.</dd>
<dt><cite>explained_variance_ratio_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components]</span></dt>
<dd>Percentage of variance explained by each of the selected components.         k is not set then all components are stored and the sum of explained         variances is equal to 1.0</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">RandomizedPCA</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pca</span> <span class="o">=</span> <span class="n">RandomizedPCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>                 
<span class="go">RandomizedPCA(copy=True, iterated_power=3, n_components=2,</span>
<span class="go">       random_state=None, whiten=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">)</span> 
<span class="go">[ 0.99244...  0.00755...]</span>
</pre></div>
</div>
<p>See also</p>
<p>PCA
ProbabilisticPCA
TruncatedSVD</p>
<p><strong>References</strong></p>
<table class="docutils citation" frame="void" id="halko2009" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[Halko2009]</td><td><cite>Finding structure with randomness: Stochastic algorithms
for constructing approximate matrix decompositions Halko, et al., 2009
(arXiv:909)</cite></td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="mrt" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[MRT]</td><td><cite>A randomized algorithm for the decomposition of matrices
Per-Gunnar Martinsson, Vladimir Rokhlin and Mark Tygert</cite></td></tr>
</tbody>
</table>
<p><strong>Notes</strong></p>
<p>This class supports sparse matrix input for backward compatibility, but
actually computes a truncated SVD instead of a PCA in that case (i.e. no
centering is performed). This support is deprecated; use the class
TruncatedSVD for sparse matrix support.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>RandomizedPCASklearnNode</strong></li>
<li><strong>RandomizedPCASklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-randomizedsearchcvsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.RandomizedSearchCVSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.RandomizedSearchCVSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RandomizedSearchCVSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-randomizedsearchcvsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.RandomizedSearchCVSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">RandomizedSearchCVSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.RandomizedSearchCVSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Randomized search on hyper parameters.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.grid_search.RandomizedSearchCV</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>RandomizedSearchCV implements a &#8220;fit&#8221; method and a &#8220;predict&#8221; method like
any classifier except that the parameters of the classifier
used to predict is optimized by cross-validation.</p>
<p>In contrast to GridSearchCV, not all parameter values are tried out, but
rather a fixed number of parameter settings is sampled from the specified
distributions. The number of parameter settings that are tried is
given by n_iter.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>estimator <span class="classifier-delimiter">:</span> <span class="classifier">object type that implements the &#8220;fit&#8221; and &#8220;predict&#8221; methods</span></dt>
<dd>A object of that type is instantiated for each parameter setting.</dd>
<dt>param_distributions <span class="classifier-delimiter">:</span> <span class="classifier">dict</span></dt>
<dd>Dictionary with parameters names (string) as keys and distributions
or lists of parameters to try. Distributions must provide a <tt class="docutils literal"><span class="pre">rvs</span></tt>
method for sampling (such as those from scipy.stats.distributions).
If a list is given, it is sampled uniformly.</dd>
<dt>n_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, default=10</span></dt>
<dd>Number of parameter settings that are sampled. n_iter trades
off runtime vs quality of the solution.</dd>
<dt>scoring <span class="classifier-delimiter">:</span> <span class="classifier">string, callable or None, optional, default: None</span></dt>
<dd>A string (see model evaluation documentation) or
a scorer callable object / function with signature
<tt class="docutils literal"><span class="pre">scorer(estimator,</span> <span class="pre">X,</span> <span class="pre">y)</span></tt>.</dd>
<dt>fit_params <span class="classifier-delimiter">:</span> <span class="classifier">dict, optional</span></dt>
<dd>Parameters to pass to the fit method.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>Number of jobs to run in parallel (default 1).</dd>
<dt>pre_dispatch <span class="classifier-delimiter">:</span> <span class="classifier">int, or string, optional</span></dt>
<dd><p class="first">Controls the number of jobs that get dispatched during parallel
execution. Reducing this number can be useful to avoid an
explosion of memory consumption when more jobs get dispatched
than CPUs can process. This parameter can be:</p>
<blockquote class="last">
<div><ul class="simple">
<li>None, in which case all the jobs are immediately
created and spawned. Use this for lightweight and
fast-running jobs, to avoid delays due to on-demand
spawning of the jobs</li>
<li>An int, giving the exact number of total jobs that are
spawned</li>
<li>A string, giving an expression as a function of n_jobs,
as in &#8216;2*n_jobs&#8217;</li>
</ul>
</div></blockquote>
</dd>
<dt>iid <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span></dt>
<dd>If True, the data is assumed to be identically distributed across
the folds, and the loss minimized is the total loss per sample,
and not the mean loss across the folds.</dd>
<dt>cv <span class="classifier-delimiter">:</span> <span class="classifier">integer or cross-validation generator, optional</span></dt>
<dd>If an integer is passed, it is the number of folds (default 3).
Specific cross-validation objects can be passed, see
sklearn.cross_validation module for the list of possible objects</dd>
<dt>refit <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd>Refit the best estimator with the entire dataset.
If &#8220;False&#8221;, it is impossible to make predictions using
this RandomizedSearchCV instance after fitting.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">integer</span></dt>
<dd>Controls the verbosity: the higher, the more messages.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>grid_scores_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">list of named tuples</span></dt>
<dd><p class="first">Contains scores for all parameter combinations in param_grid.
Each entry corresponds to one parameter setting.
Each named tuple has the attributes:</p>
<blockquote class="last">
<div><ul class="simple">
<li><tt class="docutils literal"><span class="pre">parameters</span></tt>, a dict of parameter settings</li>
<li><tt class="docutils literal"><span class="pre">mean_validation_score</span></tt>, the mean score over the
cross-validation folds</li>
<li><tt class="docutils literal"><span class="pre">cv_validation_scores</span></tt>, the list of scores for each fold</li>
</ul>
</div></blockquote>
</dd>
<dt><cite>best_estimator_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">estimator</span></dt>
<dd>Estimator that was chosen by the search, i.e. estimator
which gave highest score (or smallest loss if specified)
on the left out data.</dd>
<dt><cite>best_score_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Score of best_estimator on the left out data.</dd>
<dt><cite>best_params_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">dict</span></dt>
<dd>Parameter setting that gave the best results on the hold out data.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>The parameters selected are those that maximize the score of the held-out
data, according to the scoring parameter.</p>
<p>If <cite>n_jobs</cite> was set to a value higher than one, the data is copied for each
parameter setting(and not <cite>n_jobs</cite> times). This is done for efficiency
reasons if individual jobs take very little time, but may raise errors if
the dataset is large and not enough memory is available.  A workaround in
this case is to set <cite>pre_dispatch</cite>. Then, the memory is copied only
<cite>pre_dispatch</cite> many times. A reasonable value for <cite>pre_dispatch</cite> is <cite>2 *
n_jobs</cite>.</p>
<p>See Also</p>
<p><tt class="xref py py-class docutils literal"><span class="pre">GridSearchCV</span></tt>:</p>
<blockquote>
<div><ul class="simple">
<li>Does exhaustive search over a grid of parameters.</li>
</ul>
</div></blockquote>
<p><tt class="xref py py-class docutils literal"><span class="pre">ParameterSampler</span></tt>:</p>
<blockquote>
<div><ul class="simple">
<li>A generator over parameter settins, constructed from</li>
<li>param_distributions.</li>
</ul>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>RandomizedSearchCVSklearnNode</strong></li>
<li><strong>RandomizedSearchCVSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-ridgeclassifiercvsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.RidgeClassifierCVSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.RidgeClassifierCVSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RidgeClassifierCVSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-ridgeclassifiercvsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.RidgeClassifierCVSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">RidgeClassifierCVSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.RidgeClassifierCVSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Ridge classifier with built-in cross-validation.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.linear_model.ridge.RidgeClassifierCV</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>By default, it performs Generalized Cross-Validation, which is a form of
efficient Leave-One-Out cross-validation. Currently, only the n_features &gt;
n_samples case is handled efficiently.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>alphas: numpy array of shape [n_alphas]</dt>
<dd>Array of alpha values to try.
Small positive values of alpha improve the conditioning of the
problem and reduce the variance of the estimates.
Alpha corresponds to <tt class="docutils literal"><span class="pre">(2*C)^-1</span></tt> in other linear models such as
LogisticRegression or LinearSVC.</dd>
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd>Whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>normalize <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span></dt>
<dd>If True, the regressors X will be normalized before regression.</dd>
<dt>scoring <span class="classifier-delimiter">:</span> <span class="classifier">string, callable or None, optional, default: None</span></dt>
<dd>A string (see model evaluation documentation) or
a scorer callable object / function with signature
<tt class="docutils literal"><span class="pre">scorer(estimator,</span> <span class="pre">X,</span> <span class="pre">y)</span></tt>.</dd>
<dt>cv <span class="classifier-delimiter">:</span> <span class="classifier">cross-validation generator, optional</span></dt>
<dd>If None, Generalized Cross-Validation (efficient Leave-One-Out)
will be used.</dd>
<dt>class_weight <span class="classifier-delimiter">:</span> <span class="classifier">dict, optional</span></dt>
<dd>Weights associated with classes in the form
<tt class="docutils literal"><span class="pre">{class_label</span> <span class="pre">:</span> <span class="pre">weight}</span></tt>. If not given, all classes are
supposed to have weight one.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>cv_values_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_samples, n_alphas] or     shape = [n_samples, n_responses, n_alphas], optional</span></dt>
<dd>Cross-validation values for each alpha (if <cite>store_cv_values=True</cite> and</dd>
</dl>
<p><cite>cv=None</cite>). After <cite>fit()</cite> has been called, this attribute will contain     the mean squared errors (by default) or the values of the     <cite>{loss,score}_func</cite> function (if provided in the constructor).</p>
<dl class="docutils">
<dt><cite>coef_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_features] or [n_targets, n_features]</span></dt>
<dd>Weight vector(s).</dd>
<dt><cite>alpha_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Estimated regularization parameter</dd>
</dl>
<p>See also</p>
<p>Ridge: Ridge regression
RidgeClassifier: Ridge classifier
RidgeCV: Ridge regression with built-in cross validation</p>
<p><strong>Notes</strong></p>
<p>For multi-class classification, n_class classifiers are trained in
a one-versus-all approach. Concretely, this is implemented by taking
advantage of the multi-variate response support in Ridge.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>RidgeClassifierCVSklearnNode</strong></li>
<li><strong>RidgeClassifierCVSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-ridgeclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.RidgeClassifierSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.RidgeClassifierSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.RidgeClassifierSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-ridgeclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.RidgeClassifierSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">RidgeClassifierSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.RidgeClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Classifier using Ridge regression.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.linear_model.ridge.RidgeClassifier</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Small positive values of alpha improve the conditioning of the problem
and reduce the variance of the estimates.  Alpha corresponds to
<tt class="docutils literal"><span class="pre">(2*C)^-1</span></tt> in other linear models such as LogisticRegression or
LinearSVC.</dd>
<dt>class_weight <span class="classifier-delimiter">:</span> <span class="classifier">dict, optional</span></dt>
<dd>Weights associated with classes in the form
<tt class="docutils literal"><span class="pre">{class_label</span> <span class="pre">:</span> <span class="pre">weight}</span></tt>. If not given, all classes are
supposed to have weight one.</dd>
<dt>copy_X <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span></dt>
<dd>If True, X will be copied; else, it may be overwritten.</dd>
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span></dt>
<dd>Whether to calculate the intercept for this model. If set to false, no
intercept will be used in calculations (e.g. data is expected to be
already centered).</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>Maximum number of iterations for conjugate gradient solver.
The default value is determined by scipy.sparse.linalg.</dd>
<dt>normalize <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span></dt>
<dd>If True, the regressors X will be normalized before regression.</dd>
<dt>solver <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;auto&#8217;, &#8216;svd&#8217;, &#8216;dense_cholesky&#8217;, &#8216;lsqr&#8217;, &#8216;sparse_cg&#8217;}</span></dt>
<dd>Solver to use in the computational
routines. &#8216;svd&#8217; will use a Sinvular value decomposition to obtain
the solution, &#8216;dense_cholesky&#8217; will use the standard
scipy.linalg.solve function, &#8216;sparse_cg&#8217; will use the
conjugate gradient solver as found in
scipy.sparse.linalg.cg while &#8216;auto&#8217; will chose the most
appropriate depending on the matrix X. &#8216;lsqr&#8217; uses
a direct regularized least-squares routine provided by scipy.</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Precision of the solution.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>coef_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_features] or [n_classes, n_features]</span></dt>
<dd>Weight vector(s).</dd>
</dl>
<p>See also</p>
<p>Ridge, RidgeClassifierCV</p>
<p><strong>Notes</strong></p>
<p>For multi-class classification, n_class classifiers are trained in
a one-versus-all approach. Concretely, this is implemented by taking
advantage of the multi-variate response support in Ridge.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>RidgeClassifierSklearn</strong></li>
<li><strong>RidgeClassifierSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-sgdclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.SGDClassifierSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.SGDClassifierSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.SGDClassifierSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-sgdclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.SGDClassifierSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">SGDClassifierSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.SGDClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Linear classifiers (SVM, logistic regression, a.o.) with SGD training.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.linear_model.stochastic_gradient.SGDClassifier</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>This estimator implements regularized linear models with stochastic
gradient descent (SGD) learning: the gradient of the loss is estimated
each sample at a time and the model is updated along the way with a
decreasing strength schedule (aka learning rate). SGD allows minibatch
(online/out-of-core) learning, see the partial_fit method.</p>
<p>This implementation works with data represented as dense or sparse arrays
of floating point values for the features. The model it fits can be
controlled with the loss parameter; by default, it fits a linear support
vector machine (SVM).</p>
<p>The regularizer is a penalty added to the loss function that shrinks model
parameters towards the zero vector using either the squared euclidean norm
L2 or the absolute norm L1 or a combination of both (Elastic Net). If the
parameter update crosses the 0.0 value because of the regularizer, the
update is truncated to 0.0 to allow for learning sparse models and achieve
online feature selection.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>loss <span class="classifier-delimiter">:</span> <span class="classifier">str, &#8216;hinge&#8217;, &#8216;log&#8217;, &#8216;modified_huber&#8217;, &#8216;squared_hinge&#8217;,                &#8216;perceptron&#8217;, or a regression loss: &#8216;squared_loss&#8217;, &#8216;huber&#8217;,                &#8216;epsilon_insensitive&#8217;, or &#8216;squared_epsilon_insensitive&#8217;</span></dt>
<dd>The loss function to be used. Defaults to &#8216;hinge&#8217;, which gives a
linear SVM.
The &#8216;log&#8217; loss gives logistic regression, a probabilistic classifier.
&#8216;modified_huber&#8217; is another smooth loss that brings tolerance to
outliers as well as probability estimates.
&#8216;squared_hinge&#8217; is like hinge but is quadratically penalized.
&#8216;perceptron&#8217; is the linear loss used by the perceptron algorithm.
The other losses are designed for regression but can be useful in
classification as well; see SGDRegressor for a description.</dd>
<dt>penalty <span class="classifier-delimiter">:</span> <span class="classifier">str, &#8216;l2&#8217; or &#8216;l1&#8217; or &#8216;elasticnet&#8217;</span></dt>
<dd>The penalty (aka regularization term) to be used. Defaults to &#8216;l2&#8217;
which is the standard regularizer for linear SVM models. &#8216;l1&#8217; and
&#8216;elasticnet&#8217; migh bring sparsity to the model (feature selection)
not achievable with &#8216;l2&#8217;.</dd>
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Constant that multiplies the regularization term. Defaults to 0.0001</dd>
<dt>l1_ratio <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>The Elastic Net mixing parameter, with 0 &lt;= l1_ratio &lt;= 1.
l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.
Defaults to 0.15.</dd>
<dt>fit_intercept: bool</dt>
<dd>Whether the intercept should be estimated or not. If False, the
data is assumed to be already centered. Defaults to True.</dd>
<dt>n_iter: int, optional</dt>
<dd>The number of passes over the training data (aka epochs).
Defaults to 5.</dd>
<dt>shuffle: bool, optional</dt>
<dd>Whether or not the training data should be shuffled after each epoch.
Defaults to False.</dd>
<dt>random_state: int seed, RandomState instance, or None (default)</dt>
<dd>The seed of the pseudo random number generator to use when
shuffling the data.</dd>
<dt>verbose: integer, optional</dt>
<dd>The verbosity level</dd>
<dt>epsilon: float</dt>
<dd>Epsilon in the epsilon-insensitive loss functions; only if <cite>loss</cite> is
&#8216;huber&#8217;, &#8216;epsilon_insensitive&#8217;, or &#8216;squared_epsilon_insensitive&#8217;.
For &#8216;huber&#8217;, determines the threshold at which it becomes less
important to get the prediction exactly right.
For epsilon-insensitive, any differences between the current prediction
and the correct label are ignored if they are less than this threshold.</dd>
<dt>n_jobs: integer, optional</dt>
<dd>The number of CPUs to use to do the OVA (One Versus All, for
multi-class problems) computation. -1 means &#8216;all CPUs&#8217;. Defaults
to 1.</dd>
<dt>learning_rate <span class="classifier-delimiter">:</span> <span class="classifier">string, optional</span></dt>
<dd><p class="first">The learning rate:</p>
<ul class="last simple">
<li>constant: eta = eta0</li>
<li>optimal: eta = 1.0/(t+t0) [default]</li>
<li>invscaling: eta = eta0 / pow(t, power_t)</li>
</ul>
</dd>
<dt>eta0 <span class="classifier-delimiter">:</span> <span class="classifier">double</span></dt>
<dd>The initial learning rate [default 0.01].</dd>
<dt>power_t <span class="classifier-delimiter">:</span> <span class="classifier">double</span></dt>
<dd>The exponent for inverse scaling learning rate [default 0.5].</dd>
<dt>class_weight <span class="classifier-delimiter">:</span> <span class="classifier">dict, {class_label</span> <span class="classifier-delimiter">:</span> <span class="classifier">weight} or &#8220;auto&#8221; or None, optional</span></dt>
<dd><p class="first">Preset for the class_weight fit parameter.</p>
<p>Weights associated with classes. If not given, all classes
are supposed to have weight one.</p>
<p class="last">The &#8220;auto&#8221; mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies.</p>
</dd>
<dt>warm_start <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span></dt>
<dd>When set to True, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.</dd>
</dl>
<p><strong>Attributes</strong></p>
<p><cite>coef_</cite> : array, shape = [1, n_features] if n_classes == 2 else [n_classes,
n_features]</p>
<blockquote>
<div>Weights assigned to the features.</div></blockquote>
<dl class="docutils">
<dt><cite>intercept_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [1] if n_classes == 2 else [n_classes]</span></dt>
<dd>Constants in decision function.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">SGDClassifier</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="gp">... </span>
<span class="go">SGDClassifier(alpha=0.0001, class_weight=None, epsilon=0.1, eta0=0.0,</span>
<span class="go">        fit_intercept=True, l1_ratio=0.15, learning_rate=&#39;optimal&#39;,</span>
<span class="go">        loss=&#39;hinge&#39;, n_iter=5, n_jobs=1, penalty=&#39;l2&#39;, power_t=0.5,</span>
<span class="go">        random_state=None, rho=None, shuffle=False,</span>
<span class="go">        verbose=0, warm_start=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
<p>See also</p>
<p>LinearSVC, LogisticRegression, Perceptron</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>SGDClassifierSklearnNode</strong></li>
<li><strong>SGDClassifierSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-sgdregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.SGDRegressorSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.SGDRegressorSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.SGDRegressorSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-sgdregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.SGDRegressorSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">SGDRegressorSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.SGDRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Linear model fitted by minimizing a regularized empirical loss with SGD</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.linear_model.stochastic_gradient.SGDRegressor</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>SGD stands for Stochastic Gradient Descent: the gradient of the loss is
estimated each sample at a time and the model is updated along the way with
a decreasing strength schedule (aka learning rate).</p>
<p>The regularizer is a penalty added to the loss function that shrinks model
parameters towards the zero vector using either the squared euclidean norm
L2 or the absolute norm L1 or a combination of both (Elastic Net). If the
parameter update crosses the 0.0 value because of the regularizer, the
update is truncated to 0.0 to allow for learning sparse models and achieve
online feature selection.</p>
<p>This implementation works with data represented as dense numpy arrays of
floating point values for the features.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>loss <span class="classifier-delimiter">:</span> <span class="classifier">str, &#8216;squared_loss&#8217;, &#8216;huber&#8217;, &#8216;epsilon_insensitive&#8217;,                 or &#8216;squared_epsilon_insensitive&#8217;</span></dt>
<dd>The loss function to be used. Defaults to &#8216;squared_loss&#8217; which refers
to the ordinary least squares fit. &#8216;huber&#8217; modifies &#8216;squared_loss&#8217; to
focus less on getting outliers correct by switching from squared to
linear loss past a distance of epsilon. &#8216;epsilon_insensitive&#8217; ignores
errors less than epsilon and is linear past that; this is the loss
function used in SVR. &#8216;squared_epsilon_insensitive&#8217; is the same but
becomes squared loss past a tolerance of epsilon.</dd>
<dt>penalty <span class="classifier-delimiter">:</span> <span class="classifier">str, &#8216;l2&#8217; or &#8216;l1&#8217; or &#8216;elasticnet&#8217;</span></dt>
<dd>The penalty (aka regularization term) to be used. Defaults to &#8216;l2&#8217;
which is the standard regularizer for linear SVM models. &#8216;l1&#8217; and
&#8216;elasticnet&#8217; migh bring sparsity to the model (feature selection)
not achievable with &#8216;l2&#8217;.</dd>
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Constant that multiplies the regularization term. Defaults to 0.0001</dd>
<dt>l1_ratio <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>The Elastic Net mixing parameter, with 0 &lt;= l1_ratio &lt;= 1.
l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.
Defaults to 0.15.</dd>
<dt>fit_intercept: bool</dt>
<dd>Whether the intercept should be estimated or not. If False, the
data is assumed to be already centered. Defaults to True.</dd>
<dt>n_iter: int, optional</dt>
<dd>The number of passes over the training data (aka epochs).
Defaults to 5.</dd>
<dt>shuffle: bool, optional</dt>
<dd>Whether or not the training data should be shuffled after each epoch.
Defaults to False.</dd>
<dt>random_state: int seed, RandomState instance, or None (default)</dt>
<dd>The seed of the pseudo random number generator to use when
shuffling the data.</dd>
<dt>verbose: integer, optional</dt>
<dd>The verbosity level.</dd>
<dt>epsilon: float</dt>
<dd>Epsilon in the epsilon-insensitive loss functions; only if <cite>loss</cite> is
&#8216;huber&#8217;, &#8216;epsilon_insensitive&#8217;, or &#8216;squared_epsilon_insensitive&#8217;.
For &#8216;huber&#8217;, determines the threshold at which it becomes less
important to get the prediction exactly right.
For epsilon-insensitive, any differences between the current prediction
and the correct label are ignored if they are less than this threshold.</dd>
<dt>learning_rate <span class="classifier-delimiter">:</span> <span class="classifier">string, optional</span></dt>
<dd><p class="first">The learning rate:</p>
<ul class="last simple">
<li>constant: eta = eta0</li>
<li>optimal: eta = 1.0/(t+t0)</li>
<li>invscaling: eta = eta0 / pow(t, power_t) [default]</li>
</ul>
</dd>
<dt>eta0 <span class="classifier-delimiter">:</span> <span class="classifier">double, optional</span></dt>
<dd>The initial learning rate [default 0.01].</dd>
<dt>power_t <span class="classifier-delimiter">:</span> <span class="classifier">double, optional</span></dt>
<dd>The exponent for inverse scaling learning rate [default 0.25].</dd>
<dt>warm_start <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span></dt>
<dd>When set to True, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>coef_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_features]</span></dt>
<dd>Weights asigned to the features.</dd>
<dt><cite>intercept_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [1]</span></dt>
<dd>The intercept term.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">SGDRegressor</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">SGDRegressor(alpha=0.0001, epsilon=0.1, eta0=0.01, fit_intercept=True,</span>
<span class="go">       l1_ratio=0.15, learning_rate=&#39;invscaling&#39;, loss=&#39;squared_loss&#39;,</span>
<span class="go">       n_iter=5, penalty=&#39;l2&#39;, power_t=0.25, random_state=None, rho=None,</span>
<span class="go">       shuffle=False, verbose=0, warm_start=False)</span>
</pre></div>
</div>
<p>See also</p>
<p>Ridge, ElasticNet, Lasso, SVR</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>SGDRegressorSklearn</strong></li>
<li><strong>SGDRegressorSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-svcsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.SVCSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.SVCSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.SVCSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-svcsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.SVCSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">SVCSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.SVCSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>C-Support Vector Classification.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.svm.classes.SVC</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>The implementations is a based on libsvm. The fit time complexity
is more than quadratic with the number of samples which makes it hard
to scale to dataset with more than a couple of 10000 samples.</p>
<p>The multiclass support is handled according to a one-vs-one scheme.</p>
<p>For details on the precise mathematical formulation of the provided
kernel functions and how <cite>gamma</cite>, <cite>coef0</cite> and <cite>degree</cite> affect each,
see the corresponding section in the narrative documentation:</p>
<p><em class="xref std std-ref">svm_kernels</em>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>C <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1.0)</span></dt>
<dd>Penalty parameter C of the error term.</dd>
<dt>kernel <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=&#8217;rbf&#8217;)</span></dt>
<dd>Specifies the kernel type to be used in the algorithm.
It must be one of &#8216;linear&#8217;, &#8216;poly&#8217;, &#8216;rbf&#8217;, &#8216;sigmoid&#8217;, &#8216;precomputed&#8217; or
a callable.
If none is given, &#8216;rbf&#8217; will be used. If a callable is given it is
used to precompute the kernel matrix.</dd>
<dt>degree <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=3)</span></dt>
<dd>Degree of the polynomial kernel function (&#8216;poly&#8217;).
Ignored by all other kernels.</dd>
<dt>gamma <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.0)</span></dt>
<dd>Kernel coefficient for &#8216;rbf&#8217;, &#8216;poly&#8217; and &#8216;sigm&#8217;.
If gamma is 0.0 then 1/n_features will be used instead.</dd>
<dt>coef0 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.0)</span></dt>
<dd>Independent term in kernel function.
It is only significant in &#8216;poly&#8217; and &#8216;sigmoid&#8217;.</dd>
<dt>probability: boolean, optional (default=False)</dt>
<dd>Whether to enable probability estimates. This must be enabled prior
to calling <cite>fit</cite>, and will slow down that method.</dd>
<dt>shrinking: boolean, optional (default=True)</dt>
<dd>Whether to use the shrinking heuristic.</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1e-3)</span></dt>
<dd>Tolerance for stopping criterion.</dd>
<dt>cache_size <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Specify the size of the kernel cache (in MB)</dd>
<dt>class_weight <span class="classifier-delimiter">:</span> <span class="classifier">{dict, &#8216;auto&#8217;}, optional</span></dt>
<dd>Set the parameter C of class i to class_weight[i]*C for
SVC. If not given, all classes are supposed to have
weight one. The &#8216;auto&#8217; mode uses the values of y to
automatically adjust weights inversely proportional to
class frequencies.</dd>
<dt>verbose <span class="classifier-delimiter">:</span> <span class="classifier">bool, default: False</span></dt>
<dd>Enable verbose output. Note that this setting takes advantage of a
per-process runtime setting in libsvm that, if enabled, may not work
properly in a multithreaded context.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=-1)</span></dt>
<dd>Hard limit on iterations within solver, or -1 for no limit.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int seed, RandomState instance, or None (default)</span></dt>
<dd>The seed of the pseudo random number generator to use when
shuffling the data for probability estimation.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>support_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_SV]</span></dt>
<dd>Index of support vectors.</dd>
<dt><cite>support_vectors_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_SV, n_features]</span></dt>
<dd>Support vectors.</dd>
<dt><cite>n_support_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, dtype=int32, shape = [n_class]</span></dt>
<dd>number of support vector for each class.</dd>
<dt><cite>dual_coef_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_class-1, n_SV]</span></dt>
<dd>Coefficients of the support vector in the decision function.         For multiclass, coefficient for all 1-vs-1 classifiers.         The layout of the coefficients in the multiclass case is somewhat         non-trivial. See the section about multi-class classification in the         SVM section of the User Guide for details.</dd>
<dt><cite>coef_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_class-1, n_features]</span></dt>
<dd><p class="first">Weights asigned to the features (coefficients in the primal
problem). This is only available in the case of linear kernel.</p>
<p class="last"><cite>coef_</cite> is readonly property derived from <cite>dual_coef_</cite> and
<cite>support_vectors_</cite></p>
</dd>
<dt><cite>intercept_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_class * (n_class-1) / 2]</span></dt>
<dd>Constants in decision function.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,</span>
<span class="go">    gamma=0.0, kernel=&#39;rbf&#39;, max_iter=-1, probability=False,</span>
<span class="go">    random_state=None, shrinking=True, tol=0.001, verbose=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
<p>See also</p>
<dl class="docutils">
<dt>SVR</dt>
<dd>Support Vector Machine for Regression implemented using libsvm.</dd>
<dt>LinearSVC</dt>
<dd>Scalable Linear Support Vector Machine for classification
implemented using liblinear. Check the See also section of
LinearSVC for more comparison element.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>SVCSklearnNode</strong></li>
<li><strong>SVCSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-scalersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.ScalerSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.ScalerSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.ScalerSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-scalersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.ScalerSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">ScalerSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.ScalerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>ScalerSklearnNode</strong></li>
<li><strong>ScalerSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-selectfdrsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.SelectFdrSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.SelectFdrSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.SelectFdrSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-selectfdrsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.SelectFdrSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">SelectFdrSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.SelectFdrSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Filter: Select the p-values for an estimated false discovery rate</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.feature_selection.univariate_selection.SelectFdr</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>This uses the Benjamini-Hochberg procedure. <tt class="docutils literal"><span class="pre">alpha</span></tt> is the target false
discovery rate.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>score_func <span class="classifier-delimiter">:</span> <span class="classifier">callable</span></dt>
<dd>Function taking two arrays X and y, and returning a pair of arrays
(scores, pvalues).</dd>
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>The highest uncorrected p-value for features to keep.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>scores_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span></dt>
<dd>Scores of features.</dd>
<dt><cite>pvalues_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span></dt>
<dd>p-values of feature scores.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>SelectFdrSklearn</strong></li>
<li><strong>SelectFdrSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-selectfprsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.SelectFprSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.SelectFprSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.SelectFprSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-selectfprsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.SelectFprSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">SelectFprSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.SelectFprSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Filter: Select the pvalues below alpha based on a FPR test.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.feature_selection.univariate_selection.SelectFpr</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>FPR test stands for False Positive Rate test. It controls the total
amount of false detections.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>score_func <span class="classifier-delimiter">:</span> <span class="classifier">callable</span></dt>
<dd>Function taking two arrays X and y, and returning a pair of arrays
(scores, pvalues).</dd>
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>The highest p-value for features to be kept.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>scores_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span></dt>
<dd>Scores of features.</dd>
<dt><cite>pvalues_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span></dt>
<dd>p-values of feature scores.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>SelectFprSklearnNode</strong></li>
<li><strong>SelectFprSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-selectfwesklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.SelectFweSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.SelectFweSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.SelectFweSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-selectfwesklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.SelectFweSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">SelectFweSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.SelectFweSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Filter: Select the p-values corresponding to Family-wise error rate</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.feature_selection.univariate_selection.SelectFwe</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>score_func <span class="classifier-delimiter">:</span> <span class="classifier">callable</span></dt>
<dd>Function taking two arrays X and y, and returning a pair of arrays
(scores, pvalues).</dd>
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>The highest uncorrected p-value for features to keep.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>scores_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span></dt>
<dd>Scores of features.</dd>
<dt><cite>pvalues_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span></dt>
<dd>p-values of feature scores.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>SelectFweSklearnNode</strong></li>
<li><strong>SelectFweSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-selectkbestsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.SelectKBestSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.SelectKBestSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.SelectKBestSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-selectkbestsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.SelectKBestSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">SelectKBestSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.SelectKBestSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Select features according to the k highest scores.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.feature_selection.univariate_selection.SelectKBest</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>score_func <span class="classifier-delimiter">:</span> <span class="classifier">callable</span></dt>
<dd>Function taking two arrays X and y, and returning a pair of arrays
(scores, pvalues).</dd>
<dt>k <span class="classifier-delimiter">:</span> <span class="classifier">int or &#8220;all&#8221;, optional, default=10</span></dt>
<dd>Number of top features to select.
The &#8220;all&#8221; option bypasses selection, for use in a parameter search.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>scores_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span></dt>
<dd>Scores of features.</dd>
<dt><cite>pvalues_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span></dt>
<dd>p-values of feature scores.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>Ties between features with equal scores will be broken in an unspecified
way.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>SelectKBestSklearn</strong></li>
<li><strong>SelectKBestSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-selectpercentilesklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.SelectPercentileSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.SelectPercentileSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.SelectPercentileSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-selectpercentilesklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.SelectPercentileSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">SelectPercentileSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.SelectPercentileSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Select features according to a percentile of the highest scores.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.feature_selection.univariate_selection.SelectPercentile</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>score_func <span class="classifier-delimiter">:</span> <span class="classifier">callable</span></dt>
<dd>Function taking two arrays X and y, and returning a pair of arrays
(scores, pvalues).</dd>
<dt>percentile <span class="classifier-delimiter">:</span> <span class="classifier">int, optional, default=10</span></dt>
<dd>Percent of features to keep.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>scores_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span></dt>
<dd>Scores of features.</dd>
<dt><cite>pvalues_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span></dt>
<dd>p-values of feature scores.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>Ties between features with equal scores will be broken in an unspecified
way.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>SelectPercentileSklearn</strong></li>
<li><strong>SelectPercentileSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-sparsecodersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.SparseCoderSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.SparseCoderSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.SparseCoderSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-sparsecodersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.SparseCoderSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">SparseCoderSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.SparseCoderSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Sparse coding</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.decomposition.dict_learning.SparseCoder</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Finds a sparse representation of data against a fixed, precomputed
dictionary.</p>
<p>Each row of the result is the solution to a sparse coding problem.
The goal is to find a sparse array <cite>code</cite> such that:</p>
<div class="highlight-python"><div class="highlight"><pre>X ~= code * dictionary
</pre></div>
</div>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>dictionary <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span></dt>
<dd>The dictionary atoms used for sparse coding. Lines are assumed to be
normalized to unit norm.</dd>
<dt>transform_algorithm <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;lasso_lars&#8217;, &#8216;lasso_cd&#8217;, &#8216;lars&#8217;, &#8216;omp&#8217;,     &#8216;threshold&#8217;}</span></dt>
<dd><p class="first">Algorithm used to transform the data:</p>
<ul class="last simple">
<li>lars: uses the least angle regression method (linear_model.lars_path)</li>
<li>lasso_lars: uses Lars to compute the Lasso solution</li>
<li>lasso_cd: uses the coordinate descent method to compute the</li>
<li>Lasso solution (linear_model.Lasso). lasso_lars will be faster if</li>
<li>the estimated components are sparse.</li>
<li>omp: uses orthogonal matching pursuit to estimate the sparse solution</li>
<li>threshold: squashes to zero all coefficients less than alpha from</li>
<li>the projection <tt class="docutils literal"><span class="pre">dictionary</span> <span class="pre">*</span> <span class="pre">X'</span></tt></li>
</ul>
</dd>
<dt>transform_n_nonzero_coefs <span class="classifier-delimiter">:</span> <span class="classifier">int, <tt class="docutils literal"><span class="pre">0.1</span> <span class="pre">*</span> <span class="pre">n_features</span></tt> by default</span></dt>
<dd>Number of nonzero coefficients to target in each column of the
solution. This is only used by <cite>algorithm=&#8217;lars&#8217;</cite> and <cite>algorithm=&#8217;omp&#8217;</cite>
and is overridden by <cite>alpha</cite> in the <cite>omp</cite> case.</dd>
<dt>transform_alpha <span class="classifier-delimiter">:</span> <span class="classifier">float, 1. by default</span></dt>
<dd>If <cite>algorithm=&#8217;lasso_lars&#8217;</cite> or <cite>algorithm=&#8217;lasso_cd&#8217;</cite>, <cite>alpha</cite> is the
penalty applied to the L1 norm.
If <cite>algorithm=&#8217;threshold&#8217;</cite>, <cite>alpha</cite> is the absolute value of the
threshold below which coefficients will be squashed to zero.
If <cite>algorithm=&#8217;omp&#8217;</cite>, <cite>alpha</cite> is the tolerance parameter: the value of
the reconstruction error targeted. In this case, it overrides
<cite>n_nonzero_coefs</cite>.</dd>
<dt>split_sign <span class="classifier-delimiter">:</span> <span class="classifier">bool, False by default</span></dt>
<dd>Whether to split the sparse feature vector into the concatenation of
its negative part and its positive part. This can improve the
performance of downstream classifiers.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>number of parallel jobs to run</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>components_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span></dt>
<dd>The unchanged dictionary atoms</dd>
</dl>
<p>See also</p>
<p>DictionaryLearning
MiniBatchDictionaryLearning
SparsePCA
MiniBatchSparsePCA
sparse_encode</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>SparseCoderSklearnNode</strong></li>
<li><strong>SparseCoderSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-sparsepcasklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.SparsePCASklearnNode" title="pySPACE.missions.nodes.scikits_nodes.SparsePCASklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.SparsePCASklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-sparsepcasklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.SparsePCASklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">SparsePCASklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.SparsePCASklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Sparse Principal Components Analysis (SparsePCA)</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.decomposition.sparse_pca.SparsePCA</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Finds the set of sparse components that can optimally reconstruct
the data.  The amount of sparseness is controllable by the coefficient
of the L1 penalty, given by the parameter alpha.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>Number of sparse atoms to extract.</dd>
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float,</span></dt>
<dd>Sparsity controlling parameter. Higher values lead to sparser
components.</dd>
<dt>ridge_alpha <span class="classifier-delimiter">:</span> <span class="classifier">float,</span></dt>
<dd>Amount of ridge shrinkage to apply in order to improve
conditioning when calling the transform method.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>Maximum number of iterations to perform.</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float,</span></dt>
<dd>Tolerance for the stopping condition.</dd>
<dt>method <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;lars&#8217;, &#8216;cd&#8217;}</span></dt>
<dd>lars: uses the least angle regression method to solve the lasso problem
(linear_model.lars_path)
cd: uses the coordinate descent method to compute the
Lasso solution (linear_model.Lasso). Lars will be faster if
the estimated components are sparse.</dd>
<dt>n_jobs <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
<dd>Number of parallel jobs to run.</dd>
<dt>U_init <span class="classifier-delimiter">:</span> <span class="classifier">array of shape (n_samples, n_components),</span></dt>
<dd>Initial values for the loadings for warm restart scenarios.</dd>
<dt>V_init <span class="classifier-delimiter">:</span> <span class="classifier">array of shape (n_components, n_features),</span></dt>
<dd>Initial values for the components for warm restart scenarios.</dd>
</dl>
<p>verbose :</p>
<blockquote>
<div><ul class="simple">
<li>Degree of verbosity of the printed output.</li>
</ul>
</div></blockquote>
<dl class="docutils">
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int or RandomState</span></dt>
<dd>Pseudo number generator state used for random sampling.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>components_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span></dt>
<dd>Sparse components extracted from the data.</dd>
<dt><cite>error_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array</span></dt>
<dd>Vector of errors at each iteration.</dd>
</dl>
<p>See also</p>
<p>PCA
MiniBatchSparsePCA
DictionaryLearning</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>SparsePCASklearnNode</strong></li>
<li><strong>SparsePCASklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-standardscalersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.StandardScalerSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.StandardScalerSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.StandardScalerSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-standardscalersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.StandardScalerSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">StandardScalerSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.StandardScalerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Standardize features by removing the mean and scaling to unit variance</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.preprocessing.data.StandardScaler</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Centering and scaling happen independently on each feature by computing
the relevant statistics on the samples in the training set. Mean and
standard deviation are then stored to be used on later data using the
<cite>transform</cite> method.</p>
<p>Standardization of a dataset is a common requirement for many
machine learning estimators: they might behave badly if the
individual feature do not more or less look like standard normally
distributed data (e.g. Gaussian with 0 mean and unit variance).</p>
<p>For instance many elements used in the objective function of
a learning algorithm (such as the RBF kernel of Support Vector
Machines or the L1 and L2 regularizers of linear models) assume that
all features are centered around 0 and have variance in the same
order. If a feature has a variance that is orders of magnitude larger
that others, it might dominate the objective function and make the
estimator unable to learn from other features correctly as expected.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>with_mean <span class="classifier-delimiter">:</span> <span class="classifier">boolean, True by default</span></dt>
<dd>If True, center the data before scaling.
This does not work (and will raise an exception) when attempted on
sparse matrices, because centering them entails building a dense
matrix which in common use cases is likely to be too large to fit in
memory.</dd>
<dt>with_std <span class="classifier-delimiter">:</span> <span class="classifier">boolean, True by default</span></dt>
<dd>If True, scale the data to unit variance (or equivalently,
unit standard deviation).</dd>
<dt>copy <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default is True</span></dt>
<dd>If False, try to avoid a copy and do inplace scaling instead.
This is not guaranteed to always work inplace; e.g. if the data is
not a NumPy array or scipy.sparse CSR matrix, a copy may still be
returned.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>mean_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of floats with shape [n_features]</span></dt>
<dd>The mean value for each feature in the training set.</dd>
<dt><cite>std_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array of floats with shape [n_features]</span></dt>
<dd>The standard deviation for each feature in the training set.</dd>
</dl>
<p>See also</p>
<p><tt class="xref py py-func docutils literal"><span class="pre">sklearn.preprocessing.scale()</span></tt> to perform centering and
scaling without using the <tt class="docutils literal"><span class="pre">Transformer</span></tt> object oriented API</p>
<p><tt class="xref py py-class docutils literal"><span class="pre">sklearn.decomposition.RandomizedPCA</span></tt> with <cite>whiten=True</cite>
to further remove the linear correlation across features.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>StandardScalerSklearnNode</strong></li>
<li><strong>StandardScalerSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-tfidftransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.TfidfTransformerSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.TfidfTransformerSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.TfidfTransformerSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-tfidftransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.TfidfTransformerSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">TfidfTransformerSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.TfidfTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Transform a count matrix to a normalized tf or tf–idf representation</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.feature_extraction.text.TfidfTransformer</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Tf means term-frequency while tf–idf means term-frequency times inverse
document-frequency. This is a common term weighting scheme in information
retrieval, that has also found good use in document classification.</p>
<p>The goal of using tf–idf instead of the raw frequencies of occurrence of a
token in a given document is to scale down the impact of tokens that occur
very frequently in a given corpus and that are hence empirically less
informative than features that occur in a small fraction of the training
corpus.</p>
<p>In the SMART notation used in IR, this class implements several tf–idf
variants:</p>
<p>Tf is &#8220;n&#8221; (natural) by default, &#8220;l&#8221; (logarithmic) when sublinear_tf=True.
Idf is &#8220;t&#8221; idf is &#8220;t&#8221; when use_idf is given, &#8220;n&#8221; (none) otherwise.
Normalization is &#8220;c&#8221; (cosine) when norm=&#8217;l2&#8217;, &#8220;n&#8221; (none) when norm=None.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>norm <span class="classifier-delimiter">:</span> <span class="classifier">&#8216;l1&#8217;, &#8216;l2&#8217; or None, optional</span></dt>
<dd>Norm used to normalize term vectors. None for no normalization.</dd>
<dt>use_idf <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span></dt>
<dd>Enable inverse-document-frequency reweighting.</dd>
<dt>smooth_idf <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span></dt>
<dd>Smooth idf weights by adding one to document frequencies, as if an
extra document was seen containing every term in the collection
exactly once. Prevents zero divisions.</dd>
<dt>sublinear_tf <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span></dt>
<dd>Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).</dd>
</dl>
<p><strong>References</strong></p>
<table class="docutils citation" frame="void" id="yates2011" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[Yates2011]</td><td><cite>R. Baeza-Yates and B. Ribeiro-Neto (2011). Modern
Information Retrieval. Addison Wesley, pp. 68–74.</cite></td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="msr2008" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[MSR2008]</td><td><cite>C.D. Manning, H. Schütze and P. Raghavan (2008). Introduction
to Information Retrieval. Cambridge University Press,
pp. 121–125.</cite></td></tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>TfidfTransformerSklearnNode</strong></li>
<li><strong>TfidfTransformerSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-tfidfvectorizersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.TfidfVectorizerSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.TfidfVectorizerSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.TfidfVectorizerSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-tfidfvectorizersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.TfidfVectorizerSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">TfidfVectorizerSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.TfidfVectorizerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Convert a collection of raw documents to a matrix of TF-IDF features.</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.feature_extraction.text.TfidfVectorizer</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>Equivalent to CountVectorizer followed by TfidfTransformer.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>input <span class="classifier-delimiter">:</span> <span class="classifier">string {&#8216;filename&#8217;, &#8216;file&#8217;, &#8216;content&#8217;}</span></dt>
<dd><p class="first">If filename, the sequence passed as an argument to fit is
expected to be a list of filenames that need reading to fetch
the raw content to analyze.</p>
<p>If &#8216;file&#8217;, the sequence items must have &#8216;read&#8217; method (file-like
object) it is called to fetch the bytes in memory.</p>
<p class="last">Otherwise the input is expected to be the sequence strings or
bytes items are expected to be analyzed directly.</p>
</dd>
<dt>encoding <span class="classifier-delimiter">:</span> <span class="classifier">string, &#8216;utf-8&#8217; by default.</span></dt>
<dd>If bytes or files are given to analyze, this encoding is used to
decode.</dd>
<dt>decode_error <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;strict&#8217;, &#8216;ignore&#8217;, &#8216;replace&#8217;}</span></dt>
<dd>Instruction on what to do if a byte sequence is given to analyze that
contains characters not of the given <cite>encoding</cite>. By default, it is
&#8216;strict&#8217;, meaning that a UnicodeDecodeError will be raised. Other
values are &#8216;ignore&#8217; and &#8216;replace&#8217;.</dd>
<dt>strip_accents <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;ascii&#8217;, &#8216;unicode&#8217;, None}</span></dt>
<dd>Remove accents during the preprocessing step.
&#8216;ascii&#8217; is a fast method that only works on characters that have
an direct ASCII mapping.
&#8216;unicode&#8217; is a slightly slower method that works on any characters.
None (default) does nothing.</dd>
<dt>analyzer <span class="classifier-delimiter">:</span> <span class="classifier">string, {&#8216;word&#8217;, &#8216;char&#8217;} or callable</span></dt>
<dd><p class="first">Whether the feature should be made of word or character n-grams.</p>
<p class="last">If a callable is passed it is used to extract the sequence of features
out of the raw, unprocessed input.</p>
</dd>
<dt>preprocessor <span class="classifier-delimiter">:</span> <span class="classifier">callable or None (default)</span></dt>
<dd>Override the preprocessing (string transformation) stage while
preserving the tokenizing and n-grams generation steps.</dd>
<dt>tokenizer <span class="classifier-delimiter">:</span> <span class="classifier">callable or None (default)</span></dt>
<dd>Override the string tokenization step while preserving the
preprocessing and n-grams generation steps.</dd>
<dt>ngram_range <span class="classifier-delimiter">:</span> <span class="classifier">tuple (min_n, max_n)</span></dt>
<dd>The lower and upper boundary of the range of n-values for different
n-grams to be extracted. All values of n such that min_n &lt;= n &lt;= max_n
will be used.</dd>
<dt>stop_words <span class="classifier-delimiter">:</span> <span class="classifier">string {&#8216;english&#8217;}, list, or None (default)</span></dt>
<dd><p class="first">If a string, it is passed to _check_stop_list and the appropriate stop
list is returned. &#8216;english&#8217; is currently the only supported string
value.</p>
<p>If a list, that list is assumed to contain stop words, all of which
will be removed from the resulting tokens.</p>
<p class="last">If None, no stop words will be used. max_df can be set to a value
in the range [0.7, 1.0) to automatically detect and filter stop
words based on intra corpus document frequency of terms.</p>
</dd>
<dt>lowercase <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default True</span></dt>
<dd>Convert all characters to lowercase befor tokenizing.</dd>
<dt>token_pattern <span class="classifier-delimiter">:</span> <span class="classifier">string</span></dt>
<dd>Regular expression denoting what constitutes a &#8220;token&#8221;, only used
if <cite>tokenize == &#8216;word&#8217;</cite>. The default regexp select tokens of 2
or more letters characters (punctuation is completely ignored
and always treated as a token separator).</dd>
<dt>max_df <span class="classifier-delimiter">:</span> <span class="classifier">float in range [0.0, 1.0] or int, optional, 1.0 by default</span></dt>
<dd>When building the vocabulary ignore terms that have a term frequency
strictly higher than the given threshold (corpus specific stop words).
If float, the parameter represents a proportion of documents, integer
absolute counts.
This parameter is ignored if vocabulary is not None.</dd>
<dt>min_df <span class="classifier-delimiter">:</span> <span class="classifier">float in range [0.0, 1.0] or int, optional, 1 by default</span></dt>
<dd>When building the vocabulary ignore terms that have a term frequency
strictly lower than the given threshold.
This value is also called cut-off in the literature.
If float, the parameter represents a proportion of documents, integer
absolute counts.
This parameter is ignored if vocabulary is not None.</dd>
<dt>max_features <span class="classifier-delimiter">:</span> <span class="classifier">optional, None by default</span></dt>
<dd><p class="first">If not None, build a vocabulary that only consider the top
max_features ordered by term frequency across the corpus.</p>
<p class="last">This parameter is ignored if vocabulary is not None.</p>
</dd>
<dt>vocabulary <span class="classifier-delimiter">:</span> <span class="classifier">Mapping or iterable, optional</span></dt>
<dd>Either a Mapping (e.g., a dict) where keys are terms and values are
indices in the feature matrix, or an iterable over terms. If not
given, a vocabulary is determined from the input documents.</dd>
<dt>binary <span class="classifier-delimiter">:</span> <span class="classifier">boolean, False by default.</span></dt>
<dd>If True, all non zero counts are set to 1. This is useful for discrete
probabilistic models that model binary events rather than integer
counts.</dd>
<dt>dtype <span class="classifier-delimiter">:</span> <span class="classifier">type, optional</span></dt>
<dd>Type of the matrix returned by fit_transform() or transform().</dd>
<dt>norm <span class="classifier-delimiter">:</span> <span class="classifier">&#8216;l1&#8217;, &#8216;l2&#8217; or None, optional</span></dt>
<dd>Norm used to normalize term vectors. None for no normalization.</dd>
<dt>use_idf <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span></dt>
<dd>Enable inverse-document-frequency reweighting.</dd>
<dt>smooth_idf <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span></dt>
<dd>Smooth idf weights by adding one to document frequencies, as if an
extra document was seen containing every term in the collection
exactly once. Prevents zero divisions.</dd>
<dt>sublinear_tf <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span></dt>
<dd>Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).</dd>
</dl>
<p>See also</p>
<dl class="docutils">
<dt>CountVectorizer</dt>
<dd>Tokenize the documents and count the occurrences of token and return
them as a sparse matrix</dd>
<dt>TfidfTransformer</dt>
<dd>Apply Term Frequency Inverse Document Frequency normalization to a
sparse matrix of occurrence counts.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>TfidfVectorizerSklearnNode</strong></li>
<li><strong>TfidfVectorizerSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-truncatedsvdsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.TruncatedSVDSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.TruncatedSVDSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.TruncatedSVDSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-truncatedsvdsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.TruncatedSVDSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">TruncatedSVDSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.TruncatedSVDSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Dimensionality reduction using truncated SVD (aka LSA).</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.decomposition.truncated_svd.TruncatedSVD</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p>This transformer performs linear dimensionality reduction by means of
truncated singular value decomposition (SVD). It is very similar to PCA,
but operates on sample vectors directly, instead of on a covariance matrix.
This means it can work with scipy.sparse matrices efficiently.</p>
<p>In particular, truncated SVD works on term count/tf–idf matrices as
returned by the vectorizers in sklearn.feature_extraction.text. In that
context, it is known as latent semantic analysis (LSA).</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int, default = 2</span></dt>
<dd>Desired dimensionality of output data.
Must be strictly less than the number of features.
The default value is useful for visualisation. For LSA, a value of
100 is recommended.</dd>
<dt>algorithm <span class="classifier-delimiter">:</span> <span class="classifier">string, default = &#8220;randomized&#8221;</span></dt>
<dd>SVD solver to use. Either &#8220;arpack&#8221; for the ARPACK wrapper in SciPy
(scipy.sparse.linalg.svds), or &#8220;randomized&#8221; for the randomized
algorithm due to Halko (2009).</dd>
<dt>n_iterations <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>Number of iterations for randomized SVD solver. Not used by ARPACK.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int or RandomState, optional</span></dt>
<dd>(Seed for) pseudo-random number generator. If not given, the
numpy.random singleton is used.</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Tolerance for ARPACK. 0 means machine precision. Ignored by randomized
SVD solver.</dd>
</dl>
<p><strong>Attributes</strong></p>
<p><cite>components_</cite> : array, shape (n_components, n_features)</p>
<p>See also</p>
<p>PCA
RandomizedPCA</p>
<p><strong>References</strong></p>
<p>Finding structure with randomness: Stochastic algorithms for constructing
approximate matrix decompositions
Halko, et al., 2009 (arXiv:909) http://arxiv.org/pdf/0909.4061</p>
<p><strong>Notes</strong></p>
<p>SVD suffers from a problem called &#8220;sign indeterminancy&#8221;, which means the
sign of the <tt class="docutils literal"><span class="pre">components_</span></tt> and the output from transform depend on the
algorithm and random state. To work around this, fit instances of this
class to data once, then keep the instance around to do transformations.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>TruncatedSVDSklearnNode</strong></li>
<li><strong>TruncatedSVDSklearn</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikits-nodes-wardagglomerationsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikits_nodes.WardAgglomerationSklearnNode" title="pySPACE.missions.nodes.scikits_nodes.WardAgglomerationSklearnNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikits_nodes.WardAgglomerationSklearnNode</span></tt></a><a class="headerlink" href="#pyspace-missions-nodes-scikits-nodes-wardagglomerationsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikits_nodes.WardAgglomerationSklearnNode">
<em class="property">class </em><tt class="descclassname">pySPACE.missions.nodes.scikits_nodes.</tt><tt class="descname">WardAgglomerationSklearnNode</tt><big>(</big><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pySPACE.missions.nodes.scikits_nodes.WardAgglomerationSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><tt class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></tt></a></p>
<p>Feature agglomeration based on Ward hierarchical clustering</p>
<p>This node has been automatically generated by wrapping the <tt class="docutils literal"><span class="pre">sklearn.cluster.hierarchical.WardAgglomeration</span></tt> class
from the <tt class="docutils literal"><span class="pre">sklearn</span></tt> library.  The wrapped instance can be accessed
through the <tt class="docutils literal"><span class="pre">scikits_alg</span></tt> attribute.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_clusters <span class="classifier-delimiter">:</span> <span class="classifier">int, default=2</span></dt>
<dd>The number of clusters.</dd>
<dt>connectivity <span class="classifier-delimiter">:</span> <span class="classifier">sparse matrix (optional)</span></dt>
<dd>connectivity matrix. Defines for each feature the neighboring
features following a given structure of the data.
Default is None, i.e, the hierarchical agglomeration algorithm is
unstructured.</dd>
<dt>memory <span class="classifier-delimiter">:</span> <span class="classifier">Instance of joblib.Memory or string (optional)</span></dt>
<dd>Used to cache the output of the computation of the tree.
By default, no caching is done. If a string is given, it is the
path to the caching directory.</dd>
<dt>copy <span class="classifier-delimiter">:</span> <span class="classifier">bool, default=True</span></dt>
<dd>Copy the connectivity matrix or work in-place.</dd>
<dt>n_components <span class="classifier-delimiter">:</span> <span class="classifier">int (optional)</span></dt>
<dd>The number of connected components in the graph defined by the
connectivity matrix. If not set, it is estimated.</dd>
<dt>compute_full_tree: bool or &#8216;auto&#8217; (optional)</dt>
<dd>Stop early the construction of the tree at n_clusters. This is
useful to decrease computation time if the number of clusters is
not small compared to the number of samples. This option is
useful only when specifying a connectivity matrix. Note also that
when varying the number of cluster and using caching, it may
be advantageous to compute the full tree.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>children_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_nodes, 2]</span></dt>
<dd>The children of each non-leaf node. Values less than <cite>n_samples</cite> refer
to leaves of the tree. A greater value <cite>i</cite> indicates a node with
children <cite>children_[i - n_samples]</cite>.</dd>
<dt><cite>labels_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">array [n_features]</span></dt>
<dd>cluster labels for each feature</dd>
<dt><cite>n_leaves_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Number of leaves in the hierarchical tree.</dd>
<dt><cite>n_components_</cite> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>The estimated number of connected components in the graph.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>WardAgglomerationSklearn</strong></li>
<li><strong>WardAgglomerationSklearnNode</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="api/generated/pySPACE.tools.socket_utils.html" title="socket_utils"
             >previous</a> |</li>
        <li><a href="index.html">pySPACE 1.0 release documentation</a> &raquo;</li>
          <li><a href="content.html" >Table of Contents</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2013, pySPACE Developer Team.
      Last updated on Mar 27, 2014.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.2.2.
    </div>
  </body>
</html>